{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nprimavera/Reinforcement-Learning/blob/main/Project5_V8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MECS6616 Spring 2025 - Project 5**"
      ],
      "metadata": {
        "id": "PxhEChyomf5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "***IMPORTANT:***\n",
        "- **Before starting, make sure to read the [Assignment Instructions](https://courseworks2.columbia.edu/courses/215046/pages/assignment-instructions) page on Courseworks to understand the workflow and submission requirements for this project.**\n",
        "\n",
        "**FOR PROJECT 5!!!**\n",
        "- Apart from the link to your notebook, you are also required to submit `q_network.pth` of Part 1 and `ppo_network.zip` (model checkpoints are loaded and saved by stable_baselines3 as zip files) of Part 2 to Coursework. You should put the link to your notebook in the comment entry"
      ],
      "metadata": {
        "id": "50kTBhSwmrJB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inY7y5CRo97q"
      },
      "source": [
        "# Project Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# There will be error messages from this command. You can ignore those error messages\n",
        "# as long as you see \"Successfully installed setuptools-65.5.0\" at the end.\n",
        "\n",
        "# After installing setuptools, a pop-up window will appear and you will be prompted\n",
        "# to restart the notebook environment. Click on the restart environment button before continuing\n",
        "\n",
        "!pip install setuptools==65.5.0"
      ],
      "metadata": {
        "id": "PTRNqFBLkRDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7833830c-0943-49f5-8287-f64deecb97da"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools==65.5.0 in /usr/local/lib/python3.11/dist-packages (65.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**----------------------------**\n",
        "**WAIT FOR NOTEBOOK TO RESTART**\n",
        "**----------------------------**"
      ],
      "metadata": {
        "id": "tnf22sQzqw6_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QPIiNSZ8hb8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec9ec4c8-cdc3-49e0-a7dd-efceda649e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mecs6616_sp24_project5'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects:   6% (1/15)\u001b[K\rremote: Counting objects:  13% (2/15)\u001b[K\rremote: Counting objects:  20% (3/15)\u001b[K\rremote: Counting objects:  26% (4/15)\u001b[K\rremote: Counting objects:  33% (5/15)\u001b[K\rremote: Counting objects:  40% (6/15)\u001b[K\rremote: Counting objects:  46% (7/15)\u001b[K\rremote: Counting objects:  53% (8/15)\u001b[K\rremote: Counting objects:  60% (9/15)\u001b[K\rremote: Counting objects:  66% (10/15)\u001b[K\rremote: Counting objects:  73% (11/15)\u001b[K\rremote: Counting objects:  80% (12/15)\u001b[K\rremote: Counting objects:  86% (13/15)\u001b[K\rremote: Counting objects:  93% (14/15)\u001b[K\rremote: Counting objects: 100% (15/15)\u001b[K\rremote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects:   9% (1/11)\u001b[K\rremote: Compressing objects:  18% (2/11)\u001b[K\rremote: Compressing objects:  27% (3/11)\u001b[K\rremote: Compressing objects:  36% (4/11)\u001b[K\rremote: Compressing objects:  45% (5/11)\u001b[K\rremote: Compressing objects:  54% (6/11)\u001b[K\rremote: Compressing objects:  63% (7/11)\u001b[K\rremote: Compressing objects:  72% (8/11)\u001b[K\rremote: Compressing objects:  81% (9/11)\u001b[K\rremote: Compressing objects:  90% (10/11)\u001b[K\rremote: Compressing objects: 100% (11/11)\u001b[K\rremote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects:   6% (1/15)\rReceiving objects:  13% (2/15)\rReceiving objects:  20% (3/15)\rReceiving objects:  26% (4/15)\rReceiving objects:  33% (5/15)\rReceiving objects:  40% (6/15)\rReceiving objects:  46% (7/15)\rReceiving objects:  53% (8/15)\rReceiving objects:  60% (9/15)\rReceiving objects:  66% (10/15)\rReceiving objects:  73% (11/15)\rReceiving objects:  80% (12/15)\rReceiving objects:  86% (13/15)\rReceiving objects:  93% (14/15)\rReceiving objects: 100% (15/15)\rReceiving objects: 100% (15/15), 9.22 KiB | 9.22 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# After running this cell, the folder 'mecs6616_sp25_project5' will show up in the file explorer on the left (click on the folder icon if it's not open)\n",
        "# It may take a few seconds to appear\n",
        "!git clone https://github.com/roamlab/mecs6616_sp24_project5.git\n",
        "!mv /content/mecs6616_sp24_project5 /content/mecs6616_sp25_project5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ise8RAQhhs3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "914e1493-060c-469f-c121-5123c134294e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/mecs6616_sp25_project5/arm_dynamics_base.py' -> '/content/arm_dynamics_base.py'\n",
            "'/content/mecs6616_sp25_project5/arm_dynamics.py' -> '/content/arm_dynamics.py'\n",
            "'/content/mecs6616_sp25_project5/arm_env.py' -> '/content/arm_env.py'\n",
            "'/content/mecs6616_sp25_project5/geometry.py' -> '/content/geometry.py'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5' -> '/content/mecs6616_sp24_project5'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git' -> '/content/mecs6616_sp24_project5/.git'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/info' -> '/content/mecs6616_sp24_project5/.git/info'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/info/exclude' -> '/content/mecs6616_sp24_project5/.git/info/exclude'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/branches' -> '/content/mecs6616_sp24_project5/.git/branches'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks' -> '/content/mecs6616_sp24_project5/.git/hooks'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/commit-msg.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/commit-msg.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/applypatch-msg.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/applypatch-msg.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/pre-push.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/pre-push.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/pre-receive.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/pre-receive.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/pre-applypatch.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/pre-applypatch.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/push-to-checkout.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/push-to-checkout.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/update.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/update.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/pre-commit.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/pre-commit.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/fsmonitor-watchman.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/fsmonitor-watchman.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/prepare-commit-msg.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/prepare-commit-msg.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/pre-rebase.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/pre-rebase.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/pre-merge-commit.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/pre-merge-commit.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/post-update.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/post-update.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/description' -> '/content/mecs6616_sp24_project5/.git/description'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/refs' -> '/content/mecs6616_sp24_project5/.git/refs'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/refs/heads' -> '/content/mecs6616_sp24_project5/.git/refs/heads'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/refs/heads/main' -> '/content/mecs6616_sp24_project5/.git/refs/heads/main'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/refs/tags' -> '/content/mecs6616_sp24_project5/.git/refs/tags'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/refs/remotes' -> '/content/mecs6616_sp24_project5/.git/refs/remotes'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/refs/remotes/origin' -> '/content/mecs6616_sp24_project5/.git/refs/remotes/origin'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/refs/remotes/origin/HEAD' -> '/content/mecs6616_sp24_project5/.git/refs/remotes/origin/HEAD'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/index' -> '/content/mecs6616_sp24_project5/.git/index'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/objects' -> '/content/mecs6616_sp24_project5/.git/objects'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/objects/pack' -> '/content/mecs6616_sp24_project5/.git/objects/pack'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/objects/pack/pack-2049da8647337dd2a4651ce334d5defad719b52a.pack' -> '/content/mecs6616_sp24_project5/.git/objects/pack/pack-2049da8647337dd2a4651ce334d5defad719b52a.pack'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/objects/pack/pack-2049da8647337dd2a4651ce334d5defad719b52a.idx' -> '/content/mecs6616_sp24_project5/.git/objects/pack/pack-2049da8647337dd2a4651ce334d5defad719b52a.idx'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/objects/info' -> '/content/mecs6616_sp24_project5/.git/objects/info'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/config' -> '/content/mecs6616_sp24_project5/.git/config'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/packed-refs' -> '/content/mecs6616_sp24_project5/.git/packed-refs'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs' -> '/content/mecs6616_sp24_project5/.git/logs'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs/refs' -> '/content/mecs6616_sp24_project5/.git/logs/refs'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs/refs/remotes' -> '/content/mecs6616_sp24_project5/.git/logs/refs/remotes'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs/refs/remotes/origin' -> '/content/mecs6616_sp24_project5/.git/logs/refs/remotes/origin'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs/refs/remotes/origin/HEAD' -> '/content/mecs6616_sp24_project5/.git/logs/refs/remotes/origin/HEAD'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs/refs/heads' -> '/content/mecs6616_sp24_project5/.git/logs/refs/heads'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs/refs/heads/main' -> '/content/mecs6616_sp24_project5/.git/logs/refs/heads/main'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs/HEAD' -> '/content/mecs6616_sp24_project5/.git/logs/HEAD'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/HEAD' -> '/content/mecs6616_sp24_project5/.git/HEAD'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/README.md' -> '/content/mecs6616_sp24_project5/README.md'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/arm_dynamics.py' -> '/content/mecs6616_sp24_project5/arm_dynamics.py'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/arm_dynamics_base.py' -> '/content/mecs6616_sp24_project5/arm_dynamics_base.py'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/arm_env.py' -> '/content/mecs6616_sp24_project5/arm_env.py'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/geometry.py' -> '/content/mecs6616_sp24_project5/geometry.py'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/render.py' -> '/content/mecs6616_sp24_project5/render.py'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/robot.py' -> '/content/mecs6616_sp24_project5/robot.py'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/score.py' -> '/content/mecs6616_sp24_project5/score.py'\n",
            "'/content/mecs6616_sp25_project5/README.md' -> '/content/README.md'\n",
            "'/content/mecs6616_sp25_project5/render.py' -> '/content/render.py'\n",
            "'/content/mecs6616_sp25_project5/robot.py' -> '/content/robot.py'\n",
            "'/content/mecs6616_sp25_project5/score.py' -> '/content/score.py'\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# copy all needed files into the working directory. This is simply to make accessing files easier\n",
        "!cp -av /content/mecs6616_sp25_project5/* /content/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# There will be error messages from this command. You can ignore those error messages\n",
        "# as long as you see \"Successfully installed gym stable-baselines3\" at the end.\n",
        "\n",
        "!pip install wheel==0.38.4\n",
        "!pip install gym stable-baselines3\n",
        "!pip install shimmy>=2.0"
      ],
      "metadata": {
        "id": "qEFlC9hVkRrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d91f1ab-ea38-45ec-8b8a-c30b73058579"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wheel==0.38.4\n",
            "  Using cached wheel-0.38.4-py3-none-any.whl.metadata (2.1 kB)\n",
            "Using cached wheel-0.38.4-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: wheel\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.45.1\n",
            "    Uninstalling wheel-0.45.1:\n",
            "      Successfully uninstalled wheel-0.45.1\n",
            "Successfully installed wheel-0.38.4\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.1.1)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Implement DQN\n",
        "\n",
        "For this part, you will implement DQN from scratch. You SHOULD NOT use any RL libraries."
      ],
      "metadata": {
        "id": "-KNg9fzU5Un7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-JvzRuwNsYz"
      },
      "source": [
        "## Starter Code Explanation\n",
        "In addition to code you are already familiar with from the previous project (i.e. arm dynamics, etc.) we are providing an \"Environment\" in the `ArmEnv` class. The environment \"wraps around\" the arm dynamics and provides the key functions that an RL algorithm expects: reset(...) and step(...). The implementation of `ArmEnv` follows the [OpenAI Gym](https://www.gymlibrary.dev/api/core/) API standard. It is a standard that is accepeted by many RL libraries and allows for our problem to be easily solved with various RL libraries. Take a moment to familiarize yourself with these functions! See [here](https://www.gymlibrary.dev/api/core/) for more information on the definition of the reset(...) and step(...) functions.\n",
        "\n",
        "Important notes:\n",
        "\n",
        "* The ArmEnv expects an action similar to the one used previously: a vector with a torque for every arm joint. Thus, the native action space for this environment is high-dimensional, and continuous. DQN will require an action space that is 1-dimensional and discrete. You will need to convert between these. For example, you can have an action space of [0, 1, 2,] where each number just represents the identity of an action candidate, and a conversion dictionary {0: [-0.1, -0.1], 1: [0.1, 0.1], 2: [0, 0]}. Then, when the Q network output an action 1, it will be converted into [0.1, 0.1] and used by the environment. Note that this is just an example method to implement the conversion and you do not have to follow the same procedure.\n",
        "* The observation provided by the environment will comprise the same state vector as before, to which we append the current position of the end_effector and the goal for the end-effector. Since your policy must learn to reach arbitrary goals, the goal must be provided as part of the observation. So the observation will consist of 8 values: 4 for the state, 2 for the pos_ee and 2 for the goal.\n",
        "* The maximum episode length of the environment is 200 steps. Each step is simulated for 0.01 second. This should be used for both training and testing.\n",
        "* The reward function of this environment is by default r(s, a) = - dist(pos_ee, goal)^2 where represents the negative square of L2 distance between the current position of the end-effector and the goal position."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arm Environment Example\n",
        "You are encouraged to view the `arm_env.py` file to understand the `random_goal()`, `reset()` and `step()`  functions but do not modify the file.\n",
        "\n",
        "The `env.reset()` method, will reset the arm in the vertically downwards position and set a new random goal by calling the `random_goal()` method. By understanding how the goals are set you could guide your training in that direction. You can also provide your own goal as a (2,1) array to the reset function as an argument. This could come handy later when training the model.\n",
        "\n",
        "The `env.step()` function takes an action as a (2,1) shaped array and outputs the next observation, reward, done and info. `info` is a dictionary with pos_ee and vel_ee values. This can come handy if you attempt to do some reward engineering.\n",
        "\n",
        "The cell below provides an example of random policy interacting with the ArmEnv for 50 steps (0.5 seconds)"
      ],
      "metadata": {
        "id": "gw8H0PZcSv7F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o6r9kJ5jpeds",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "outputId": "686d10e8-c15c-4353-ef8f-41ff52e601bc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "import numpy as np\n",
        "\n",
        "# DO NOT CHANGE arm parameters\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "# ------------------\n",
        "\n",
        "env = ArmEnv(arm, gui=True)\n",
        "\n",
        "# Passing our own defined goal to the reset function\n",
        "# goal = np.array([[0.5], [-1.5]])\n",
        "# obs = env.reset(goal)\n",
        "\n",
        "# Resetting the environment without the goal will set a random goal position\n",
        "obs = env.reset()\n",
        "\n",
        "for _ in range(50):\n",
        "  rand_action = np.random.uniform(-1.5, 1.5, (2,1))\n",
        "  obs, reward, done, info = env.step(rand_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jmXTT_ngdqG"
      },
      "source": [
        "### QNetwork\n",
        "This class defines the architecture of your network. You must fill in the __init__(...) function which defines your network, and the forward(...) function which performs the forward pass.\n",
        "\n",
        "Your action space should be discrete, with whatever cardinality you decide. The size of the output layer of your Q-Network should thus be the same as the cardinality of your action space. When selecting an action, a policy must choose the one that has the highest estimated Q-value for the current state. As part of the QNetwork class, we are providing the function select_discrete_action(...) which does exactly that.\n",
        "\n",
        "The arm environment itself however expects a 2-dimensional, continuous action vector. Therefore, when it comes to send an action to the environment, you must provide the kind of action the environment expects. It is your job to determine how to convert between the discrete action space of your Q-Network and the continuous action space of the arm. You do this by filling in the action_discrete_to_continuous(...) function in your QNetwork. You can expect to call the step function of the environment like this:\n",
        "\n",
        "```\n",
        "self.env.step(self.q_network.action_discrete_to_continuous(discrete_action))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7UyguLRKgf_I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "  def __init__(self, env):\n",
        "    super(QNetwork, self).__init__()\n",
        "    #--------- YOUR CODE HERE --------------\n",
        "\n",
        "\n",
        "    self.env = env\n",
        "\n",
        "    # Define discrete action space: 0 = [-0.1, -0.1], 1 = [-0.1, 0.1], 2 = [0.1, -0.1], 3 = [0.1, 0.1], 4 = [0, 0]\n",
        "    self.action_map = {\n",
        "        0: np.array([[-0.1], [-0.1]]),\n",
        "        1: np.array([[-0.1], [ 0.1]]),\n",
        "        2: np.array([[ 0.1], [-0.1]]),\n",
        "        3: np.array([[ 0.1], [ 0.1]]),\n",
        "        4: np.array([[ 0.0], [ 0.0]])\n",
        "    }\n",
        "    self.num_actions = len(self.action_map)\n",
        "\n",
        "    # Simple feedforward network: input=8, hidden=64/64, output=num_actions\n",
        "    self.fc1 = nn.Linear(8, 64)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    self.out = nn.Linear(64, self.num_actions)\n",
        "\n",
        "\n",
        "    #---------------------------------------\n",
        "\n",
        "  def forward(self, x, device):\n",
        "    #--------- YOUR CODE HERE --------------\n",
        "\n",
        "\n",
        "    x = torch.FloatTensor(x).to(device)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    q_vals = self.out(x)\n",
        "    return q_vals\n",
        "\n",
        "\n",
        "    #---------------------------------------\n",
        "\n",
        "\n",
        "  def select_discrete_action(self, obs, device):\n",
        "\n",
        "    # Put the observation through the network to estimate q values for all possible discrete actions\n",
        "    est_q_vals = self.forward(obs.reshape((1,) + obs.shape), device)\n",
        "\n",
        "    # Choose the discrete action with the highest estimated q value\n",
        "    discrete_action = torch.argmax(est_q_vals, dim=1).tolist()[0]\n",
        "\n",
        "    return discrete_action\n",
        "\n",
        "  def action_discrete_to_continuous(self, discrete_action):\n",
        "    #--------- YOUR CODE HERE --------------\n",
        "\n",
        "\n",
        "    return self.action_map[discrete_action]\n",
        "\n",
        "\n",
        "    #---------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide you with code to use the replay buffer in your RL implementation. You do not need to change the ReplayBuffer class.\n",
        "```\n",
        "rb = ReplayBuffer()\n",
        "```\n",
        "After creating a ReplayBuffer object you can add samples in the buffer using `put()`:\n",
        "```\n",
        "rb.put((obs, action, reward, next_obs, done))\n",
        "```\n",
        "Take random samples from the buffer using:\n",
        "```\n",
        "obs, actions, rewards, next_obses, dones = rb.sample(batch_size)\n",
        "```\n"
      ],
      "metadata": {
        "id": "IUjAeQcPdsGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "q7NytRAXtYkE"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, buffer_limit):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done_mask = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append(a)\n",
        "            r_lst.append(r)\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append(done_mask)\n",
        "\n",
        "        return np.array(s_lst), np.array(a_lst), \\\n",
        "               np.array(r_lst), np.array(s_prime_lst), \\\n",
        "               np.array(done_mask_lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TrainDQN\n",
        "Here, you must fill in the train(...) function that actually trains your network.\n",
        "\n",
        "We are providing a helper function called save_model(...) that will save the current Q-network. Use this as you see fit.\n",
        "\n",
        "To set one network equal to another one, you can use code like this:\n",
        "```\n",
        "target_network.load_state_dict(self.q_network.state_dict())\n",
        "```\n",
        "\n",
        "If you would like to be graded with a specific seed for the random number generators, make sure to change the default seed in the initialization of the TrainDQN class.\n",
        "\n",
        "The time taken to train the model will depend mainly on how big is your model architecture and the number of episodes you run the training for. As a reference, the time taken to train a model on 1500 episodes, which passed all evaluation metrics was about an hour.\n",
        "* Reference value for clipping the gradient value as mentioned in class: 0.2\n",
        "* Reference value for a typical size of Replay Buffer: >10k\n",
        "* Reference value for batch size while training: 64 - 512\n",
        "\n",
        "Note that these are just reference values and larger is not always better as it may slow things down.\n",
        "\n",
        "It is good practice in RL to ensure simpler things are working before complicating environments or training techniques.\n",
        "\n",
        "If you think your training method is not working at all, you could pass a fixed goal to the `env.reset()` method during the training loop to ensure that your model is learning."
      ],
      "metadata": {
        "id": "pxVawoBLe3bd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EwS8xVR7tbeQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "import numpy as np\n",
        "from math import dist\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "def linear_decay(start, end, duration, t):\n",
        "    return max(end, start - (start - end) * t / duration)\n",
        "\n",
        "class TrainDQN:\n",
        "\n",
        "  def __init__(self, env, seed=0):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    self.env = env\n",
        "    self.device = torch.device('cpu')\n",
        "    self.q_network = QNetwork(env).to(self.device)\n",
        "    self.target_network = QNetwork(env).to(self.device)\n",
        "    self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "    self.optimizer = optim.Adam(self.q_network.parameters(), lr=1e-3)\n",
        "\n",
        "  def save_model(self, episode_num, save_dir='models'):\n",
        "    timestr = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    model_dir = os.path.join(save_dir, timestr)\n",
        "    if not os.path.exists(os.path.join(model_dir)):\n",
        "      os.makedirs(os.path.join(model_dir))\n",
        "    savepath = os.path.join(model_dir, f'q_network_ep_{episode_num:04d}.pth')\n",
        "    torch.save(self.q_network.state_dict(), savepath)\n",
        "    print(f'model saved to {savepath}\\n')\n",
        "\n",
        "\n",
        "  def train(self, num_episodes=1500, buffer_limit=20000, batch_size=128,\n",
        "            gamma=0.99, epsilon_start=1.0, epsilon_end=0.05, epsilon_decay=1000,\n",
        "            target_update_freq=10):\n",
        "\n",
        "    replay_buffer = ReplayBuffer(buffer_limit)\n",
        "    epsilon = epsilon_start\n",
        "\n",
        "    for episode in range(1, num_episodes + 1):\n",
        "      obs = self.env.reset()\n",
        "      done = False\n",
        "      total_reward = 0\n",
        "\n",
        "      while not done:\n",
        "        # Epsilon-greedy action\n",
        "        if np.random.rand() < epsilon:\n",
        "          action_discrete = np.random.randint(self.q_network.num_actions)\n",
        "        else:\n",
        "          obs_tensor = torch.FloatTensor(obs).to(self.device)\n",
        "          action_discrete = self.q_network.select_discrete_action(obs_tensor, self.device)\n",
        "\n",
        "        action_continuous = self.q_network.action_discrete_to_continuous(action_discrete)\n",
        "        next_obs, reward, done, _ = self.env.step(action_continuous)\n",
        "\n",
        "        # Store transition\n",
        "        replay_buffer.put((obs, action_discrete, reward, next_obs, float(done)))\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "\n",
        "        # Train if buffer is ready\n",
        "        if len(replay_buffer.buffer) >= batch_size:\n",
        "          s, a, r, s_prime, done_mask = replay_buffer.sample(batch_size)\n",
        "\n",
        "          s = torch.FloatTensor(s).to(self.device)\n",
        "          a = torch.LongTensor(a).unsqueeze(1).to(self.device)\n",
        "          r = torch.FloatTensor(r).unsqueeze(1).to(self.device)\n",
        "          s_prime = torch.FloatTensor(s_prime).to(self.device)\n",
        "          done_mask = torch.FloatTensor(done_mask).unsqueeze(1).to(self.device)\n",
        "\n",
        "          q_vals = self.q_network(s, self.device).gather(1, a)\n",
        "          next_q_vals = self.target_network(s_prime, self.device).max(1)[0].unsqueeze(1)\n",
        "          target = r + gamma * next_q_vals * (1 - done_mask)\n",
        "\n",
        "          loss = F.mse_loss(q_vals, target)\n",
        "\n",
        "          self.optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=0.2)\n",
        "          self.optimizer.step()\n",
        "\n",
        "      # Decay epsilon\n",
        "      epsilon = linear_decay(epsilon_start, epsilon_end, epsilon_decay, episode)\n",
        "\n",
        "      # Update target network\n",
        "      if episode % target_update_freq == 0:\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "      # Save model occasionally\n",
        "      if episode % 100 == 0:\n",
        "        self.save_model(episode)\n",
        "\n",
        "      print(f\"Episode {episode} | Total Reward: {total_reward:.2f} | Epsilon: {epsilon:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sEHSV1Q1BT1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c0c6995-4c05-4b73-fc30-d2fd907255db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/geometry.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  R[0,0] = np.cos(theta)\n",
            "/content/arm_dynamics.py:113: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  right_hand[self.idx_tau_eqbm(i), 0] += (tau[i + 1] if i < self.num_links - 1 else 0.0) - tau[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1 | Total Reward: -49.89 | Epsilon: 0.999\n",
            "Episode 2 | Total Reward: -72.76 | Epsilon: 0.998\n",
            "Episode 3 | Total Reward: -78.91 | Epsilon: 0.997\n",
            "Episode 4 | Total Reward: -137.69 | Epsilon: 0.996\n",
            "Episode 5 | Total Reward: -128.50 | Epsilon: 0.995\n",
            "Episode 6 | Total Reward: -23.12 | Epsilon: 0.994\n",
            "Episode 7 | Total Reward: -53.81 | Epsilon: 0.993\n",
            "Episode 8 | Total Reward: -62.08 | Epsilon: 0.992\n",
            "Episode 9 | Total Reward: -31.38 | Epsilon: 0.991\n",
            "Episode 10 | Total Reward: -285.83 | Epsilon: 0.991\n",
            "Episode 11 | Total Reward: -83.72 | Epsilon: 0.990\n",
            "Episode 12 | Total Reward: -28.04 | Epsilon: 0.989\n",
            "Episode 13 | Total Reward: -87.67 | Epsilon: 0.988\n",
            "Episode 14 | Total Reward: -112.61 | Epsilon: 0.987\n",
            "Episode 15 | Total Reward: -47.57 | Epsilon: 0.986\n",
            "Episode 16 | Total Reward: -26.31 | Epsilon: 0.985\n",
            "Episode 17 | Total Reward: -101.93 | Epsilon: 0.984\n",
            "Episode 18 | Total Reward: -87.38 | Epsilon: 0.983\n",
            "Episode 19 | Total Reward: -192.09 | Epsilon: 0.982\n",
            "Episode 20 | Total Reward: -12.76 | Epsilon: 0.981\n",
            "Episode 21 | Total Reward: -8.06 | Epsilon: 0.980\n",
            "Episode 22 | Total Reward: -129.29 | Epsilon: 0.979\n",
            "Episode 23 | Total Reward: -40.74 | Epsilon: 0.978\n",
            "Episode 24 | Total Reward: -6.30 | Epsilon: 0.977\n",
            "Episode 25 | Total Reward: -141.68 | Epsilon: 0.976\n",
            "Episode 26 | Total Reward: -130.08 | Epsilon: 0.975\n",
            "Episode 27 | Total Reward: -50.25 | Epsilon: 0.974\n",
            "Episode 28 | Total Reward: -200.50 | Epsilon: 0.973\n",
            "Episode 29 | Total Reward: -34.87 | Epsilon: 0.972\n",
            "Episode 30 | Total Reward: -198.93 | Epsilon: 0.972\n",
            "Episode 31 | Total Reward: -121.45 | Epsilon: 0.971\n",
            "Episode 32 | Total Reward: -84.44 | Epsilon: 0.970\n",
            "Episode 33 | Total Reward: -49.23 | Epsilon: 0.969\n",
            "Episode 34 | Total Reward: -114.49 | Epsilon: 0.968\n",
            "Episode 35 | Total Reward: -139.89 | Epsilon: 0.967\n",
            "Episode 36 | Total Reward: -64.72 | Epsilon: 0.966\n",
            "Episode 37 | Total Reward: -74.94 | Epsilon: 0.965\n",
            "Episode 38 | Total Reward: -20.67 | Epsilon: 0.964\n",
            "Episode 39 | Total Reward: -41.70 | Epsilon: 0.963\n",
            "Episode 40 | Total Reward: -122.95 | Epsilon: 0.962\n",
            "Episode 41 | Total Reward: -123.63 | Epsilon: 0.961\n",
            "Episode 42 | Total Reward: -46.30 | Epsilon: 0.960\n",
            "Episode 43 | Total Reward: -146.14 | Epsilon: 0.959\n",
            "Episode 44 | Total Reward: -102.36 | Epsilon: 0.958\n",
            "Episode 45 | Total Reward: -15.94 | Epsilon: 0.957\n",
            "Episode 46 | Total Reward: -42.15 | Epsilon: 0.956\n",
            "Episode 47 | Total Reward: -138.56 | Epsilon: 0.955\n",
            "Episode 48 | Total Reward: -22.67 | Epsilon: 0.954\n",
            "Episode 49 | Total Reward: -53.72 | Epsilon: 0.953\n",
            "Episode 50 | Total Reward: -47.59 | Epsilon: 0.953\n",
            "Episode 51 | Total Reward: -62.62 | Epsilon: 0.952\n",
            "Episode 52 | Total Reward: -67.57 | Epsilon: 0.951\n",
            "Episode 53 | Total Reward: -167.34 | Epsilon: 0.950\n",
            "Episode 54 | Total Reward: -2.63 | Epsilon: 0.949\n",
            "Episode 55 | Total Reward: -80.61 | Epsilon: 0.948\n",
            "Episode 56 | Total Reward: -2.74 | Epsilon: 0.947\n",
            "Episode 57 | Total Reward: -165.27 | Epsilon: 0.946\n",
            "Episode 58 | Total Reward: -41.00 | Epsilon: 0.945\n",
            "Episode 59 | Total Reward: -203.61 | Epsilon: 0.944\n",
            "Episode 60 | Total Reward: -35.44 | Epsilon: 0.943\n",
            "Episode 61 | Total Reward: -96.03 | Epsilon: 0.942\n",
            "Episode 62 | Total Reward: -20.21 | Epsilon: 0.941\n",
            "Episode 63 | Total Reward: -13.00 | Epsilon: 0.940\n",
            "Episode 64 | Total Reward: -28.47 | Epsilon: 0.939\n",
            "Episode 65 | Total Reward: -139.06 | Epsilon: 0.938\n",
            "Episode 66 | Total Reward: -14.46 | Epsilon: 0.937\n",
            "Episode 67 | Total Reward: -46.78 | Epsilon: 0.936\n",
            "Episode 68 | Total Reward: -5.09 | Epsilon: 0.935\n",
            "Episode 69 | Total Reward: -179.37 | Epsilon: 0.934\n",
            "Episode 70 | Total Reward: -31.35 | Epsilon: 0.933\n",
            "Episode 71 | Total Reward: -17.04 | Epsilon: 0.933\n",
            "Episode 72 | Total Reward: -37.11 | Epsilon: 0.932\n",
            "Episode 73 | Total Reward: -77.70 | Epsilon: 0.931\n",
            "Episode 74 | Total Reward: -164.36 | Epsilon: 0.930\n",
            "Episode 75 | Total Reward: -29.50 | Epsilon: 0.929\n",
            "Episode 76 | Total Reward: -239.03 | Epsilon: 0.928\n",
            "Episode 77 | Total Reward: -21.94 | Epsilon: 0.927\n",
            "Episode 78 | Total Reward: -237.65 | Epsilon: 0.926\n",
            "Episode 79 | Total Reward: -6.94 | Epsilon: 0.925\n",
            "Episode 80 | Total Reward: -27.59 | Epsilon: 0.924\n",
            "Episode 81 | Total Reward: -11.70 | Epsilon: 0.923\n",
            "Episode 82 | Total Reward: -136.77 | Epsilon: 0.922\n",
            "Episode 83 | Total Reward: -49.47 | Epsilon: 0.921\n",
            "Episode 84 | Total Reward: -238.05 | Epsilon: 0.920\n",
            "Episode 85 | Total Reward: -23.55 | Epsilon: 0.919\n",
            "Episode 86 | Total Reward: -181.39 | Epsilon: 0.918\n",
            "Episode 87 | Total Reward: -87.74 | Epsilon: 0.917\n",
            "Episode 88 | Total Reward: -81.63 | Epsilon: 0.916\n",
            "Episode 89 | Total Reward: -93.85 | Epsilon: 0.915\n",
            "Episode 90 | Total Reward: -119.65 | Epsilon: 0.914\n",
            "Episode 91 | Total Reward: -152.06 | Epsilon: 0.914\n",
            "Episode 92 | Total Reward: -37.09 | Epsilon: 0.913\n",
            "Episode 93 | Total Reward: -105.65 | Epsilon: 0.912\n",
            "Episode 94 | Total Reward: -73.86 | Epsilon: 0.911\n",
            "Episode 95 | Total Reward: -36.53 | Epsilon: 0.910\n",
            "Episode 96 | Total Reward: -28.43 | Epsilon: 0.909\n",
            "Episode 97 | Total Reward: -35.44 | Epsilon: 0.908\n",
            "Episode 98 | Total Reward: -30.00 | Epsilon: 0.907\n",
            "Episode 99 | Total Reward: -5.63 | Epsilon: 0.906\n",
            "model saved to models/2025-05-09_23-53-28/q_network_ep_0100.pth\n",
            "\n",
            "Episode 100 | Total Reward: -20.13 | Epsilon: 0.905\n",
            "Episode 101 | Total Reward: -17.35 | Epsilon: 0.904\n",
            "Episode 102 | Total Reward: -94.82 | Epsilon: 0.903\n",
            "Episode 103 | Total Reward: -11.17 | Epsilon: 0.902\n",
            "Episode 104 | Total Reward: -112.74 | Epsilon: 0.901\n",
            "Episode 105 | Total Reward: -124.51 | Epsilon: 0.900\n",
            "Episode 106 | Total Reward: -102.16 | Epsilon: 0.899\n",
            "Episode 107 | Total Reward: -172.82 | Epsilon: 0.898\n",
            "Episode 108 | Total Reward: -102.13 | Epsilon: 0.897\n",
            "Episode 109 | Total Reward: -32.84 | Epsilon: 0.896\n",
            "Episode 110 | Total Reward: -3.39 | Epsilon: 0.895\n",
            "Episode 111 | Total Reward: -33.66 | Epsilon: 0.895\n",
            "Episode 112 | Total Reward: -163.67 | Epsilon: 0.894\n",
            "Episode 113 | Total Reward: -41.42 | Epsilon: 0.893\n",
            "Episode 114 | Total Reward: -146.51 | Epsilon: 0.892\n",
            "Episode 115 | Total Reward: -88.03 | Epsilon: 0.891\n",
            "Episode 116 | Total Reward: -79.45 | Epsilon: 0.890\n",
            "Episode 117 | Total Reward: -164.35 | Epsilon: 0.889\n",
            "Episode 118 | Total Reward: -27.94 | Epsilon: 0.888\n",
            "Episode 119 | Total Reward: -147.99 | Epsilon: 0.887\n",
            "Episode 120 | Total Reward: -72.31 | Epsilon: 0.886\n",
            "Episode 121 | Total Reward: -27.62 | Epsilon: 0.885\n",
            "Episode 122 | Total Reward: -133.39 | Epsilon: 0.884\n",
            "Episode 123 | Total Reward: -44.16 | Epsilon: 0.883\n",
            "Episode 124 | Total Reward: -95.50 | Epsilon: 0.882\n",
            "Episode 125 | Total Reward: -186.92 | Epsilon: 0.881\n",
            "Episode 126 | Total Reward: -77.46 | Epsilon: 0.880\n",
            "Episode 127 | Total Reward: -19.66 | Epsilon: 0.879\n",
            "Episode 128 | Total Reward: -101.54 | Epsilon: 0.878\n",
            "Episode 129 | Total Reward: -62.69 | Epsilon: 0.877\n",
            "Episode 130 | Total Reward: -30.63 | Epsilon: 0.877\n",
            "Episode 131 | Total Reward: -72.66 | Epsilon: 0.876\n",
            "Episode 132 | Total Reward: -36.72 | Epsilon: 0.875\n",
            "Episode 133 | Total Reward: -91.94 | Epsilon: 0.874\n",
            "Episode 134 | Total Reward: -63.71 | Epsilon: 0.873\n",
            "Episode 135 | Total Reward: -48.66 | Epsilon: 0.872\n",
            "Episode 136 | Total Reward: -97.64 | Epsilon: 0.871\n",
            "Episode 137 | Total Reward: -213.10 | Epsilon: 0.870\n",
            "Episode 138 | Total Reward: -16.41 | Epsilon: 0.869\n",
            "Episode 139 | Total Reward: -100.80 | Epsilon: 0.868\n",
            "Episode 140 | Total Reward: -112.45 | Epsilon: 0.867\n",
            "Episode 141 | Total Reward: -7.59 | Epsilon: 0.866\n",
            "Episode 142 | Total Reward: -118.88 | Epsilon: 0.865\n",
            "Episode 143 | Total Reward: -135.12 | Epsilon: 0.864\n",
            "Episode 144 | Total Reward: -20.15 | Epsilon: 0.863\n",
            "Episode 145 | Total Reward: -33.71 | Epsilon: 0.862\n",
            "Episode 146 | Total Reward: -25.16 | Epsilon: 0.861\n",
            "Episode 147 | Total Reward: -27.84 | Epsilon: 0.860\n",
            "Episode 148 | Total Reward: -29.20 | Epsilon: 0.859\n",
            "Episode 149 | Total Reward: -61.04 | Epsilon: 0.858\n",
            "Episode 150 | Total Reward: -247.05 | Epsilon: 0.858\n",
            "Episode 151 | Total Reward: -121.73 | Epsilon: 0.857\n",
            "Episode 152 | Total Reward: -54.38 | Epsilon: 0.856\n",
            "Episode 153 | Total Reward: -101.02 | Epsilon: 0.855\n",
            "Episode 154 | Total Reward: -10.15 | Epsilon: 0.854\n",
            "Episode 155 | Total Reward: -163.17 | Epsilon: 0.853\n",
            "Episode 156 | Total Reward: -42.34 | Epsilon: 0.852\n",
            "Episode 157 | Total Reward: -28.12 | Epsilon: 0.851\n",
            "Episode 158 | Total Reward: -11.88 | Epsilon: 0.850\n",
            "Episode 159 | Total Reward: -26.93 | Epsilon: 0.849\n",
            "Episode 160 | Total Reward: -42.70 | Epsilon: 0.848\n",
            "Episode 161 | Total Reward: -26.09 | Epsilon: 0.847\n",
            "Episode 162 | Total Reward: -17.17 | Epsilon: 0.846\n",
            "Episode 163 | Total Reward: -19.87 | Epsilon: 0.845\n",
            "Episode 164 | Total Reward: -89.10 | Epsilon: 0.844\n",
            "Episode 165 | Total Reward: -91.71 | Epsilon: 0.843\n",
            "Episode 166 | Total Reward: -60.98 | Epsilon: 0.842\n",
            "Episode 167 | Total Reward: -47.76 | Epsilon: 0.841\n",
            "Episode 168 | Total Reward: -114.21 | Epsilon: 0.840\n",
            "Episode 169 | Total Reward: -117.95 | Epsilon: 0.839\n",
            "Episode 170 | Total Reward: -47.54 | Epsilon: 0.839\n",
            "Episode 171 | Total Reward: -65.30 | Epsilon: 0.838\n",
            "Episode 172 | Total Reward: -18.66 | Epsilon: 0.837\n",
            "Episode 173 | Total Reward: -37.11 | Epsilon: 0.836\n",
            "Episode 174 | Total Reward: -114.52 | Epsilon: 0.835\n",
            "Episode 175 | Total Reward: -115.35 | Epsilon: 0.834\n",
            "Episode 176 | Total Reward: -63.38 | Epsilon: 0.833\n",
            "Episode 177 | Total Reward: -18.76 | Epsilon: 0.832\n",
            "Episode 178 | Total Reward: -81.30 | Epsilon: 0.831\n",
            "Episode 179 | Total Reward: -106.70 | Epsilon: 0.830\n",
            "Episode 180 | Total Reward: -79.14 | Epsilon: 0.829\n",
            "Episode 181 | Total Reward: -18.54 | Epsilon: 0.828\n",
            "Episode 182 | Total Reward: -96.65 | Epsilon: 0.827\n",
            "Episode 183 | Total Reward: -25.32 | Epsilon: 0.826\n",
            "Episode 184 | Total Reward: -26.42 | Epsilon: 0.825\n",
            "Episode 185 | Total Reward: -57.26 | Epsilon: 0.824\n",
            "Episode 186 | Total Reward: -47.47 | Epsilon: 0.823\n",
            "Episode 187 | Total Reward: -52.37 | Epsilon: 0.822\n",
            "Episode 188 | Total Reward: -90.50 | Epsilon: 0.821\n",
            "Episode 189 | Total Reward: -41.39 | Epsilon: 0.820\n",
            "Episode 190 | Total Reward: -68.67 | Epsilon: 0.820\n",
            "Episode 191 | Total Reward: -102.76 | Epsilon: 0.819\n",
            "Episode 192 | Total Reward: -68.61 | Epsilon: 0.818\n",
            "Episode 193 | Total Reward: -18.78 | Epsilon: 0.817\n",
            "Episode 194 | Total Reward: -71.92 | Epsilon: 0.816\n",
            "Episode 195 | Total Reward: -81.19 | Epsilon: 0.815\n",
            "Episode 196 | Total Reward: -46.67 | Epsilon: 0.814\n",
            "Episode 197 | Total Reward: -77.47 | Epsilon: 0.813\n",
            "Episode 198 | Total Reward: -80.27 | Epsilon: 0.812\n",
            "Episode 199 | Total Reward: -94.08 | Epsilon: 0.811\n",
            "model saved to models/2025-05-09_23-55-18/q_network_ep_0200.pth\n",
            "\n",
            "Episode 200 | Total Reward: -271.29 | Epsilon: 0.810\n",
            "Episode 201 | Total Reward: -79.91 | Epsilon: 0.809\n",
            "Episode 202 | Total Reward: -25.69 | Epsilon: 0.808\n",
            "Episode 203 | Total Reward: -126.60 | Epsilon: 0.807\n",
            "Episode 204 | Total Reward: -82.02 | Epsilon: 0.806\n",
            "Episode 205 | Total Reward: -11.81 | Epsilon: 0.805\n",
            "Episode 206 | Total Reward: -21.49 | Epsilon: 0.804\n",
            "Episode 207 | Total Reward: -59.84 | Epsilon: 0.803\n",
            "Episode 208 | Total Reward: -50.39 | Epsilon: 0.802\n",
            "Episode 209 | Total Reward: -98.84 | Epsilon: 0.801\n",
            "Episode 210 | Total Reward: -34.88 | Epsilon: 0.800\n",
            "Episode 211 | Total Reward: -63.28 | Epsilon: 0.800\n",
            "Episode 212 | Total Reward: -113.08 | Epsilon: 0.799\n",
            "Episode 213 | Total Reward: -36.85 | Epsilon: 0.798\n",
            "Episode 214 | Total Reward: -112.04 | Epsilon: 0.797\n",
            "Episode 215 | Total Reward: -55.01 | Epsilon: 0.796\n",
            "Episode 216 | Total Reward: -69.47 | Epsilon: 0.795\n",
            "Episode 217 | Total Reward: -33.00 | Epsilon: 0.794\n",
            "Episode 218 | Total Reward: -55.88 | Epsilon: 0.793\n",
            "Episode 219 | Total Reward: -113.87 | Epsilon: 0.792\n",
            "Episode 220 | Total Reward: -14.03 | Epsilon: 0.791\n",
            "Episode 221 | Total Reward: -25.70 | Epsilon: 0.790\n",
            "Episode 222 | Total Reward: -152.31 | Epsilon: 0.789\n",
            "Episode 223 | Total Reward: -32.38 | Epsilon: 0.788\n",
            "Episode 224 | Total Reward: -88.99 | Epsilon: 0.787\n",
            "Episode 225 | Total Reward: -72.67 | Epsilon: 0.786\n",
            "Episode 226 | Total Reward: -74.08 | Epsilon: 0.785\n",
            "Episode 227 | Total Reward: -19.78 | Epsilon: 0.784\n",
            "Episode 228 | Total Reward: -7.38 | Epsilon: 0.783\n",
            "Episode 229 | Total Reward: -45.09 | Epsilon: 0.782\n",
            "Episode 230 | Total Reward: -56.82 | Epsilon: 0.781\n",
            "Episode 231 | Total Reward: -2.24 | Epsilon: 0.781\n",
            "Episode 232 | Total Reward: -176.66 | Epsilon: 0.780\n",
            "Episode 233 | Total Reward: -79.46 | Epsilon: 0.779\n",
            "Episode 234 | Total Reward: -83.51 | Epsilon: 0.778\n",
            "Episode 235 | Total Reward: -48.53 | Epsilon: 0.777\n",
            "Episode 236 | Total Reward: -98.70 | Epsilon: 0.776\n",
            "Episode 237 | Total Reward: -40.28 | Epsilon: 0.775\n",
            "Episode 238 | Total Reward: -95.55 | Epsilon: 0.774\n",
            "Episode 239 | Total Reward: -21.46 | Epsilon: 0.773\n",
            "Episode 240 | Total Reward: -59.27 | Epsilon: 0.772\n",
            "Episode 241 | Total Reward: -33.09 | Epsilon: 0.771\n",
            "Episode 242 | Total Reward: -27.97 | Epsilon: 0.770\n",
            "Episode 243 | Total Reward: -74.06 | Epsilon: 0.769\n",
            "Episode 244 | Total Reward: -72.44 | Epsilon: 0.768\n",
            "Episode 245 | Total Reward: -92.50 | Epsilon: 0.767\n",
            "Episode 246 | Total Reward: -46.76 | Epsilon: 0.766\n",
            "Episode 247 | Total Reward: -92.69 | Epsilon: 0.765\n",
            "Episode 248 | Total Reward: -115.16 | Epsilon: 0.764\n",
            "Episode 249 | Total Reward: -43.03 | Epsilon: 0.763\n",
            "Episode 250 | Total Reward: -81.84 | Epsilon: 0.762\n",
            "Episode 251 | Total Reward: -65.92 | Epsilon: 0.762\n",
            "Episode 252 | Total Reward: -132.45 | Epsilon: 0.761\n",
            "Episode 253 | Total Reward: -8.67 | Epsilon: 0.760\n",
            "Episode 254 | Total Reward: -25.43 | Epsilon: 0.759\n",
            "Episode 255 | Total Reward: -12.61 | Epsilon: 0.758\n",
            "Episode 256 | Total Reward: -87.11 | Epsilon: 0.757\n",
            "Episode 257 | Total Reward: -22.58 | Epsilon: 0.756\n",
            "Episode 258 | Total Reward: -112.94 | Epsilon: 0.755\n",
            "Episode 259 | Total Reward: -177.13 | Epsilon: 0.754\n",
            "Episode 260 | Total Reward: -47.28 | Epsilon: 0.753\n",
            "Episode 261 | Total Reward: -115.91 | Epsilon: 0.752\n",
            "Episode 262 | Total Reward: -130.60 | Epsilon: 0.751\n",
            "Episode 263 | Total Reward: -74.77 | Epsilon: 0.750\n",
            "Episode 264 | Total Reward: -20.66 | Epsilon: 0.749\n",
            "Episode 265 | Total Reward: -133.79 | Epsilon: 0.748\n",
            "Episode 266 | Total Reward: -80.76 | Epsilon: 0.747\n",
            "Episode 267 | Total Reward: -64.28 | Epsilon: 0.746\n",
            "Episode 268 | Total Reward: -9.17 | Epsilon: 0.745\n",
            "Episode 269 | Total Reward: -131.60 | Epsilon: 0.744\n",
            "Episode 270 | Total Reward: -60.86 | Epsilon: 0.744\n",
            "Episode 271 | Total Reward: -2.80 | Epsilon: 0.743\n",
            "Episode 272 | Total Reward: -72.83 | Epsilon: 0.742\n",
            "Episode 273 | Total Reward: -42.92 | Epsilon: 0.741\n",
            "Episode 274 | Total Reward: -111.92 | Epsilon: 0.740\n",
            "Episode 275 | Total Reward: -9.46 | Epsilon: 0.739\n",
            "Episode 276 | Total Reward: -84.63 | Epsilon: 0.738\n",
            "Episode 277 | Total Reward: -127.20 | Epsilon: 0.737\n",
            "Episode 278 | Total Reward: -6.05 | Epsilon: 0.736\n",
            "Episode 279 | Total Reward: -19.21 | Epsilon: 0.735\n",
            "Episode 280 | Total Reward: -43.67 | Epsilon: 0.734\n",
            "Episode 281 | Total Reward: -45.54 | Epsilon: 0.733\n",
            "Episode 282 | Total Reward: -22.52 | Epsilon: 0.732\n",
            "Episode 283 | Total Reward: -5.78 | Epsilon: 0.731\n",
            "Episode 284 | Total Reward: -18.95 | Epsilon: 0.730\n",
            "Episode 285 | Total Reward: -18.42 | Epsilon: 0.729\n",
            "Episode 286 | Total Reward: -34.00 | Epsilon: 0.728\n",
            "Episode 287 | Total Reward: -107.06 | Epsilon: 0.727\n",
            "Episode 288 | Total Reward: -170.74 | Epsilon: 0.726\n",
            "Episode 289 | Total Reward: -91.40 | Epsilon: 0.725\n",
            "Episode 290 | Total Reward: -67.59 | Epsilon: 0.724\n",
            "Episode 291 | Total Reward: -43.70 | Epsilon: 0.724\n",
            "Episode 292 | Total Reward: -168.82 | Epsilon: 0.723\n",
            "Episode 293 | Total Reward: -34.26 | Epsilon: 0.722\n",
            "Episode 294 | Total Reward: -37.69 | Epsilon: 0.721\n",
            "Episode 295 | Total Reward: -34.64 | Epsilon: 0.720\n",
            "Episode 296 | Total Reward: -4.83 | Epsilon: 0.719\n",
            "Episode 297 | Total Reward: -135.11 | Epsilon: 0.718\n",
            "Episode 298 | Total Reward: -6.27 | Epsilon: 0.717\n",
            "Episode 299 | Total Reward: -20.21 | Epsilon: 0.716\n",
            "model saved to models/2025-05-09_23-57-06/q_network_ep_0300.pth\n",
            "\n",
            "Episode 300 | Total Reward: -19.87 | Epsilon: 0.715\n",
            "Episode 301 | Total Reward: -21.33 | Epsilon: 0.714\n",
            "Episode 302 | Total Reward: -9.03 | Epsilon: 0.713\n",
            "Episode 303 | Total Reward: -88.55 | Epsilon: 0.712\n",
            "Episode 304 | Total Reward: -193.62 | Epsilon: 0.711\n",
            "Episode 305 | Total Reward: -31.26 | Epsilon: 0.710\n",
            "Episode 306 | Total Reward: -85.06 | Epsilon: 0.709\n",
            "Episode 307 | Total Reward: -28.84 | Epsilon: 0.708\n",
            "Episode 308 | Total Reward: -13.00 | Epsilon: 0.707\n",
            "Episode 309 | Total Reward: -60.63 | Epsilon: 0.706\n",
            "Episode 310 | Total Reward: -97.05 | Epsilon: 0.706\n",
            "Episode 311 | Total Reward: -259.83 | Epsilon: 0.705\n",
            "Episode 312 | Total Reward: -22.29 | Epsilon: 0.704\n",
            "Episode 313 | Total Reward: -74.44 | Epsilon: 0.703\n",
            "Episode 314 | Total Reward: -237.56 | Epsilon: 0.702\n",
            "Episode 315 | Total Reward: -46.30 | Epsilon: 0.701\n",
            "Episode 316 | Total Reward: -65.83 | Epsilon: 0.700\n",
            "Episode 317 | Total Reward: -16.20 | Epsilon: 0.699\n",
            "Episode 318 | Total Reward: -151.07 | Epsilon: 0.698\n",
            "Episode 319 | Total Reward: -5.71 | Epsilon: 0.697\n",
            "Episode 320 | Total Reward: -17.63 | Epsilon: 0.696\n",
            "Episode 321 | Total Reward: -73.18 | Epsilon: 0.695\n",
            "Episode 322 | Total Reward: -8.35 | Epsilon: 0.694\n",
            "Episode 323 | Total Reward: -185.51 | Epsilon: 0.693\n",
            "Episode 324 | Total Reward: -27.34 | Epsilon: 0.692\n",
            "Episode 325 | Total Reward: -154.15 | Epsilon: 0.691\n",
            "Episode 326 | Total Reward: -47.52 | Epsilon: 0.690\n",
            "Episode 327 | Total Reward: -3.88 | Epsilon: 0.689\n",
            "Episode 328 | Total Reward: -247.48 | Epsilon: 0.688\n",
            "Episode 329 | Total Reward: -144.43 | Epsilon: 0.687\n",
            "Episode 330 | Total Reward: -43.00 | Epsilon: 0.686\n",
            "Episode 331 | Total Reward: -12.82 | Epsilon: 0.686\n",
            "Episode 332 | Total Reward: -17.96 | Epsilon: 0.685\n",
            "Episode 333 | Total Reward: -30.56 | Epsilon: 0.684\n",
            "Episode 334 | Total Reward: -38.64 | Epsilon: 0.683\n",
            "Episode 335 | Total Reward: -16.27 | Epsilon: 0.682\n",
            "Episode 336 | Total Reward: -80.76 | Epsilon: 0.681\n",
            "Episode 337 | Total Reward: -5.45 | Epsilon: 0.680\n",
            "Episode 338 | Total Reward: -174.45 | Epsilon: 0.679\n",
            "Episode 339 | Total Reward: -55.08 | Epsilon: 0.678\n",
            "Episode 340 | Total Reward: -33.49 | Epsilon: 0.677\n",
            "Episode 341 | Total Reward: -16.19 | Epsilon: 0.676\n",
            "Episode 342 | Total Reward: -134.85 | Epsilon: 0.675\n",
            "Episode 343 | Total Reward: -142.44 | Epsilon: 0.674\n",
            "Episode 344 | Total Reward: -76.50 | Epsilon: 0.673\n",
            "Episode 345 | Total Reward: -51.01 | Epsilon: 0.672\n",
            "Episode 346 | Total Reward: -66.11 | Epsilon: 0.671\n",
            "Episode 347 | Total Reward: -53.13 | Epsilon: 0.670\n",
            "Episode 348 | Total Reward: -218.02 | Epsilon: 0.669\n",
            "Episode 349 | Total Reward: -107.99 | Epsilon: 0.668\n",
            "Episode 350 | Total Reward: -42.22 | Epsilon: 0.667\n",
            "Episode 351 | Total Reward: -29.43 | Epsilon: 0.667\n",
            "Episode 352 | Total Reward: -115.11 | Epsilon: 0.666\n",
            "Episode 353 | Total Reward: -38.54 | Epsilon: 0.665\n",
            "Episode 354 | Total Reward: -31.10 | Epsilon: 0.664\n",
            "Episode 355 | Total Reward: -17.96 | Epsilon: 0.663\n",
            "Episode 356 | Total Reward: -32.22 | Epsilon: 0.662\n",
            "Episode 357 | Total Reward: -16.41 | Epsilon: 0.661\n",
            "Episode 358 | Total Reward: -10.77 | Epsilon: 0.660\n",
            "Episode 359 | Total Reward: -74.15 | Epsilon: 0.659\n",
            "Episode 360 | Total Reward: -10.22 | Epsilon: 0.658\n",
            "Episode 361 | Total Reward: -5.29 | Epsilon: 0.657\n",
            "Episode 362 | Total Reward: -181.17 | Epsilon: 0.656\n",
            "Episode 363 | Total Reward: -29.59 | Epsilon: 0.655\n",
            "Episode 364 | Total Reward: -110.48 | Epsilon: 0.654\n",
            "Episode 365 | Total Reward: -14.49 | Epsilon: 0.653\n",
            "Episode 366 | Total Reward: -10.45 | Epsilon: 0.652\n",
            "Episode 367 | Total Reward: -15.66 | Epsilon: 0.651\n",
            "Episode 368 | Total Reward: -17.04 | Epsilon: 0.650\n",
            "Episode 369 | Total Reward: -117.51 | Epsilon: 0.649\n",
            "Episode 370 | Total Reward: -17.71 | Epsilon: 0.649\n",
            "Episode 371 | Total Reward: -8.25 | Epsilon: 0.648\n",
            "Episode 372 | Total Reward: -28.52 | Epsilon: 0.647\n",
            "Episode 373 | Total Reward: -60.76 | Epsilon: 0.646\n",
            "Episode 374 | Total Reward: -44.36 | Epsilon: 0.645\n",
            "Episode 375 | Total Reward: -201.23 | Epsilon: 0.644\n",
            "Episode 376 | Total Reward: -3.73 | Epsilon: 0.643\n",
            "Episode 377 | Total Reward: -11.33 | Epsilon: 0.642\n",
            "Episode 378 | Total Reward: -70.55 | Epsilon: 0.641\n",
            "Episode 379 | Total Reward: -19.55 | Epsilon: 0.640\n",
            "Episode 380 | Total Reward: -124.51 | Epsilon: 0.639\n",
            "Episode 381 | Total Reward: -26.89 | Epsilon: 0.638\n",
            "Episode 382 | Total Reward: -15.11 | Epsilon: 0.637\n",
            "Episode 383 | Total Reward: -7.37 | Epsilon: 0.636\n",
            "Episode 384 | Total Reward: -6.92 | Epsilon: 0.635\n",
            "Episode 385 | Total Reward: -18.69 | Epsilon: 0.634\n",
            "Episode 386 | Total Reward: -19.51 | Epsilon: 0.633\n",
            "Episode 387 | Total Reward: -2.62 | Epsilon: 0.632\n",
            "Episode 388 | Total Reward: -20.97 | Epsilon: 0.631\n",
            "Episode 389 | Total Reward: -16.02 | Epsilon: 0.630\n",
            "Episode 390 | Total Reward: -6.71 | Epsilon: 0.629\n",
            "Episode 391 | Total Reward: -11.79 | Epsilon: 0.629\n",
            "Episode 392 | Total Reward: -46.14 | Epsilon: 0.628\n",
            "Episode 393 | Total Reward: -14.48 | Epsilon: 0.627\n",
            "Episode 394 | Total Reward: -20.64 | Epsilon: 0.626\n",
            "Episode 395 | Total Reward: -40.83 | Epsilon: 0.625\n",
            "Episode 396 | Total Reward: -4.32 | Epsilon: 0.624\n",
            "Episode 397 | Total Reward: -14.43 | Epsilon: 0.623\n",
            "Episode 398 | Total Reward: -28.32 | Epsilon: 0.622\n",
            "Episode 399 | Total Reward: -69.92 | Epsilon: 0.621\n",
            "model saved to models/2025-05-09_23-58-54/q_network_ep_0400.pth\n",
            "\n",
            "Episode 400 | Total Reward: -8.31 | Epsilon: 0.620\n",
            "Episode 401 | Total Reward: -43.89 | Epsilon: 0.619\n",
            "Episode 402 | Total Reward: -6.03 | Epsilon: 0.618\n",
            "Episode 403 | Total Reward: -78.60 | Epsilon: 0.617\n",
            "Episode 404 | Total Reward: -3.03 | Epsilon: 0.616\n",
            "Episode 405 | Total Reward: -18.61 | Epsilon: 0.615\n",
            "Episode 406 | Total Reward: -9.68 | Epsilon: 0.614\n",
            "Episode 407 | Total Reward: -15.63 | Epsilon: 0.613\n",
            "Episode 408 | Total Reward: -6.14 | Epsilon: 0.612\n",
            "Episode 409 | Total Reward: -9.66 | Epsilon: 0.611\n",
            "Episode 410 | Total Reward: -23.80 | Epsilon: 0.611\n",
            "Episode 411 | Total Reward: -32.31 | Epsilon: 0.610\n",
            "Episode 412 | Total Reward: -28.97 | Epsilon: 0.609\n",
            "Episode 413 | Total Reward: -16.26 | Epsilon: 0.608\n",
            "Episode 414 | Total Reward: -4.43 | Epsilon: 0.607\n",
            "Episode 415 | Total Reward: -30.46 | Epsilon: 0.606\n",
            "Episode 416 | Total Reward: -8.95 | Epsilon: 0.605\n",
            "Episode 417 | Total Reward: -39.72 | Epsilon: 0.604\n",
            "Episode 418 | Total Reward: -35.79 | Epsilon: 0.603\n",
            "Episode 419 | Total Reward: -4.83 | Epsilon: 0.602\n",
            "Episode 420 | Total Reward: -2.48 | Epsilon: 0.601\n",
            "Episode 421 | Total Reward: -47.74 | Epsilon: 0.600\n",
            "Episode 422 | Total Reward: -16.69 | Epsilon: 0.599\n",
            "Episode 423 | Total Reward: -7.62 | Epsilon: 0.598\n",
            "Episode 424 | Total Reward: -5.56 | Epsilon: 0.597\n",
            "Episode 425 | Total Reward: -15.06 | Epsilon: 0.596\n",
            "Episode 426 | Total Reward: -9.95 | Epsilon: 0.595\n",
            "Episode 427 | Total Reward: -24.62 | Epsilon: 0.594\n",
            "Episode 428 | Total Reward: -26.43 | Epsilon: 0.593\n",
            "Episode 429 | Total Reward: -16.71 | Epsilon: 0.592\n",
            "Episode 430 | Total Reward: -25.47 | Epsilon: 0.592\n",
            "Episode 431 | Total Reward: -4.29 | Epsilon: 0.591\n",
            "Episode 432 | Total Reward: -28.01 | Epsilon: 0.590\n",
            "Episode 433 | Total Reward: -22.40 | Epsilon: 0.589\n",
            "Episode 434 | Total Reward: -28.58 | Epsilon: 0.588\n",
            "Episode 435 | Total Reward: -47.12 | Epsilon: 0.587\n",
            "Episode 436 | Total Reward: -12.65 | Epsilon: 0.586\n",
            "Episode 437 | Total Reward: -38.83 | Epsilon: 0.585\n",
            "Episode 438 | Total Reward: -5.53 | Epsilon: 0.584\n",
            "Episode 439 | Total Reward: -72.99 | Epsilon: 0.583\n",
            "Episode 440 | Total Reward: -6.16 | Epsilon: 0.582\n",
            "Episode 441 | Total Reward: -60.02 | Epsilon: 0.581\n",
            "Episode 442 | Total Reward: -11.69 | Epsilon: 0.580\n",
            "Episode 443 | Total Reward: -2.98 | Epsilon: 0.579\n",
            "Episode 444 | Total Reward: -30.58 | Epsilon: 0.578\n",
            "Episode 445 | Total Reward: -36.49 | Epsilon: 0.577\n",
            "Episode 446 | Total Reward: -1.98 | Epsilon: 0.576\n",
            "Episode 447 | Total Reward: -17.76 | Epsilon: 0.575\n",
            "Episode 448 | Total Reward: -32.67 | Epsilon: 0.574\n",
            "Episode 449 | Total Reward: -62.91 | Epsilon: 0.573\n",
            "Episode 450 | Total Reward: -5.95 | Epsilon: 0.573\n",
            "Episode 451 | Total Reward: -16.27 | Epsilon: 0.572\n",
            "Episode 452 | Total Reward: -40.81 | Epsilon: 0.571\n",
            "Episode 453 | Total Reward: -19.48 | Epsilon: 0.570\n",
            "Episode 454 | Total Reward: -52.22 | Epsilon: 0.569\n",
            "Episode 455 | Total Reward: -32.84 | Epsilon: 0.568\n",
            "Episode 456 | Total Reward: -24.93 | Epsilon: 0.567\n",
            "Episode 457 | Total Reward: -11.88 | Epsilon: 0.566\n",
            "Episode 458 | Total Reward: -51.34 | Epsilon: 0.565\n",
            "Episode 459 | Total Reward: -5.70 | Epsilon: 0.564\n",
            "Episode 460 | Total Reward: -3.90 | Epsilon: 0.563\n",
            "Episode 461 | Total Reward: -32.35 | Epsilon: 0.562\n",
            "Episode 462 | Total Reward: -11.31 | Epsilon: 0.561\n",
            "Episode 463 | Total Reward: -29.87 | Epsilon: 0.560\n",
            "Episode 464 | Total Reward: -19.78 | Epsilon: 0.559\n",
            "Episode 465 | Total Reward: -4.09 | Epsilon: 0.558\n",
            "Episode 466 | Total Reward: -23.62 | Epsilon: 0.557\n",
            "Episode 467 | Total Reward: -11.31 | Epsilon: 0.556\n",
            "Episode 468 | Total Reward: -30.88 | Epsilon: 0.555\n",
            "Episode 469 | Total Reward: -21.01 | Epsilon: 0.554\n",
            "Episode 470 | Total Reward: -11.06 | Epsilon: 0.553\n",
            "Episode 471 | Total Reward: -65.16 | Epsilon: 0.553\n",
            "Episode 472 | Total Reward: -21.46 | Epsilon: 0.552\n",
            "Episode 473 | Total Reward: -20.15 | Epsilon: 0.551\n",
            "Episode 474 | Total Reward: -9.16 | Epsilon: 0.550\n",
            "Episode 475 | Total Reward: -36.32 | Epsilon: 0.549\n",
            "Episode 476 | Total Reward: -33.64 | Epsilon: 0.548\n",
            "Episode 477 | Total Reward: -18.60 | Epsilon: 0.547\n",
            "Episode 478 | Total Reward: -14.01 | Epsilon: 0.546\n",
            "Episode 479 | Total Reward: -59.41 | Epsilon: 0.545\n",
            "Episode 480 | Total Reward: -9.86 | Epsilon: 0.544\n",
            "Episode 481 | Total Reward: -33.49 | Epsilon: 0.543\n",
            "Episode 482 | Total Reward: -16.21 | Epsilon: 0.542\n",
            "Episode 483 | Total Reward: -12.96 | Epsilon: 0.541\n",
            "Episode 484 | Total Reward: -24.44 | Epsilon: 0.540\n",
            "Episode 485 | Total Reward: -37.02 | Epsilon: 0.539\n",
            "Episode 486 | Total Reward: -25.03 | Epsilon: 0.538\n",
            "Episode 487 | Total Reward: -16.13 | Epsilon: 0.537\n",
            "Episode 488 | Total Reward: -37.20 | Epsilon: 0.536\n",
            "Episode 489 | Total Reward: -8.38 | Epsilon: 0.535\n",
            "Episode 490 | Total Reward: -7.17 | Epsilon: 0.534\n",
            "Episode 491 | Total Reward: -2.63 | Epsilon: 0.534\n",
            "Episode 492 | Total Reward: -25.61 | Epsilon: 0.533\n",
            "Episode 493 | Total Reward: -50.51 | Epsilon: 0.532\n",
            "Episode 494 | Total Reward: -45.96 | Epsilon: 0.531\n",
            "Episode 495 | Total Reward: -7.90 | Epsilon: 0.530\n",
            "Episode 496 | Total Reward: -3.95 | Epsilon: 0.529\n",
            "Episode 497 | Total Reward: -21.40 | Epsilon: 0.528\n",
            "Episode 498 | Total Reward: -6.16 | Epsilon: 0.527\n",
            "Episode 499 | Total Reward: -6.54 | Epsilon: 0.526\n",
            "model saved to models/2025-05-10_00-00-58/q_network_ep_0500.pth\n",
            "\n",
            "Episode 500 | Total Reward: -18.00 | Epsilon: 0.525\n",
            "Episode 501 | Total Reward: -47.49 | Epsilon: 0.524\n",
            "Episode 502 | Total Reward: -16.83 | Epsilon: 0.523\n",
            "Episode 503 | Total Reward: -10.14 | Epsilon: 0.522\n",
            "Episode 504 | Total Reward: -9.82 | Epsilon: 0.521\n",
            "Episode 505 | Total Reward: -23.46 | Epsilon: 0.520\n",
            "Episode 506 | Total Reward: -16.36 | Epsilon: 0.519\n",
            "Episode 507 | Total Reward: -30.78 | Epsilon: 0.518\n",
            "Episode 508 | Total Reward: -17.80 | Epsilon: 0.517\n",
            "Episode 509 | Total Reward: -26.25 | Epsilon: 0.516\n",
            "Episode 510 | Total Reward: -39.88 | Epsilon: 0.516\n",
            "Episode 511 | Total Reward: -17.07 | Epsilon: 0.515\n",
            "Episode 512 | Total Reward: -8.89 | Epsilon: 0.514\n",
            "Episode 513 | Total Reward: -45.34 | Epsilon: 0.513\n",
            "Episode 514 | Total Reward: -7.35 | Epsilon: 0.512\n",
            "Episode 515 | Total Reward: -8.24 | Epsilon: 0.511\n",
            "Episode 516 | Total Reward: -16.90 | Epsilon: 0.510\n",
            "Episode 517 | Total Reward: -19.15 | Epsilon: 0.509\n",
            "Episode 518 | Total Reward: -20.99 | Epsilon: 0.508\n",
            "Episode 519 | Total Reward: -15.81 | Epsilon: 0.507\n",
            "Episode 520 | Total Reward: -15.42 | Epsilon: 0.506\n",
            "Episode 521 | Total Reward: -30.84 | Epsilon: 0.505\n",
            "Episode 522 | Total Reward: -8.99 | Epsilon: 0.504\n",
            "Episode 523 | Total Reward: -12.63 | Epsilon: 0.503\n",
            "Episode 524 | Total Reward: -38.14 | Epsilon: 0.502\n",
            "Episode 525 | Total Reward: -25.66 | Epsilon: 0.501\n",
            "Episode 526 | Total Reward: -1.99 | Epsilon: 0.500\n",
            "Episode 527 | Total Reward: -12.24 | Epsilon: 0.499\n",
            "Episode 528 | Total Reward: -12.44 | Epsilon: 0.498\n",
            "Episode 529 | Total Reward: -23.86 | Epsilon: 0.497\n",
            "Episode 530 | Total Reward: -4.22 | Epsilon: 0.497\n",
            "Episode 531 | Total Reward: -11.84 | Epsilon: 0.496\n",
            "Episode 532 | Total Reward: -10.41 | Epsilon: 0.495\n",
            "Episode 533 | Total Reward: -23.19 | Epsilon: 0.494\n",
            "Episode 534 | Total Reward: -3.24 | Epsilon: 0.493\n",
            "Episode 535 | Total Reward: -11.98 | Epsilon: 0.492\n",
            "Episode 536 | Total Reward: -30.18 | Epsilon: 0.491\n",
            "Episode 537 | Total Reward: -7.67 | Epsilon: 0.490\n",
            "Episode 538 | Total Reward: -10.18 | Epsilon: 0.489\n",
            "Episode 539 | Total Reward: -17.28 | Epsilon: 0.488\n",
            "Episode 540 | Total Reward: -49.63 | Epsilon: 0.487\n",
            "Episode 541 | Total Reward: -2.56 | Epsilon: 0.486\n",
            "Episode 542 | Total Reward: -4.77 | Epsilon: 0.485\n",
            "Episode 543 | Total Reward: -31.15 | Epsilon: 0.484\n",
            "Episode 544 | Total Reward: -45.03 | Epsilon: 0.483\n",
            "Episode 545 | Total Reward: -27.28 | Epsilon: 0.482\n",
            "Episode 546 | Total Reward: -14.58 | Epsilon: 0.481\n",
            "Episode 547 | Total Reward: -9.04 | Epsilon: 0.480\n",
            "Episode 548 | Total Reward: -7.01 | Epsilon: 0.479\n",
            "Episode 549 | Total Reward: -7.22 | Epsilon: 0.478\n",
            "Episode 550 | Total Reward: -13.70 | Epsilon: 0.478\n",
            "Episode 551 | Total Reward: -13.07 | Epsilon: 0.477\n",
            "Episode 552 | Total Reward: -9.15 | Epsilon: 0.476\n",
            "Episode 553 | Total Reward: -12.82 | Epsilon: 0.475\n",
            "Episode 554 | Total Reward: -18.81 | Epsilon: 0.474\n",
            "Episode 555 | Total Reward: -11.48 | Epsilon: 0.473\n",
            "Episode 556 | Total Reward: -9.52 | Epsilon: 0.472\n",
            "Episode 557 | Total Reward: -25.20 | Epsilon: 0.471\n",
            "Episode 558 | Total Reward: -35.11 | Epsilon: 0.470\n",
            "Episode 559 | Total Reward: -12.01 | Epsilon: 0.469\n",
            "Episode 560 | Total Reward: -7.11 | Epsilon: 0.468\n",
            "Episode 561 | Total Reward: -16.55 | Epsilon: 0.467\n",
            "Episode 562 | Total Reward: -2.25 | Epsilon: 0.466\n",
            "Episode 563 | Total Reward: -10.02 | Epsilon: 0.465\n",
            "Episode 564 | Total Reward: -28.57 | Epsilon: 0.464\n",
            "Episode 565 | Total Reward: -12.60 | Epsilon: 0.463\n",
            "Episode 566 | Total Reward: -8.93 | Epsilon: 0.462\n",
            "Episode 567 | Total Reward: -6.13 | Epsilon: 0.461\n",
            "Episode 568 | Total Reward: -8.22 | Epsilon: 0.460\n",
            "Episode 569 | Total Reward: -12.41 | Epsilon: 0.459\n",
            "Episode 570 | Total Reward: -8.15 | Epsilon: 0.459\n",
            "Episode 571 | Total Reward: -21.09 | Epsilon: 0.458\n",
            "Episode 572 | Total Reward: -7.51 | Epsilon: 0.457\n",
            "Episode 573 | Total Reward: -13.53 | Epsilon: 0.456\n",
            "Episode 574 | Total Reward: -17.05 | Epsilon: 0.455\n",
            "Episode 575 | Total Reward: -29.39 | Epsilon: 0.454\n",
            "Episode 576 | Total Reward: -9.99 | Epsilon: 0.453\n",
            "Episode 577 | Total Reward: -34.98 | Epsilon: 0.452\n",
            "Episode 578 | Total Reward: -27.04 | Epsilon: 0.451\n",
            "Episode 579 | Total Reward: -14.51 | Epsilon: 0.450\n",
            "Episode 580 | Total Reward: -8.21 | Epsilon: 0.449\n",
            "Episode 581 | Total Reward: -9.01 | Epsilon: 0.448\n",
            "Episode 582 | Total Reward: -24.87 | Epsilon: 0.447\n",
            "Episode 583 | Total Reward: -13.04 | Epsilon: 0.446\n",
            "Episode 584 | Total Reward: -23.90 | Epsilon: 0.445\n",
            "Episode 585 | Total Reward: -10.11 | Epsilon: 0.444\n",
            "Episode 586 | Total Reward: -7.40 | Epsilon: 0.443\n",
            "Episode 587 | Total Reward: -10.19 | Epsilon: 0.442\n",
            "Episode 588 | Total Reward: -20.95 | Epsilon: 0.441\n",
            "Episode 589 | Total Reward: -18.29 | Epsilon: 0.440\n",
            "Episode 590 | Total Reward: -16.54 | Epsilon: 0.440\n",
            "Episode 591 | Total Reward: -7.64 | Epsilon: 0.439\n",
            "Episode 592 | Total Reward: -20.71 | Epsilon: 0.438\n",
            "Episode 593 | Total Reward: -18.57 | Epsilon: 0.437\n",
            "Episode 594 | Total Reward: -9.81 | Epsilon: 0.436\n",
            "Episode 595 | Total Reward: -10.40 | Epsilon: 0.435\n",
            "Episode 596 | Total Reward: -35.57 | Epsilon: 0.434\n",
            "Episode 597 | Total Reward: -18.50 | Epsilon: 0.433\n",
            "Episode 598 | Total Reward: -7.55 | Epsilon: 0.432\n",
            "Episode 599 | Total Reward: -37.14 | Epsilon: 0.431\n",
            "model saved to models/2025-05-10_00-02-56/q_network_ep_0600.pth\n",
            "\n",
            "Episode 600 | Total Reward: -6.45 | Epsilon: 0.430\n",
            "Episode 601 | Total Reward: -3.06 | Epsilon: 0.429\n",
            "Episode 602 | Total Reward: -22.11 | Epsilon: 0.428\n",
            "Episode 603 | Total Reward: -39.08 | Epsilon: 0.427\n",
            "Episode 604 | Total Reward: -4.99 | Epsilon: 0.426\n",
            "Episode 605 | Total Reward: -48.72 | Epsilon: 0.425\n",
            "Episode 606 | Total Reward: -5.64 | Epsilon: 0.424\n",
            "Episode 607 | Total Reward: -11.19 | Epsilon: 0.423\n",
            "Episode 608 | Total Reward: -8.55 | Epsilon: 0.422\n",
            "Episode 609 | Total Reward: -4.19 | Epsilon: 0.421\n",
            "Episode 610 | Total Reward: -20.66 | Epsilon: 0.420\n",
            "Episode 611 | Total Reward: -21.49 | Epsilon: 0.420\n",
            "Episode 612 | Total Reward: -30.62 | Epsilon: 0.419\n",
            "Episode 613 | Total Reward: -10.60 | Epsilon: 0.418\n",
            "Episode 614 | Total Reward: -13.42 | Epsilon: 0.417\n",
            "Episode 615 | Total Reward: -9.68 | Epsilon: 0.416\n",
            "Episode 616 | Total Reward: -35.02 | Epsilon: 0.415\n",
            "Episode 617 | Total Reward: -3.65 | Epsilon: 0.414\n",
            "Episode 618 | Total Reward: -7.00 | Epsilon: 0.413\n",
            "Episode 619 | Total Reward: -4.67 | Epsilon: 0.412\n",
            "Episode 620 | Total Reward: -9.74 | Epsilon: 0.411\n",
            "Episode 621 | Total Reward: -5.93 | Epsilon: 0.410\n",
            "Episode 622 | Total Reward: -17.99 | Epsilon: 0.409\n",
            "Episode 623 | Total Reward: -8.23 | Epsilon: 0.408\n",
            "Episode 624 | Total Reward: -5.27 | Epsilon: 0.407\n",
            "Episode 625 | Total Reward: -18.16 | Epsilon: 0.406\n",
            "Episode 626 | Total Reward: -25.28 | Epsilon: 0.405\n",
            "Episode 627 | Total Reward: -9.00 | Epsilon: 0.404\n",
            "Episode 628 | Total Reward: -15.92 | Epsilon: 0.403\n",
            "Episode 629 | Total Reward: -5.81 | Epsilon: 0.402\n",
            "Episode 630 | Total Reward: -5.12 | Epsilon: 0.401\n",
            "Episode 631 | Total Reward: -3.07 | Epsilon: 0.401\n",
            "Episode 632 | Total Reward: -22.61 | Epsilon: 0.400\n",
            "Episode 633 | Total Reward: -19.88 | Epsilon: 0.399\n",
            "Episode 634 | Total Reward: -27.05 | Epsilon: 0.398\n",
            "Episode 635 | Total Reward: -17.28 | Epsilon: 0.397\n",
            "Episode 636 | Total Reward: -8.64 | Epsilon: 0.396\n",
            "Episode 637 | Total Reward: -31.53 | Epsilon: 0.395\n",
            "Episode 638 | Total Reward: -15.45 | Epsilon: 0.394\n",
            "Episode 639 | Total Reward: -22.62 | Epsilon: 0.393\n",
            "Episode 640 | Total Reward: -5.60 | Epsilon: 0.392\n",
            "Episode 641 | Total Reward: -3.06 | Epsilon: 0.391\n",
            "Episode 642 | Total Reward: -29.40 | Epsilon: 0.390\n",
            "Episode 643 | Total Reward: -8.24 | Epsilon: 0.389\n",
            "Episode 644 | Total Reward: -3.99 | Epsilon: 0.388\n",
            "Episode 645 | Total Reward: -19.40 | Epsilon: 0.387\n",
            "Episode 646 | Total Reward: -6.56 | Epsilon: 0.386\n",
            "Episode 647 | Total Reward: -4.64 | Epsilon: 0.385\n",
            "Episode 648 | Total Reward: -35.06 | Epsilon: 0.384\n",
            "Episode 649 | Total Reward: -20.05 | Epsilon: 0.383\n",
            "Episode 650 | Total Reward: -18.43 | Epsilon: 0.382\n",
            "Episode 651 | Total Reward: -2.67 | Epsilon: 0.382\n",
            "Episode 652 | Total Reward: -12.77 | Epsilon: 0.381\n",
            "Episode 653 | Total Reward: -27.68 | Epsilon: 0.380\n",
            "Episode 654 | Total Reward: -35.44 | Epsilon: 0.379\n",
            "Episode 655 | Total Reward: -2.13 | Epsilon: 0.378\n",
            "Episode 656 | Total Reward: -33.85 | Epsilon: 0.377\n",
            "Episode 657 | Total Reward: -21.88 | Epsilon: 0.376\n",
            "Episode 658 | Total Reward: -12.25 | Epsilon: 0.375\n",
            "Episode 659 | Total Reward: -4.89 | Epsilon: 0.374\n",
            "Episode 660 | Total Reward: -6.59 | Epsilon: 0.373\n",
            "Episode 661 | Total Reward: -34.37 | Epsilon: 0.372\n",
            "Episode 662 | Total Reward: -9.35 | Epsilon: 0.371\n",
            "Episode 663 | Total Reward: -13.30 | Epsilon: 0.370\n",
            "Episode 664 | Total Reward: -4.82 | Epsilon: 0.369\n",
            "Episode 665 | Total Reward: -19.65 | Epsilon: 0.368\n",
            "Episode 666 | Total Reward: -21.44 | Epsilon: 0.367\n",
            "Episode 667 | Total Reward: -35.56 | Epsilon: 0.366\n",
            "Episode 668 | Total Reward: -5.23 | Epsilon: 0.365\n",
            "Episode 669 | Total Reward: -4.55 | Epsilon: 0.364\n",
            "Episode 670 | Total Reward: -12.77 | Epsilon: 0.364\n",
            "Episode 671 | Total Reward: -7.87 | Epsilon: 0.363\n",
            "Episode 672 | Total Reward: -6.02 | Epsilon: 0.362\n",
            "Episode 673 | Total Reward: -26.12 | Epsilon: 0.361\n",
            "Episode 674 | Total Reward: -20.99 | Epsilon: 0.360\n",
            "Episode 675 | Total Reward: -3.44 | Epsilon: 0.359\n",
            "Episode 676 | Total Reward: -26.09 | Epsilon: 0.358\n",
            "Episode 677 | Total Reward: -1.74 | Epsilon: 0.357\n",
            "Episode 678 | Total Reward: -29.09 | Epsilon: 0.356\n",
            "Episode 679 | Total Reward: -4.40 | Epsilon: 0.355\n",
            "Episode 680 | Total Reward: -5.94 | Epsilon: 0.354\n",
            "Episode 681 | Total Reward: -4.96 | Epsilon: 0.353\n",
            "Episode 682 | Total Reward: -4.69 | Epsilon: 0.352\n",
            "Episode 683 | Total Reward: -18.40 | Epsilon: 0.351\n",
            "Episode 684 | Total Reward: -2.72 | Epsilon: 0.350\n",
            "Episode 685 | Total Reward: -31.57 | Epsilon: 0.349\n",
            "Episode 686 | Total Reward: -4.72 | Epsilon: 0.348\n",
            "Episode 687 | Total Reward: -6.02 | Epsilon: 0.347\n",
            "Episode 688 | Total Reward: -19.21 | Epsilon: 0.346\n",
            "Episode 689 | Total Reward: -9.18 | Epsilon: 0.345\n",
            "Episode 690 | Total Reward: -8.64 | Epsilon: 0.345\n",
            "Episode 691 | Total Reward: -12.98 | Epsilon: 0.344\n",
            "Episode 692 | Total Reward: -7.73 | Epsilon: 0.343\n",
            "Episode 693 | Total Reward: -35.15 | Epsilon: 0.342\n",
            "Episode 694 | Total Reward: -19.48 | Epsilon: 0.341\n",
            "Episode 695 | Total Reward: -10.46 | Epsilon: 0.340\n",
            "Episode 696 | Total Reward: -3.00 | Epsilon: 0.339\n",
            "Episode 697 | Total Reward: -9.26 | Epsilon: 0.338\n",
            "Episode 698 | Total Reward: -18.92 | Epsilon: 0.337\n",
            "Episode 699 | Total Reward: -9.66 | Epsilon: 0.336\n",
            "model saved to models/2025-05-10_00-04-49/q_network_ep_0700.pth\n",
            "\n",
            "Episode 700 | Total Reward: -28.78 | Epsilon: 0.335\n",
            "Episode 701 | Total Reward: -22.33 | Epsilon: 0.334\n",
            "Episode 702 | Total Reward: -12.72 | Epsilon: 0.333\n",
            "Episode 703 | Total Reward: -21.98 | Epsilon: 0.332\n",
            "Episode 704 | Total Reward: -7.37 | Epsilon: 0.331\n",
            "Episode 705 | Total Reward: -4.18 | Epsilon: 0.330\n",
            "Episode 706 | Total Reward: -32.82 | Epsilon: 0.329\n",
            "Episode 707 | Total Reward: -25.64 | Epsilon: 0.328\n",
            "Episode 708 | Total Reward: -19.88 | Epsilon: 0.327\n",
            "Episode 709 | Total Reward: -8.85 | Epsilon: 0.326\n",
            "Episode 710 | Total Reward: -16.44 | Epsilon: 0.326\n",
            "Episode 711 | Total Reward: -12.44 | Epsilon: 0.325\n",
            "Episode 712 | Total Reward: -22.74 | Epsilon: 0.324\n",
            "Episode 713 | Total Reward: -12.28 | Epsilon: 0.323\n",
            "Episode 714 | Total Reward: -10.66 | Epsilon: 0.322\n",
            "Episode 715 | Total Reward: -4.92 | Epsilon: 0.321\n",
            "Episode 716 | Total Reward: -17.45 | Epsilon: 0.320\n",
            "Episode 717 | Total Reward: -21.23 | Epsilon: 0.319\n",
            "Episode 718 | Total Reward: -4.24 | Epsilon: 0.318\n",
            "Episode 719 | Total Reward: -8.27 | Epsilon: 0.317\n",
            "Episode 720 | Total Reward: -6.67 | Epsilon: 0.316\n",
            "Episode 721 | Total Reward: -18.45 | Epsilon: 0.315\n",
            "Episode 722 | Total Reward: -12.88 | Epsilon: 0.314\n",
            "Episode 723 | Total Reward: -9.54 | Epsilon: 0.313\n",
            "Episode 724 | Total Reward: -8.18 | Epsilon: 0.312\n",
            "Episode 725 | Total Reward: -31.42 | Epsilon: 0.311\n",
            "Episode 726 | Total Reward: -5.18 | Epsilon: 0.310\n",
            "Episode 727 | Total Reward: -3.79 | Epsilon: 0.309\n",
            "Episode 728 | Total Reward: -30.70 | Epsilon: 0.308\n",
            "Episode 729 | Total Reward: -4.94 | Epsilon: 0.307\n",
            "Episode 730 | Total Reward: -6.07 | Epsilon: 0.306\n",
            "Episode 731 | Total Reward: -18.07 | Epsilon: 0.306\n",
            "Episode 732 | Total Reward: -30.42 | Epsilon: 0.305\n",
            "Episode 733 | Total Reward: -3.43 | Epsilon: 0.304\n",
            "Episode 734 | Total Reward: -8.20 | Epsilon: 0.303\n",
            "Episode 735 | Total Reward: -27.66 | Epsilon: 0.302\n",
            "Episode 736 | Total Reward: -2.29 | Epsilon: 0.301\n",
            "Episode 737 | Total Reward: -35.52 | Epsilon: 0.300\n",
            "Episode 738 | Total Reward: -2.32 | Epsilon: 0.299\n",
            "Episode 739 | Total Reward: -1.62 | Epsilon: 0.298\n",
            "Episode 740 | Total Reward: -21.07 | Epsilon: 0.297\n",
            "Episode 741 | Total Reward: -3.45 | Epsilon: 0.296\n",
            "Episode 742 | Total Reward: -18.73 | Epsilon: 0.295\n",
            "Episode 743 | Total Reward: -13.77 | Epsilon: 0.294\n",
            "Episode 744 | Total Reward: -35.09 | Epsilon: 0.293\n",
            "Episode 745 | Total Reward: -25.54 | Epsilon: 0.292\n",
            "Episode 746 | Total Reward: -18.01 | Epsilon: 0.291\n",
            "Episode 747 | Total Reward: -14.02 | Epsilon: 0.290\n",
            "Episode 748 | Total Reward: -3.18 | Epsilon: 0.289\n",
            "Episode 749 | Total Reward: -27.60 | Epsilon: 0.288\n",
            "Episode 750 | Total Reward: -16.23 | Epsilon: 0.287\n",
            "Episode 751 | Total Reward: -13.86 | Epsilon: 0.287\n",
            "Episode 752 | Total Reward: -8.09 | Epsilon: 0.286\n",
            "Episode 753 | Total Reward: -19.56 | Epsilon: 0.285\n",
            "Episode 754 | Total Reward: -15.84 | Epsilon: 0.284\n",
            "Episode 755 | Total Reward: -11.34 | Epsilon: 0.283\n",
            "Episode 756 | Total Reward: -4.01 | Epsilon: 0.282\n",
            "Episode 757 | Total Reward: -1.94 | Epsilon: 0.281\n",
            "Episode 758 | Total Reward: -4.22 | Epsilon: 0.280\n",
            "Episode 759 | Total Reward: -38.10 | Epsilon: 0.279\n",
            "Episode 760 | Total Reward: -12.68 | Epsilon: 0.278\n",
            "Episode 761 | Total Reward: -19.67 | Epsilon: 0.277\n",
            "Episode 762 | Total Reward: -33.27 | Epsilon: 0.276\n",
            "Episode 763 | Total Reward: -6.67 | Epsilon: 0.275\n",
            "Episode 764 | Total Reward: -12.39 | Epsilon: 0.274\n",
            "Episode 765 | Total Reward: -14.31 | Epsilon: 0.273\n",
            "Episode 766 | Total Reward: -9.23 | Epsilon: 0.272\n",
            "Episode 767 | Total Reward: -19.30 | Epsilon: 0.271\n",
            "Episode 768 | Total Reward: -6.46 | Epsilon: 0.270\n",
            "Episode 769 | Total Reward: -7.46 | Epsilon: 0.269\n",
            "Episode 770 | Total Reward: -23.21 | Epsilon: 0.268\n",
            "Episode 771 | Total Reward: -9.60 | Epsilon: 0.268\n",
            "Episode 772 | Total Reward: -18.42 | Epsilon: 0.267\n",
            "Episode 773 | Total Reward: -7.33 | Epsilon: 0.266\n",
            "Episode 774 | Total Reward: -2.42 | Epsilon: 0.265\n",
            "Episode 775 | Total Reward: -5.71 | Epsilon: 0.264\n",
            "Episode 776 | Total Reward: -7.42 | Epsilon: 0.263\n",
            "Episode 777 | Total Reward: -12.95 | Epsilon: 0.262\n",
            "Episode 778 | Total Reward: -13.98 | Epsilon: 0.261\n",
            "Episode 779 | Total Reward: -12.59 | Epsilon: 0.260\n",
            "Episode 780 | Total Reward: -36.54 | Epsilon: 0.259\n",
            "Episode 781 | Total Reward: -15.50 | Epsilon: 0.258\n",
            "Episode 782 | Total Reward: -2.01 | Epsilon: 0.257\n",
            "Episode 783 | Total Reward: -19.20 | Epsilon: 0.256\n",
            "Episode 784 | Total Reward: -23.40 | Epsilon: 0.255\n",
            "Episode 785 | Total Reward: -20.37 | Epsilon: 0.254\n",
            "Episode 786 | Total Reward: -3.58 | Epsilon: 0.253\n",
            "Episode 787 | Total Reward: -1.14 | Epsilon: 0.252\n",
            "Episode 788 | Total Reward: -21.48 | Epsilon: 0.251\n",
            "Episode 789 | Total Reward: -1.53 | Epsilon: 0.250\n",
            "Episode 790 | Total Reward: -12.00 | Epsilon: 0.250\n",
            "Episode 791 | Total Reward: -8.68 | Epsilon: 0.249\n",
            "Episode 792 | Total Reward: -6.11 | Epsilon: 0.248\n",
            "Episode 793 | Total Reward: -25.44 | Epsilon: 0.247\n",
            "Episode 794 | Total Reward: -5.73 | Epsilon: 0.246\n",
            "Episode 795 | Total Reward: -23.74 | Epsilon: 0.245\n",
            "Episode 796 | Total Reward: -2.38 | Epsilon: 0.244\n",
            "Episode 797 | Total Reward: -35.36 | Epsilon: 0.243\n",
            "Episode 798 | Total Reward: -6.82 | Epsilon: 0.242\n",
            "Episode 799 | Total Reward: -2.19 | Epsilon: 0.241\n",
            "model saved to models/2025-05-10_00-06-39/q_network_ep_0800.pth\n",
            "\n",
            "Episode 800 | Total Reward: -6.97 | Epsilon: 0.240\n",
            "Episode 801 | Total Reward: -4.50 | Epsilon: 0.239\n",
            "Episode 802 | Total Reward: -2.95 | Epsilon: 0.238\n",
            "Episode 803 | Total Reward: -23.44 | Epsilon: 0.237\n",
            "Episode 804 | Total Reward: -0.84 | Epsilon: 0.236\n",
            "Episode 805 | Total Reward: -2.66 | Epsilon: 0.235\n",
            "Episode 806 | Total Reward: -29.83 | Epsilon: 0.234\n",
            "Episode 807 | Total Reward: -15.49 | Epsilon: 0.233\n",
            "Episode 808 | Total Reward: -14.23 | Epsilon: 0.232\n",
            "Episode 809 | Total Reward: -2.30 | Epsilon: 0.231\n",
            "Episode 810 | Total Reward: -19.19 | Epsilon: 0.231\n",
            "Episode 811 | Total Reward: -3.66 | Epsilon: 0.230\n",
            "Episode 812 | Total Reward: -16.02 | Epsilon: 0.229\n",
            "Episode 813 | Total Reward: -6.40 | Epsilon: 0.228\n",
            "Episode 814 | Total Reward: -2.88 | Epsilon: 0.227\n",
            "Episode 815 | Total Reward: -19.66 | Epsilon: 0.226\n",
            "Episode 816 | Total Reward: -24.69 | Epsilon: 0.225\n",
            "Episode 817 | Total Reward: -3.95 | Epsilon: 0.224\n",
            "Episode 818 | Total Reward: -6.00 | Epsilon: 0.223\n",
            "Episode 819 | Total Reward: -6.90 | Epsilon: 0.222\n",
            "Episode 820 | Total Reward: -1.81 | Epsilon: 0.221\n",
            "Episode 821 | Total Reward: -24.94 | Epsilon: 0.220\n",
            "Episode 822 | Total Reward: -3.37 | Epsilon: 0.219\n",
            "Episode 823 | Total Reward: -3.14 | Epsilon: 0.218\n",
            "Episode 824 | Total Reward: -16.63 | Epsilon: 0.217\n",
            "Episode 825 | Total Reward: -6.82 | Epsilon: 0.216\n",
            "Episode 826 | Total Reward: -10.46 | Epsilon: 0.215\n",
            "Episode 827 | Total Reward: -2.86 | Epsilon: 0.214\n",
            "Episode 828 | Total Reward: -5.92 | Epsilon: 0.213\n",
            "Episode 829 | Total Reward: -8.54 | Epsilon: 0.212\n",
            "Episode 830 | Total Reward: -3.21 | Epsilon: 0.212\n",
            "Episode 831 | Total Reward: -13.65 | Epsilon: 0.211\n",
            "Episode 832 | Total Reward: -14.31 | Epsilon: 0.210\n",
            "Episode 833 | Total Reward: -12.26 | Epsilon: 0.209\n",
            "Episode 834 | Total Reward: -7.93 | Epsilon: 0.208\n",
            "Episode 835 | Total Reward: -3.03 | Epsilon: 0.207\n",
            "Episode 836 | Total Reward: -23.86 | Epsilon: 0.206\n",
            "Episode 837 | Total Reward: -4.11 | Epsilon: 0.205\n",
            "Episode 838 | Total Reward: -24.30 | Epsilon: 0.204\n",
            "Episode 839 | Total Reward: -9.96 | Epsilon: 0.203\n",
            "Episode 840 | Total Reward: -3.62 | Epsilon: 0.202\n",
            "Episode 841 | Total Reward: -13.36 | Epsilon: 0.201\n",
            "Episode 842 | Total Reward: -5.35 | Epsilon: 0.200\n",
            "Episode 843 | Total Reward: -29.01 | Epsilon: 0.199\n",
            "Episode 844 | Total Reward: -18.37 | Epsilon: 0.198\n",
            "Episode 845 | Total Reward: -6.23 | Epsilon: 0.197\n",
            "Episode 846 | Total Reward: -4.66 | Epsilon: 0.196\n",
            "Episode 847 | Total Reward: -33.92 | Epsilon: 0.195\n",
            "Episode 848 | Total Reward: -22.00 | Epsilon: 0.194\n",
            "Episode 849 | Total Reward: -8.22 | Epsilon: 0.193\n",
            "Episode 850 | Total Reward: -16.12 | Epsilon: 0.193\n",
            "Episode 851 | Total Reward: -223.64 | Epsilon: 0.192\n",
            "Episode 852 | Total Reward: -27.93 | Epsilon: 0.191\n",
            "Episode 853 | Total Reward: -26.57 | Epsilon: 0.190\n",
            "Episode 854 | Total Reward: -12.28 | Epsilon: 0.189\n",
            "Episode 855 | Total Reward: -20.73 | Epsilon: 0.188\n",
            "Episode 856 | Total Reward: -3.19 | Epsilon: 0.187\n",
            "Episode 857 | Total Reward: -2.62 | Epsilon: 0.186\n",
            "Episode 858 | Total Reward: -23.40 | Epsilon: 0.185\n",
            "Episode 859 | Total Reward: -4.26 | Epsilon: 0.184\n",
            "Episode 860 | Total Reward: -3.16 | Epsilon: 0.183\n",
            "Episode 861 | Total Reward: -82.52 | Epsilon: 0.182\n",
            "Episode 862 | Total Reward: -4.76 | Epsilon: 0.181\n",
            "Episode 863 | Total Reward: -138.63 | Epsilon: 0.180\n",
            "Episode 864 | Total Reward: -3.32 | Epsilon: 0.179\n",
            "Episode 865 | Total Reward: -17.45 | Epsilon: 0.178\n",
            "Episode 866 | Total Reward: -17.64 | Epsilon: 0.177\n",
            "Episode 867 | Total Reward: -19.99 | Epsilon: 0.176\n",
            "Episode 868 | Total Reward: -2.61 | Epsilon: 0.175\n",
            "Episode 869 | Total Reward: -8.40 | Epsilon: 0.174\n",
            "Episode 870 | Total Reward: -6.48 | Epsilon: 0.173\n",
            "Episode 871 | Total Reward: -17.25 | Epsilon: 0.173\n",
            "Episode 872 | Total Reward: -26.53 | Epsilon: 0.172\n",
            "Episode 873 | Total Reward: -2.59 | Epsilon: 0.171\n",
            "Episode 874 | Total Reward: -13.92 | Epsilon: 0.170\n",
            "Episode 875 | Total Reward: -12.11 | Epsilon: 0.169\n",
            "Episode 876 | Total Reward: -10.39 | Epsilon: 0.168\n",
            "Episode 877 | Total Reward: -17.59 | Epsilon: 0.167\n",
            "Episode 878 | Total Reward: -17.03 | Epsilon: 0.166\n",
            "Episode 879 | Total Reward: -4.73 | Epsilon: 0.165\n",
            "Episode 880 | Total Reward: -6.04 | Epsilon: 0.164\n",
            "Episode 881 | Total Reward: -18.11 | Epsilon: 0.163\n",
            "Episode 882 | Total Reward: -2.96 | Epsilon: 0.162\n",
            "Episode 883 | Total Reward: -21.27 | Epsilon: 0.161\n",
            "Episode 884 | Total Reward: -3.96 | Epsilon: 0.160\n",
            "Episode 885 | Total Reward: -1.94 | Epsilon: 0.159\n",
            "Episode 886 | Total Reward: -5.28 | Epsilon: 0.158\n",
            "Episode 887 | Total Reward: -5.76 | Epsilon: 0.157\n",
            "Episode 888 | Total Reward: -14.69 | Epsilon: 0.156\n",
            "Episode 889 | Total Reward: -2.31 | Epsilon: 0.155\n",
            "Episode 890 | Total Reward: -36.22 | Epsilon: 0.154\n",
            "Episode 891 | Total Reward: -5.41 | Epsilon: 0.154\n",
            "Episode 892 | Total Reward: -25.93 | Epsilon: 0.153\n",
            "Episode 893 | Total Reward: -25.65 | Epsilon: 0.152\n",
            "Episode 894 | Total Reward: -27.74 | Epsilon: 0.151\n",
            "Episode 895 | Total Reward: -13.24 | Epsilon: 0.150\n",
            "Episode 896 | Total Reward: -22.74 | Epsilon: 0.149\n",
            "Episode 897 | Total Reward: -8.06 | Epsilon: 0.148\n",
            "Episode 898 | Total Reward: -12.13 | Epsilon: 0.147\n",
            "Episode 899 | Total Reward: -5.72 | Epsilon: 0.146\n",
            "model saved to models/2025-05-10_00-08-28/q_network_ep_0900.pth\n",
            "\n",
            "Episode 900 | Total Reward: -3.47 | Epsilon: 0.145\n",
            "Episode 901 | Total Reward: -3.43 | Epsilon: 0.144\n",
            "Episode 902 | Total Reward: -2.72 | Epsilon: 0.143\n",
            "Episode 903 | Total Reward: -4.79 | Epsilon: 0.142\n",
            "Episode 904 | Total Reward: -17.96 | Epsilon: 0.141\n",
            "Episode 905 | Total Reward: -12.82 | Epsilon: 0.140\n",
            "Episode 906 | Total Reward: -4.57 | Epsilon: 0.139\n",
            "Episode 907 | Total Reward: -40.32 | Epsilon: 0.138\n",
            "Episode 908 | Total Reward: -27.98 | Epsilon: 0.137\n",
            "Episode 909 | Total Reward: -2.04 | Epsilon: 0.136\n",
            "Episode 910 | Total Reward: -5.09 | Epsilon: 0.135\n",
            "Episode 911 | Total Reward: -13.17 | Epsilon: 0.135\n",
            "Episode 912 | Total Reward: -9.31 | Epsilon: 0.134\n",
            "Episode 913 | Total Reward: -46.34 | Epsilon: 0.133\n",
            "Episode 914 | Total Reward: -6.48 | Epsilon: 0.132\n",
            "Episode 915 | Total Reward: -16.92 | Epsilon: 0.131\n",
            "Episode 916 | Total Reward: -18.74 | Epsilon: 0.130\n",
            "Episode 917 | Total Reward: -3.58 | Epsilon: 0.129\n",
            "Episode 918 | Total Reward: -3.19 | Epsilon: 0.128\n",
            "Episode 919 | Total Reward: -5.10 | Epsilon: 0.127\n",
            "Episode 920 | Total Reward: -28.53 | Epsilon: 0.126\n",
            "Episode 921 | Total Reward: -26.28 | Epsilon: 0.125\n",
            "Episode 922 | Total Reward: -2.72 | Epsilon: 0.124\n",
            "Episode 923 | Total Reward: -6.25 | Epsilon: 0.123\n",
            "Episode 924 | Total Reward: -30.04 | Epsilon: 0.122\n",
            "Episode 925 | Total Reward: -15.83 | Epsilon: 0.121\n",
            "Episode 926 | Total Reward: -2.09 | Epsilon: 0.120\n",
            "Episode 927 | Total Reward: -5.01 | Epsilon: 0.119\n",
            "Episode 928 | Total Reward: -18.78 | Epsilon: 0.118\n",
            "Episode 929 | Total Reward: -6.28 | Epsilon: 0.117\n",
            "Episode 930 | Total Reward: -12.07 | Epsilon: 0.117\n",
            "Episode 931 | Total Reward: -1.36 | Epsilon: 0.116\n",
            "Episode 932 | Total Reward: -29.69 | Epsilon: 0.115\n",
            "Episode 933 | Total Reward: -6.27 | Epsilon: 0.114\n",
            "Episode 934 | Total Reward: -4.42 | Epsilon: 0.113\n",
            "Episode 935 | Total Reward: -33.48 | Epsilon: 0.112\n",
            "Episode 936 | Total Reward: -5.09 | Epsilon: 0.111\n",
            "Episode 937 | Total Reward: -10.47 | Epsilon: 0.110\n",
            "Episode 938 | Total Reward: -6.19 | Epsilon: 0.109\n",
            "Episode 939 | Total Reward: -29.44 | Epsilon: 0.108\n",
            "Episode 940 | Total Reward: -5.62 | Epsilon: 0.107\n",
            "Episode 941 | Total Reward: -33.35 | Epsilon: 0.106\n",
            "Episode 942 | Total Reward: -3.87 | Epsilon: 0.105\n",
            "Episode 943 | Total Reward: -3.26 | Epsilon: 0.104\n",
            "Episode 944 | Total Reward: -43.69 | Epsilon: 0.103\n",
            "Episode 945 | Total Reward: -6.95 | Epsilon: 0.102\n",
            "Episode 946 | Total Reward: -18.28 | Epsilon: 0.101\n",
            "Episode 947 | Total Reward: -30.04 | Epsilon: 0.100\n",
            "Episode 948 | Total Reward: -12.12 | Epsilon: 0.099\n",
            "Episode 949 | Total Reward: -7.12 | Epsilon: 0.098\n",
            "Episode 950 | Total Reward: -15.72 | Epsilon: 0.098\n",
            "Episode 951 | Total Reward: -12.74 | Epsilon: 0.097\n",
            "Episode 952 | Total Reward: -12.83 | Epsilon: 0.096\n",
            "Episode 953 | Total Reward: -3.11 | Epsilon: 0.095\n",
            "Episode 954 | Total Reward: -6.86 | Epsilon: 0.094\n",
            "Episode 955 | Total Reward: -9.16 | Epsilon: 0.093\n",
            "Episode 956 | Total Reward: -26.66 | Epsilon: 0.092\n",
            "Episode 957 | Total Reward: -2.15 | Epsilon: 0.091\n",
            "Episode 958 | Total Reward: -6.50 | Epsilon: 0.090\n",
            "Episode 959 | Total Reward: -5.71 | Epsilon: 0.089\n",
            "Episode 960 | Total Reward: -34.24 | Epsilon: 0.088\n",
            "Episode 961 | Total Reward: -8.49 | Epsilon: 0.087\n",
            "Episode 962 | Total Reward: -1.85 | Epsilon: 0.086\n",
            "Episode 963 | Total Reward: -2.36 | Epsilon: 0.085\n",
            "Episode 964 | Total Reward: -7.62 | Epsilon: 0.084\n",
            "Episode 965 | Total Reward: -25.96 | Epsilon: 0.083\n",
            "Episode 966 | Total Reward: -6.26 | Epsilon: 0.082\n",
            "Episode 967 | Total Reward: -3.13 | Epsilon: 0.081\n",
            "Episode 968 | Total Reward: -4.10 | Epsilon: 0.080\n",
            "Episode 969 | Total Reward: -4.16 | Epsilon: 0.079\n",
            "Episode 970 | Total Reward: -13.30 | Epsilon: 0.079\n",
            "Episode 971 | Total Reward: -14.35 | Epsilon: 0.078\n",
            "Episode 972 | Total Reward: -2.89 | Epsilon: 0.077\n",
            "Episode 973 | Total Reward: -10.53 | Epsilon: 0.076\n",
            "Episode 974 | Total Reward: -6.89 | Epsilon: 0.075\n",
            "Episode 975 | Total Reward: -31.00 | Epsilon: 0.074\n",
            "Episode 976 | Total Reward: -56.67 | Epsilon: 0.073\n",
            "Episode 977 | Total Reward: -5.91 | Epsilon: 0.072\n",
            "Episode 978 | Total Reward: -12.64 | Epsilon: 0.071\n",
            "Episode 979 | Total Reward: -7.50 | Epsilon: 0.070\n",
            "Episode 980 | Total Reward: -4.20 | Epsilon: 0.069\n",
            "Episode 981 | Total Reward: -9.08 | Epsilon: 0.068\n",
            "Episode 982 | Total Reward: -2.10 | Epsilon: 0.067\n",
            "Episode 983 | Total Reward: -9.13 | Epsilon: 0.066\n",
            "Episode 984 | Total Reward: -7.09 | Epsilon: 0.065\n",
            "Episode 985 | Total Reward: -6.16 | Epsilon: 0.064\n",
            "Episode 986 | Total Reward: -27.90 | Epsilon: 0.063\n",
            "Episode 987 | Total Reward: -6.87 | Epsilon: 0.062\n",
            "Episode 988 | Total Reward: -5.81 | Epsilon: 0.061\n",
            "Episode 989 | Total Reward: -10.74 | Epsilon: 0.060\n",
            "Episode 990 | Total Reward: -12.72 | Epsilon: 0.059\n",
            "Episode 991 | Total Reward: -28.59 | Epsilon: 0.059\n",
            "Episode 992 | Total Reward: -2.67 | Epsilon: 0.058\n",
            "Episode 993 | Total Reward: -3.11 | Epsilon: 0.057\n",
            "Episode 994 | Total Reward: -7.07 | Epsilon: 0.056\n",
            "Episode 995 | Total Reward: -16.50 | Epsilon: 0.055\n",
            "Episode 996 | Total Reward: -6.19 | Epsilon: 0.054\n",
            "Episode 997 | Total Reward: -1.96 | Epsilon: 0.053\n",
            "Episode 998 | Total Reward: -14.50 | Epsilon: 0.052\n",
            "Episode 999 | Total Reward: -39.11 | Epsilon: 0.051\n",
            "model saved to models/2025-05-10_00-10-21/q_network_ep_1000.pth\n",
            "\n",
            "Episode 1000 | Total Reward: -5.42 | Epsilon: 0.050\n",
            "Episode 1001 | Total Reward: -12.43 | Epsilon: 0.050\n",
            "Episode 1002 | Total Reward: -3.57 | Epsilon: 0.050\n",
            "Episode 1003 | Total Reward: -2.80 | Epsilon: 0.050\n",
            "Episode 1004 | Total Reward: -12.94 | Epsilon: 0.050\n",
            "Episode 1005 | Total Reward: -24.93 | Epsilon: 0.050\n",
            "Episode 1006 | Total Reward: -15.85 | Epsilon: 0.050\n",
            "Episode 1007 | Total Reward: -2.57 | Epsilon: 0.050\n",
            "Episode 1008 | Total Reward: -10.38 | Epsilon: 0.050\n",
            "Episode 1009 | Total Reward: -2.75 | Epsilon: 0.050\n",
            "Episode 1010 | Total Reward: -4.36 | Epsilon: 0.050\n",
            "Episode 1011 | Total Reward: -13.23 | Epsilon: 0.050\n",
            "Episode 1012 | Total Reward: -2.71 | Epsilon: 0.050\n",
            "Episode 1013 | Total Reward: -15.75 | Epsilon: 0.050\n",
            "Episode 1014 | Total Reward: -21.60 | Epsilon: 0.050\n",
            "Episode 1015 | Total Reward: -11.76 | Epsilon: 0.050\n",
            "Episode 1016 | Total Reward: -2.28 | Epsilon: 0.050\n",
            "Episode 1017 | Total Reward: -5.50 | Epsilon: 0.050\n",
            "Episode 1018 | Total Reward: -2.55 | Epsilon: 0.050\n",
            "Episode 1019 | Total Reward: -29.89 | Epsilon: 0.050\n",
            "Episode 1020 | Total Reward: -17.48 | Epsilon: 0.050\n",
            "Episode 1021 | Total Reward: -8.22 | Epsilon: 0.050\n",
            "Episode 1022 | Total Reward: -17.20 | Epsilon: 0.050\n",
            "Episode 1023 | Total Reward: -18.03 | Epsilon: 0.050\n",
            "Episode 1024 | Total Reward: -4.94 | Epsilon: 0.050\n",
            "Episode 1025 | Total Reward: -18.72 | Epsilon: 0.050\n",
            "Episode 1026 | Total Reward: -23.53 | Epsilon: 0.050\n",
            "Episode 1027 | Total Reward: -5.46 | Epsilon: 0.050\n",
            "Episode 1028 | Total Reward: -10.78 | Epsilon: 0.050\n",
            "Episode 1029 | Total Reward: -28.96 | Epsilon: 0.050\n",
            "Episode 1030 | Total Reward: -2.12 | Epsilon: 0.050\n",
            "Episode 1031 | Total Reward: -12.80 | Epsilon: 0.050\n",
            "Episode 1032 | Total Reward: -9.82 | Epsilon: 0.050\n",
            "Episode 1033 | Total Reward: -24.70 | Epsilon: 0.050\n",
            "Episode 1034 | Total Reward: -7.51 | Epsilon: 0.050\n",
            "Episode 1035 | Total Reward: -3.39 | Epsilon: 0.050\n",
            "Episode 1036 | Total Reward: -11.08 | Epsilon: 0.050\n",
            "Episode 1037 | Total Reward: -2.69 | Epsilon: 0.050\n",
            "Episode 1038 | Total Reward: -5.75 | Epsilon: 0.050\n",
            "Episode 1039 | Total Reward: -24.34 | Epsilon: 0.050\n",
            "Episode 1040 | Total Reward: -4.07 | Epsilon: 0.050\n",
            "Episode 1041 | Total Reward: -7.06 | Epsilon: 0.050\n",
            "Episode 1042 | Total Reward: -2.73 | Epsilon: 0.050\n",
            "Episode 1043 | Total Reward: -19.25 | Epsilon: 0.050\n",
            "Episode 1044 | Total Reward: -19.79 | Epsilon: 0.050\n",
            "Episode 1045 | Total Reward: -26.66 | Epsilon: 0.050\n",
            "Episode 1046 | Total Reward: -24.30 | Epsilon: 0.050\n",
            "Episode 1047 | Total Reward: -11.48 | Epsilon: 0.050\n",
            "Episode 1048 | Total Reward: -2.32 | Epsilon: 0.050\n",
            "Episode 1049 | Total Reward: -3.51 | Epsilon: 0.050\n",
            "Episode 1050 | Total Reward: -7.03 | Epsilon: 0.050\n",
            "Episode 1051 | Total Reward: -11.52 | Epsilon: 0.050\n",
            "Episode 1052 | Total Reward: -14.11 | Epsilon: 0.050\n",
            "Episode 1053 | Total Reward: -2.32 | Epsilon: 0.050\n",
            "Episode 1054 | Total Reward: -9.87 | Epsilon: 0.050\n",
            "Episode 1055 | Total Reward: -2.85 | Epsilon: 0.050\n",
            "Episode 1056 | Total Reward: -21.14 | Epsilon: 0.050\n",
            "Episode 1057 | Total Reward: -21.39 | Epsilon: 0.050\n",
            "Episode 1058 | Total Reward: -14.06 | Epsilon: 0.050\n",
            "Episode 1059 | Total Reward: -1.94 | Epsilon: 0.050\n",
            "Episode 1060 | Total Reward: -2.88 | Epsilon: 0.050\n",
            "Episode 1061 | Total Reward: -6.26 | Epsilon: 0.050\n",
            "Episode 1062 | Total Reward: -25.77 | Epsilon: 0.050\n",
            "Episode 1063 | Total Reward: -28.76 | Epsilon: 0.050\n",
            "Episode 1064 | Total Reward: -6.77 | Epsilon: 0.050\n",
            "Episode 1065 | Total Reward: -10.27 | Epsilon: 0.050\n",
            "Episode 1066 | Total Reward: -15.60 | Epsilon: 0.050\n",
            "Episode 1067 | Total Reward: -11.47 | Epsilon: 0.050\n",
            "Episode 1068 | Total Reward: -6.94 | Epsilon: 0.050\n",
            "Episode 1069 | Total Reward: -11.22 | Epsilon: 0.050\n",
            "Episode 1070 | Total Reward: -5.99 | Epsilon: 0.050\n",
            "Episode 1071 | Total Reward: -2.39 | Epsilon: 0.050\n",
            "Episode 1072 | Total Reward: -2.18 | Epsilon: 0.050\n",
            "Episode 1073 | Total Reward: -1.75 | Epsilon: 0.050\n",
            "Episode 1074 | Total Reward: -11.94 | Epsilon: 0.050\n",
            "Episode 1075 | Total Reward: -4.80 | Epsilon: 0.050\n",
            "Episode 1076 | Total Reward: -11.71 | Epsilon: 0.050\n",
            "Episode 1077 | Total Reward: -9.74 | Epsilon: 0.050\n",
            "Episode 1078 | Total Reward: -3.96 | Epsilon: 0.050\n",
            "Episode 1079 | Total Reward: -13.90 | Epsilon: 0.050\n",
            "Episode 1080 | Total Reward: -15.03 | Epsilon: 0.050\n",
            "Episode 1081 | Total Reward: -18.51 | Epsilon: 0.050\n",
            "Episode 1082 | Total Reward: -20.40 | Epsilon: 0.050\n",
            "Episode 1083 | Total Reward: -18.31 | Epsilon: 0.050\n",
            "Episode 1084 | Total Reward: -5.44 | Epsilon: 0.050\n",
            "Episode 1085 | Total Reward: -16.52 | Epsilon: 0.050\n",
            "Episode 1086 | Total Reward: -1.37 | Epsilon: 0.050\n",
            "Episode 1087 | Total Reward: -1.29 | Epsilon: 0.050\n",
            "Episode 1088 | Total Reward: -5.26 | Epsilon: 0.050\n",
            "Episode 1089 | Total Reward: -18.93 | Epsilon: 0.050\n",
            "Episode 1090 | Total Reward: -3.05 | Epsilon: 0.050\n",
            "Episode 1091 | Total Reward: -13.40 | Epsilon: 0.050\n",
            "Episode 1092 | Total Reward: -6.87 | Epsilon: 0.050\n",
            "Episode 1093 | Total Reward: -24.37 | Epsilon: 0.050\n",
            "Episode 1094 | Total Reward: -12.16 | Epsilon: 0.050\n",
            "Episode 1095 | Total Reward: -23.49 | Epsilon: 0.050\n",
            "Episode 1096 | Total Reward: -40.34 | Epsilon: 0.050\n",
            "Episode 1097 | Total Reward: -21.96 | Epsilon: 0.050\n",
            "Episode 1098 | Total Reward: -12.79 | Epsilon: 0.050\n",
            "Episode 1099 | Total Reward: -2.66 | Epsilon: 0.050\n",
            "model saved to models/2025-05-10_00-12-11/q_network_ep_1100.pth\n",
            "\n",
            "Episode 1100 | Total Reward: -9.74 | Epsilon: 0.050\n",
            "Episode 1101 | Total Reward: -2.50 | Epsilon: 0.050\n",
            "Episode 1102 | Total Reward: -14.34 | Epsilon: 0.050\n",
            "Episode 1103 | Total Reward: -22.08 | Epsilon: 0.050\n",
            "Episode 1104 | Total Reward: -19.75 | Epsilon: 0.050\n",
            "Episode 1105 | Total Reward: -2.46 | Epsilon: 0.050\n",
            "Episode 1106 | Total Reward: -5.54 | Epsilon: 0.050\n",
            "Episode 1107 | Total Reward: -2.38 | Epsilon: 0.050\n",
            "Episode 1108 | Total Reward: -4.83 | Epsilon: 0.050\n",
            "Episode 1109 | Total Reward: -22.05 | Epsilon: 0.050\n",
            "Episode 1110 | Total Reward: -3.78 | Epsilon: 0.050\n",
            "Episode 1111 | Total Reward: -21.81 | Epsilon: 0.050\n",
            "Episode 1112 | Total Reward: -13.24 | Epsilon: 0.050\n",
            "Episode 1113 | Total Reward: -15.11 | Epsilon: 0.050\n",
            "Episode 1114 | Total Reward: -6.51 | Epsilon: 0.050\n",
            "Episode 1115 | Total Reward: -10.55 | Epsilon: 0.050\n",
            "Episode 1116 | Total Reward: -3.08 | Epsilon: 0.050\n",
            "Episode 1117 | Total Reward: -15.93 | Epsilon: 0.050\n",
            "Episode 1118 | Total Reward: -10.30 | Epsilon: 0.050\n",
            "Episode 1119 | Total Reward: -13.76 | Epsilon: 0.050\n",
            "Episode 1120 | Total Reward: -5.74 | Epsilon: 0.050\n",
            "Episode 1121 | Total Reward: -3.17 | Epsilon: 0.050\n",
            "Episode 1122 | Total Reward: -25.72 | Epsilon: 0.050\n",
            "Episode 1123 | Total Reward: -6.76 | Epsilon: 0.050\n",
            "Episode 1124 | Total Reward: -6.45 | Epsilon: 0.050\n",
            "Episode 1125 | Total Reward: -17.60 | Epsilon: 0.050\n",
            "Episode 1126 | Total Reward: -8.70 | Epsilon: 0.050\n",
            "Episode 1127 | Total Reward: -3.76 | Epsilon: 0.050\n",
            "Episode 1128 | Total Reward: -10.87 | Epsilon: 0.050\n",
            "Episode 1129 | Total Reward: -2.60 | Epsilon: 0.050\n",
            "Episode 1130 | Total Reward: -4.78 | Epsilon: 0.050\n",
            "Episode 1131 | Total Reward: -2.46 | Epsilon: 0.050\n",
            "Episode 1132 | Total Reward: -18.75 | Epsilon: 0.050\n",
            "Episode 1133 | Total Reward: -8.21 | Epsilon: 0.050\n",
            "Episode 1134 | Total Reward: -7.68 | Epsilon: 0.050\n",
            "Episode 1135 | Total Reward: -12.13 | Epsilon: 0.050\n",
            "Episode 1136 | Total Reward: -20.54 | Epsilon: 0.050\n",
            "Episode 1137 | Total Reward: -5.45 | Epsilon: 0.050\n",
            "Episode 1138 | Total Reward: -1.81 | Epsilon: 0.050\n",
            "Episode 1139 | Total Reward: -17.78 | Epsilon: 0.050\n",
            "Episode 1140 | Total Reward: -13.76 | Epsilon: 0.050\n",
            "Episode 1141 | Total Reward: -1.90 | Epsilon: 0.050\n",
            "Episode 1142 | Total Reward: -25.96 | Epsilon: 0.050\n",
            "Episode 1143 | Total Reward: -21.54 | Epsilon: 0.050\n",
            "Episode 1144 | Total Reward: -7.33 | Epsilon: 0.050\n",
            "Episode 1145 | Total Reward: -2.05 | Epsilon: 0.050\n",
            "Episode 1146 | Total Reward: -7.43 | Epsilon: 0.050\n",
            "Episode 1147 | Total Reward: -9.59 | Epsilon: 0.050\n",
            "Episode 1148 | Total Reward: -5.23 | Epsilon: 0.050\n",
            "Episode 1149 | Total Reward: -28.59 | Epsilon: 0.050\n",
            "Episode 1150 | Total Reward: -17.74 | Epsilon: 0.050\n",
            "Episode 1151 | Total Reward: -4.12 | Epsilon: 0.050\n",
            "Episode 1152 | Total Reward: -3.64 | Epsilon: 0.050\n",
            "Episode 1153 | Total Reward: -6.52 | Epsilon: 0.050\n",
            "Episode 1154 | Total Reward: -29.55 | Epsilon: 0.050\n",
            "Episode 1155 | Total Reward: -20.42 | Epsilon: 0.050\n",
            "Episode 1156 | Total Reward: -3.84 | Epsilon: 0.050\n",
            "Episode 1157 | Total Reward: -11.80 | Epsilon: 0.050\n",
            "Episode 1158 | Total Reward: -1.19 | Epsilon: 0.050\n",
            "Episode 1159 | Total Reward: -8.92 | Epsilon: 0.050\n",
            "Episode 1160 | Total Reward: -34.98 | Epsilon: 0.050\n",
            "Episode 1161 | Total Reward: -10.74 | Epsilon: 0.050\n",
            "Episode 1162 | Total Reward: -5.03 | Epsilon: 0.050\n",
            "Episode 1163 | Total Reward: -5.40 | Epsilon: 0.050\n",
            "Episode 1164 | Total Reward: -10.96 | Epsilon: 0.050\n",
            "Episode 1165 | Total Reward: -9.84 | Epsilon: 0.050\n",
            "Episode 1166 | Total Reward: -23.40 | Epsilon: 0.050\n",
            "Episode 1167 | Total Reward: -6.07 | Epsilon: 0.050\n",
            "Episode 1168 | Total Reward: -4.93 | Epsilon: 0.050\n",
            "Episode 1169 | Total Reward: -7.97 | Epsilon: 0.050\n",
            "Episode 1170 | Total Reward: -3.72 | Epsilon: 0.050\n",
            "Episode 1171 | Total Reward: -22.35 | Epsilon: 0.050\n",
            "Episode 1172 | Total Reward: -5.23 | Epsilon: 0.050\n",
            "Episode 1173 | Total Reward: -2.19 | Epsilon: 0.050\n",
            "Episode 1174 | Total Reward: -4.62 | Epsilon: 0.050\n",
            "Episode 1175 | Total Reward: -3.89 | Epsilon: 0.050\n",
            "Episode 1176 | Total Reward: -28.72 | Epsilon: 0.050\n",
            "Episode 1177 | Total Reward: -9.58 | Epsilon: 0.050\n",
            "Episode 1178 | Total Reward: -4.31 | Epsilon: 0.050\n",
            "Episode 1179 | Total Reward: -23.03 | Epsilon: 0.050\n",
            "Episode 1180 | Total Reward: -3.68 | Epsilon: 0.050\n",
            "Episode 1181 | Total Reward: -32.75 | Epsilon: 0.050\n",
            "Episode 1182 | Total Reward: -23.05 | Epsilon: 0.050\n",
            "Episode 1183 | Total Reward: -48.20 | Epsilon: 0.050\n",
            "Episode 1184 | Total Reward: -21.11 | Epsilon: 0.050\n",
            "Episode 1185 | Total Reward: -8.72 | Epsilon: 0.050\n",
            "Episode 1186 | Total Reward: -4.72 | Epsilon: 0.050\n",
            "Episode 1187 | Total Reward: -12.99 | Epsilon: 0.050\n",
            "Episode 1188 | Total Reward: -2.17 | Epsilon: 0.050\n",
            "Episode 1189 | Total Reward: -23.50 | Epsilon: 0.050\n",
            "Episode 1190 | Total Reward: -2.66 | Epsilon: 0.050\n",
            "Episode 1191 | Total Reward: -1.93 | Epsilon: 0.050\n",
            "Episode 1192 | Total Reward: -21.81 | Epsilon: 0.050\n",
            "Episode 1193 | Total Reward: -14.21 | Epsilon: 0.050\n",
            "Episode 1194 | Total Reward: -3.59 | Epsilon: 0.050\n",
            "Episode 1195 | Total Reward: -5.71 | Epsilon: 0.050\n",
            "Episode 1196 | Total Reward: -4.41 | Epsilon: 0.050\n",
            "Episode 1197 | Total Reward: -4.99 | Epsilon: 0.050\n",
            "Episode 1198 | Total Reward: -20.27 | Epsilon: 0.050\n",
            "Episode 1199 | Total Reward: -4.41 | Epsilon: 0.050\n",
            "model saved to models/2025-05-10_00-14-03/q_network_ep_1200.pth\n",
            "\n",
            "Episode 1200 | Total Reward: -18.57 | Epsilon: 0.050\n",
            "Episode 1201 | Total Reward: -6.56 | Epsilon: 0.050\n",
            "Episode 1202 | Total Reward: -8.94 | Epsilon: 0.050\n",
            "Episode 1203 | Total Reward: -6.47 | Epsilon: 0.050\n",
            "Episode 1204 | Total Reward: -0.89 | Epsilon: 0.050\n",
            "Episode 1205 | Total Reward: -1.53 | Epsilon: 0.050\n",
            "Episode 1206 | Total Reward: -2.20 | Epsilon: 0.050\n",
            "Episode 1207 | Total Reward: -12.21 | Epsilon: 0.050\n",
            "Episode 1208 | Total Reward: -7.54 | Epsilon: 0.050\n",
            "Episode 1209 | Total Reward: -15.02 | Epsilon: 0.050\n",
            "Episode 1210 | Total Reward: -2.88 | Epsilon: 0.050\n",
            "Episode 1211 | Total Reward: -19.39 | Epsilon: 0.050\n",
            "Episode 1212 | Total Reward: -6.74 | Epsilon: 0.050\n",
            "Episode 1213 | Total Reward: -10.25 | Epsilon: 0.050\n",
            "Episode 1214 | Total Reward: -9.23 | Epsilon: 0.050\n",
            "Episode 1215 | Total Reward: -7.47 | Epsilon: 0.050\n",
            "Episode 1216 | Total Reward: -15.80 | Epsilon: 0.050\n",
            "Episode 1217 | Total Reward: -6.15 | Epsilon: 0.050\n",
            "Episode 1218 | Total Reward: -9.00 | Epsilon: 0.050\n",
            "Episode 1219 | Total Reward: -10.99 | Epsilon: 0.050\n",
            "Episode 1220 | Total Reward: -18.45 | Epsilon: 0.050\n",
            "Episode 1221 | Total Reward: -9.38 | Epsilon: 0.050\n",
            "Episode 1222 | Total Reward: -3.54 | Epsilon: 0.050\n",
            "Episode 1223 | Total Reward: -5.58 | Epsilon: 0.050\n",
            "Episode 1224 | Total Reward: -8.45 | Epsilon: 0.050\n",
            "Episode 1225 | Total Reward: -6.51 | Epsilon: 0.050\n",
            "Episode 1226 | Total Reward: -19.43 | Epsilon: 0.050\n",
            "Episode 1227 | Total Reward: -11.41 | Epsilon: 0.050\n",
            "Episode 1228 | Total Reward: -5.63 | Epsilon: 0.050\n",
            "Episode 1229 | Total Reward: -15.59 | Epsilon: 0.050\n",
            "Episode 1230 | Total Reward: -2.58 | Epsilon: 0.050\n",
            "Episode 1231 | Total Reward: -17.59 | Epsilon: 0.050\n",
            "Episode 1232 | Total Reward: -9.09 | Epsilon: 0.050\n",
            "Episode 1233 | Total Reward: -2.07 | Epsilon: 0.050\n",
            "Episode 1234 | Total Reward: -13.10 | Epsilon: 0.050\n",
            "Episode 1235 | Total Reward: -3.73 | Epsilon: 0.050\n",
            "Episode 1236 | Total Reward: -14.46 | Epsilon: 0.050\n",
            "Episode 1237 | Total Reward: -2.24 | Epsilon: 0.050\n",
            "Episode 1238 | Total Reward: -7.87 | Epsilon: 0.050\n",
            "Episode 1239 | Total Reward: -20.91 | Epsilon: 0.050\n",
            "Episode 1240 | Total Reward: -26.33 | Epsilon: 0.050\n",
            "Episode 1241 | Total Reward: -7.45 | Epsilon: 0.050\n",
            "Episode 1242 | Total Reward: -2.72 | Epsilon: 0.050\n",
            "Episode 1243 | Total Reward: -9.52 | Epsilon: 0.050\n",
            "Episode 1244 | Total Reward: -1.39 | Epsilon: 0.050\n",
            "Episode 1245 | Total Reward: -14.94 | Epsilon: 0.050\n",
            "Episode 1246 | Total Reward: -6.55 | Epsilon: 0.050\n",
            "Episode 1247 | Total Reward: -2.57 | Epsilon: 0.050\n",
            "Episode 1248 | Total Reward: -6.68 | Epsilon: 0.050\n",
            "Episode 1249 | Total Reward: -5.62 | Epsilon: 0.050\n",
            "Episode 1250 | Total Reward: -13.08 | Epsilon: 0.050\n",
            "Episode 1251 | Total Reward: -4.35 | Epsilon: 0.050\n",
            "Episode 1252 | Total Reward: -6.43 | Epsilon: 0.050\n",
            "Episode 1253 | Total Reward: -1.92 | Epsilon: 0.050\n",
            "Episode 1254 | Total Reward: -13.27 | Epsilon: 0.050\n",
            "Episode 1255 | Total Reward: -3.68 | Epsilon: 0.050\n",
            "Episode 1256 | Total Reward: -9.68 | Epsilon: 0.050\n",
            "Episode 1257 | Total Reward: -12.41 | Epsilon: 0.050\n",
            "Episode 1258 | Total Reward: -11.93 | Epsilon: 0.050\n",
            "Episode 1259 | Total Reward: -8.50 | Epsilon: 0.050\n",
            "Episode 1260 | Total Reward: -5.33 | Epsilon: 0.050\n",
            "Episode 1261 | Total Reward: -38.71 | Epsilon: 0.050\n",
            "Episode 1262 | Total Reward: -3.16 | Epsilon: 0.050\n",
            "Episode 1263 | Total Reward: -11.75 | Epsilon: 0.050\n",
            "Episode 1264 | Total Reward: -18.15 | Epsilon: 0.050\n",
            "Episode 1265 | Total Reward: -5.72 | Epsilon: 0.050\n",
            "Episode 1266 | Total Reward: -5.75 | Epsilon: 0.050\n",
            "Episode 1267 | Total Reward: -2.14 | Epsilon: 0.050\n",
            "Episode 1268 | Total Reward: -26.73 | Epsilon: 0.050\n",
            "Episode 1269 | Total Reward: -12.59 | Epsilon: 0.050\n",
            "Episode 1270 | Total Reward: -8.60 | Epsilon: 0.050\n",
            "Episode 1271 | Total Reward: -19.26 | Epsilon: 0.050\n",
            "Episode 1272 | Total Reward: -1.96 | Epsilon: 0.050\n",
            "Episode 1273 | Total Reward: -5.35 | Epsilon: 0.050\n",
            "Episode 1274 | Total Reward: -7.08 | Epsilon: 0.050\n",
            "Episode 1275 | Total Reward: -26.80 | Epsilon: 0.050\n",
            "Episode 1276 | Total Reward: -9.78 | Epsilon: 0.050\n",
            "Episode 1277 | Total Reward: -9.10 | Epsilon: 0.050\n",
            "Episode 1278 | Total Reward: -9.32 | Epsilon: 0.050\n",
            "Episode 1279 | Total Reward: -11.02 | Epsilon: 0.050\n",
            "Episode 1280 | Total Reward: -2.48 | Epsilon: 0.050\n",
            "Episode 1281 | Total Reward: -22.12 | Epsilon: 0.050\n",
            "Episode 1282 | Total Reward: -24.31 | Epsilon: 0.050\n",
            "Episode 1283 | Total Reward: -63.76 | Epsilon: 0.050\n",
            "Episode 1284 | Total Reward: -3.59 | Epsilon: 0.050\n",
            "Episode 1285 | Total Reward: -23.79 | Epsilon: 0.050\n",
            "Episode 1286 | Total Reward: -2.66 | Epsilon: 0.050\n",
            "Episode 1287 | Total Reward: -39.92 | Epsilon: 0.050\n",
            "Episode 1288 | Total Reward: -1.34 | Epsilon: 0.050\n",
            "Episode 1289 | Total Reward: -54.74 | Epsilon: 0.050\n",
            "Episode 1290 | Total Reward: -21.12 | Epsilon: 0.050\n",
            "Episode 1291 | Total Reward: -23.25 | Epsilon: 0.050\n",
            "Episode 1292 | Total Reward: -9.23 | Epsilon: 0.050\n",
            "Episode 1293 | Total Reward: -6.77 | Epsilon: 0.050\n",
            "Episode 1294 | Total Reward: -1.30 | Epsilon: 0.050\n",
            "Episode 1295 | Total Reward: -7.85 | Epsilon: 0.050\n",
            "Episode 1296 | Total Reward: -19.77 | Epsilon: 0.050\n",
            "Episode 1297 | Total Reward: -64.41 | Epsilon: 0.050\n",
            "Episode 1298 | Total Reward: -10.63 | Epsilon: 0.050\n",
            "Episode 1299 | Total Reward: -6.73 | Epsilon: 0.050\n",
            "model saved to models/2025-05-10_00-16-00/q_network_ep_1300.pth\n",
            "\n",
            "Episode 1300 | Total Reward: -12.64 | Epsilon: 0.050\n",
            "Episode 1301 | Total Reward: -8.09 | Epsilon: 0.050\n",
            "Episode 1302 | Total Reward: -27.67 | Epsilon: 0.050\n",
            "Episode 1303 | Total Reward: -5.31 | Epsilon: 0.050\n",
            "Episode 1304 | Total Reward: -11.28 | Epsilon: 0.050\n",
            "Episode 1305 | Total Reward: -792.33 | Epsilon: 0.050\n",
            "Episode 1306 | Total Reward: -2.52 | Epsilon: 0.050\n",
            "Episode 1307 | Total Reward: -55.11 | Epsilon: 0.050\n",
            "Episode 1308 | Total Reward: -13.26 | Epsilon: 0.050\n",
            "Episode 1309 | Total Reward: -9.29 | Epsilon: 0.050\n",
            "Episode 1310 | Total Reward: -34.58 | Epsilon: 0.050\n",
            "Episode 1311 | Total Reward: -17.19 | Epsilon: 0.050\n",
            "Episode 1312 | Total Reward: -23.51 | Epsilon: 0.050\n",
            "Episode 1313 | Total Reward: -4.10 | Epsilon: 0.050\n",
            "Episode 1314 | Total Reward: -1070.66 | Epsilon: 0.050\n",
            "Episode 1315 | Total Reward: -25.43 | Epsilon: 0.050\n",
            "Episode 1316 | Total Reward: -38.19 | Epsilon: 0.050\n",
            "Episode 1317 | Total Reward: -7.66 | Epsilon: 0.050\n",
            "Episode 1318 | Total Reward: -12.01 | Epsilon: 0.050\n",
            "Episode 1319 | Total Reward: -2.42 | Epsilon: 0.050\n",
            "Episode 1320 | Total Reward: -5.70 | Epsilon: 0.050\n",
            "Episode 1321 | Total Reward: -24.37 | Epsilon: 0.050\n",
            "Episode 1322 | Total Reward: -13.93 | Epsilon: 0.050\n",
            "Episode 1323 | Total Reward: -4.30 | Epsilon: 0.050\n",
            "Episode 1324 | Total Reward: -10.16 | Epsilon: 0.050\n",
            "Episode 1325 | Total Reward: -10.27 | Epsilon: 0.050\n",
            "Episode 1326 | Total Reward: -33.30 | Epsilon: 0.050\n",
            "Episode 1327 | Total Reward: -21.83 | Epsilon: 0.050\n",
            "Episode 1328 | Total Reward: -1.72 | Epsilon: 0.050\n",
            "Episode 1329 | Total Reward: -30.70 | Epsilon: 0.050\n",
            "Episode 1330 | Total Reward: -20.02 | Epsilon: 0.050\n",
            "Episode 1331 | Total Reward: -4.42 | Epsilon: 0.050\n",
            "Episode 1332 | Total Reward: -58.64 | Epsilon: 0.050\n",
            "Episode 1333 | Total Reward: -327.36 | Epsilon: 0.050\n",
            "Episode 1334 | Total Reward: -7.94 | Epsilon: 0.050\n",
            "Episode 1335 | Total Reward: -34.25 | Epsilon: 0.050\n",
            "Episode 1336 | Total Reward: -13.45 | Epsilon: 0.050\n",
            "Episode 1337 | Total Reward: -26.09 | Epsilon: 0.050\n",
            "Episode 1338 | Total Reward: -18.04 | Epsilon: 0.050\n",
            "Episode 1339 | Total Reward: -2.90 | Epsilon: 0.050\n",
            "Episode 1340 | Total Reward: -49.79 | Epsilon: 0.050\n",
            "Episode 1341 | Total Reward: -17.60 | Epsilon: 0.050\n",
            "Episode 1342 | Total Reward: -9.96 | Epsilon: 0.050\n",
            "Episode 1343 | Total Reward: -4.63 | Epsilon: 0.050\n",
            "Episode 1344 | Total Reward: -16.61 | Epsilon: 0.050\n",
            "Episode 1345 | Total Reward: -4.05 | Epsilon: 0.050\n",
            "Episode 1346 | Total Reward: -12.73 | Epsilon: 0.050\n",
            "Episode 1347 | Total Reward: -2.09 | Epsilon: 0.050\n",
            "Episode 1348 | Total Reward: -12.16 | Epsilon: 0.050\n",
            "Episode 1349 | Total Reward: -2.49 | Epsilon: 0.050\n",
            "Episode 1350 | Total Reward: -10.03 | Epsilon: 0.050\n",
            "Episode 1351 | Total Reward: -1.55 | Epsilon: 0.050\n",
            "Episode 1352 | Total Reward: -4.12 | Epsilon: 0.050\n",
            "Episode 1353 | Total Reward: -1.31 | Epsilon: 0.050\n",
            "Episode 1354 | Total Reward: -28.04 | Epsilon: 0.050\n",
            "Episode 1355 | Total Reward: -4.62 | Epsilon: 0.050\n",
            "Episode 1356 | Total Reward: -3.61 | Epsilon: 0.050\n",
            "Episode 1357 | Total Reward: -8.25 | Epsilon: 0.050\n",
            "Episode 1358 | Total Reward: -40.93 | Epsilon: 0.050\n",
            "Episode 1359 | Total Reward: -12.02 | Epsilon: 0.050\n",
            "Episode 1360 | Total Reward: -4.34 | Epsilon: 0.050\n",
            "Episode 1361 | Total Reward: -1.74 | Epsilon: 0.050\n",
            "Episode 1362 | Total Reward: -33.74 | Epsilon: 0.050\n",
            "Episode 1363 | Total Reward: -10.01 | Epsilon: 0.050\n",
            "Episode 1364 | Total Reward: -5.16 | Epsilon: 0.050\n",
            "Episode 1365 | Total Reward: -20.84 | Epsilon: 0.050\n",
            "Episode 1366 | Total Reward: -1.62 | Epsilon: 0.050\n",
            "Episode 1367 | Total Reward: -100.82 | Epsilon: 0.050\n",
            "Episode 1368 | Total Reward: -2.26 | Epsilon: 0.050\n",
            "Episode 1369 | Total Reward: -8.21 | Epsilon: 0.050\n",
            "Episode 1370 | Total Reward: -4.64 | Epsilon: 0.050\n",
            "Episode 1371 | Total Reward: -12.60 | Epsilon: 0.050\n",
            "Episode 1372 | Total Reward: -5.54 | Epsilon: 0.050\n",
            "Episode 1373 | Total Reward: -35.83 | Epsilon: 0.050\n",
            "Episode 1374 | Total Reward: -4.61 | Epsilon: 0.050\n",
            "Episode 1375 | Total Reward: -18.97 | Epsilon: 0.050\n",
            "Episode 1376 | Total Reward: -4.41 | Epsilon: 0.050\n",
            "Episode 1377 | Total Reward: -3.28 | Epsilon: 0.050\n",
            "Episode 1378 | Total Reward: -7.60 | Epsilon: 0.050\n",
            "Episode 1379 | Total Reward: -12.28 | Epsilon: 0.050\n",
            "Episode 1380 | Total Reward: -4.36 | Epsilon: 0.050\n",
            "Episode 1381 | Total Reward: -23.71 | Epsilon: 0.050\n",
            "Episode 1382 | Total Reward: -4.26 | Epsilon: 0.050\n",
            "Episode 1383 | Total Reward: -26.66 | Epsilon: 0.050\n",
            "Episode 1384 | Total Reward: -6.42 | Epsilon: 0.050\n",
            "Episode 1385 | Total Reward: -9.27 | Epsilon: 0.050\n",
            "Episode 1386 | Total Reward: -1.36 | Epsilon: 0.050\n",
            "Episode 1387 | Total Reward: -27.28 | Epsilon: 0.050\n",
            "Episode 1388 | Total Reward: -43.86 | Epsilon: 0.050\n",
            "Episode 1389 | Total Reward: -4.74 | Epsilon: 0.050\n",
            "Episode 1390 | Total Reward: -18.97 | Epsilon: 0.050\n",
            "Episode 1391 | Total Reward: -5.28 | Epsilon: 0.050\n",
            "Episode 1392 | Total Reward: -4.01 | Epsilon: 0.050\n",
            "Episode 1393 | Total Reward: -5.23 | Epsilon: 0.050\n",
            "Episode 1394 | Total Reward: -1.70 | Epsilon: 0.050\n",
            "Episode 1395 | Total Reward: -11.40 | Epsilon: 0.050\n",
            "Episode 1396 | Total Reward: -21.14 | Epsilon: 0.050\n",
            "Episode 1397 | Total Reward: -8.57 | Epsilon: 0.050\n",
            "Episode 1398 | Total Reward: -6.74 | Epsilon: 0.050\n",
            "Episode 1399 | Total Reward: -1.89 | Epsilon: 0.050\n",
            "model saved to models/2025-05-10_00-17-56/q_network_ep_1400.pth\n",
            "\n",
            "Episode 1400 | Total Reward: -24.83 | Epsilon: 0.050\n",
            "Episode 1401 | Total Reward: -1.35 | Epsilon: 0.050\n",
            "Episode 1402 | Total Reward: -12.85 | Epsilon: 0.050\n",
            "Episode 1403 | Total Reward: -6.40 | Epsilon: 0.050\n",
            "Episode 1404 | Total Reward: -7.68 | Epsilon: 0.050\n",
            "Episode 1405 | Total Reward: -26.28 | Epsilon: 0.050\n",
            "Episode 1406 | Total Reward: -11.16 | Epsilon: 0.050\n",
            "Episode 1407 | Total Reward: -27.71 | Epsilon: 0.050\n",
            "Episode 1408 | Total Reward: -6.32 | Epsilon: 0.050\n",
            "Episode 1409 | Total Reward: -3.76 | Epsilon: 0.050\n",
            "Episode 1410 | Total Reward: -9.12 | Epsilon: 0.050\n",
            "Episode 1411 | Total Reward: -4.36 | Epsilon: 0.050\n",
            "Episode 1412 | Total Reward: -5.26 | Epsilon: 0.050\n",
            "Episode 1413 | Total Reward: -0.76 | Epsilon: 0.050\n",
            "Episode 1414 | Total Reward: -5.09 | Epsilon: 0.050\n",
            "Episode 1415 | Total Reward: -4.69 | Epsilon: 0.050\n",
            "Episode 1416 | Total Reward: -1.65 | Epsilon: 0.050\n",
            "Episode 1417 | Total Reward: -4.56 | Epsilon: 0.050\n",
            "Episode 1418 | Total Reward: -2.47 | Epsilon: 0.050\n",
            "Episode 1419 | Total Reward: -9.60 | Epsilon: 0.050\n",
            "Episode 1420 | Total Reward: -432.62 | Epsilon: 0.050\n",
            "Episode 1421 | Total Reward: -5.29 | Epsilon: 0.050\n",
            "Episode 1422 | Total Reward: -4.11 | Epsilon: 0.050\n",
            "Episode 1423 | Total Reward: -8.77 | Epsilon: 0.050\n",
            "Episode 1424 | Total Reward: -20.41 | Epsilon: 0.050\n",
            "Episode 1425 | Total Reward: -9.37 | Epsilon: 0.050\n",
            "Episode 1426 | Total Reward: -6.95 | Epsilon: 0.050\n",
            "Episode 1427 | Total Reward: -9.36 | Epsilon: 0.050\n",
            "Episode 1428 | Total Reward: -9.81 | Epsilon: 0.050\n",
            "Episode 1429 | Total Reward: -18.84 | Epsilon: 0.050\n",
            "Episode 1430 | Total Reward: -5.92 | Epsilon: 0.050\n",
            "Episode 1431 | Total Reward: -22.32 | Epsilon: 0.050\n",
            "Episode 1432 | Total Reward: -8.24 | Epsilon: 0.050\n",
            "Episode 1433 | Total Reward: -7.89 | Epsilon: 0.050\n",
            "Episode 1434 | Total Reward: -19.21 | Epsilon: 0.050\n",
            "Episode 1435 | Total Reward: -8.49 | Epsilon: 0.050\n",
            "Episode 1436 | Total Reward: -10.99 | Epsilon: 0.050\n",
            "Episode 1437 | Total Reward: -4.83 | Epsilon: 0.050\n",
            "Episode 1438 | Total Reward: -4.98 | Epsilon: 0.050\n",
            "Episode 1439 | Total Reward: -72.27 | Epsilon: 0.050\n",
            "Episode 1440 | Total Reward: -4.24 | Epsilon: 0.050\n",
            "Episode 1441 | Total Reward: -1.95 | Epsilon: 0.050\n",
            "Episode 1442 | Total Reward: -5.14 | Epsilon: 0.050\n",
            "Episode 1443 | Total Reward: -1.03 | Epsilon: 0.050\n",
            "Episode 1444 | Total Reward: -24.22 | Epsilon: 0.050\n",
            "Episode 1445 | Total Reward: -37.04 | Epsilon: 0.050\n",
            "Episode 1446 | Total Reward: -10.86 | Epsilon: 0.050\n",
            "Episode 1447 | Total Reward: -3.99 | Epsilon: 0.050\n",
            "Episode 1448 | Total Reward: -1.36 | Epsilon: 0.050\n",
            "Episode 1449 | Total Reward: -24.30 | Epsilon: 0.050\n",
            "Episode 1450 | Total Reward: -14.13 | Epsilon: 0.050\n",
            "Episode 1451 | Total Reward: -8.01 | Epsilon: 0.050\n",
            "Episode 1452 | Total Reward: -13.03 | Epsilon: 0.050\n",
            "Episode 1453 | Total Reward: -5.27 | Epsilon: 0.050\n",
            "Episode 1454 | Total Reward: -19.13 | Epsilon: 0.050\n",
            "Episode 1455 | Total Reward: -1.16 | Epsilon: 0.050\n",
            "Episode 1456 | Total Reward: -3.19 | Epsilon: 0.050\n",
            "Episode 1457 | Total Reward: -5.59 | Epsilon: 0.050\n",
            "Episode 1458 | Total Reward: -16.60 | Epsilon: 0.050\n",
            "Episode 1459 | Total Reward: -16.32 | Epsilon: 0.050\n",
            "Episode 1460 | Total Reward: -3.97 | Epsilon: 0.050\n",
            "Episode 1461 | Total Reward: -151.19 | Epsilon: 0.050\n",
            "Episode 1462 | Total Reward: -2.03 | Epsilon: 0.050\n",
            "Episode 1463 | Total Reward: -5.65 | Epsilon: 0.050\n",
            "Episode 1464 | Total Reward: -1.07 | Epsilon: 0.050\n",
            "Episode 1465 | Total Reward: -5.71 | Epsilon: 0.050\n",
            "Episode 1466 | Total Reward: -7.81 | Epsilon: 0.050\n",
            "Episode 1467 | Total Reward: -967.29 | Epsilon: 0.050\n",
            "Episode 1468 | Total Reward: -47.82 | Epsilon: 0.050\n",
            "Episode 1469 | Total Reward: -6.10 | Epsilon: 0.050\n",
            "Episode 1470 | Total Reward: -9.97 | Epsilon: 0.050\n",
            "Episode 1471 | Total Reward: -10.74 | Epsilon: 0.050\n",
            "Episode 1472 | Total Reward: -3.82 | Epsilon: 0.050\n",
            "Episode 1473 | Total Reward: -12.80 | Epsilon: 0.050\n",
            "Episode 1474 | Total Reward: -5.50 | Epsilon: 0.050\n",
            "Episode 1475 | Total Reward: -4.62 | Epsilon: 0.050\n",
            "Episode 1476 | Total Reward: -17.38 | Epsilon: 0.050\n",
            "Episode 1477 | Total Reward: -1.35 | Epsilon: 0.050\n",
            "Episode 1478 | Total Reward: -22.47 | Epsilon: 0.050\n",
            "Episode 1479 | Total Reward: -29.22 | Epsilon: 0.050\n",
            "Episode 1480 | Total Reward: -3.17 | Epsilon: 0.050\n",
            "Episode 1481 | Total Reward: -18.04 | Epsilon: 0.050\n",
            "Episode 1482 | Total Reward: -23.19 | Epsilon: 0.050\n",
            "Episode 1483 | Total Reward: -7.60 | Epsilon: 0.050\n",
            "Episode 1484 | Total Reward: -17.07 | Epsilon: 0.050\n",
            "Episode 1485 | Total Reward: -10.43 | Epsilon: 0.050\n",
            "Episode 1486 | Total Reward: -12.74 | Epsilon: 0.050\n",
            "Episode 1487 | Total Reward: -3.72 | Epsilon: 0.050\n",
            "Episode 1488 | Total Reward: -4.81 | Epsilon: 0.050\n",
            "Episode 1489 | Total Reward: -6.36 | Epsilon: 0.050\n",
            "Episode 1490 | Total Reward: -14.63 | Epsilon: 0.050\n",
            "Episode 1491 | Total Reward: -8.70 | Epsilon: 0.050\n",
            "Episode 1492 | Total Reward: -16.57 | Epsilon: 0.050\n",
            "Episode 1493 | Total Reward: -5.52 | Epsilon: 0.050\n",
            "Episode 1494 | Total Reward: -13.55 | Epsilon: 0.050\n",
            "Episode 1495 | Total Reward: -5.30 | Epsilon: 0.050\n",
            "Episode 1496 | Total Reward: -18.10 | Epsilon: 0.050\n",
            "Episode 1497 | Total Reward: -2.28 | Epsilon: 0.050\n",
            "Episode 1498 | Total Reward: -79.41 | Epsilon: 0.050\n",
            "Episode 1499 | Total Reward: -12.36 | Epsilon: 0.050\n",
            "model saved to models/2025-05-10_00-19-44/q_network_ep_1500.pth\n",
            "\n",
            "Episode 1500 | Total Reward: -24.10 | Epsilon: 0.050\n"
          ]
        }
      ],
      "source": [
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "\n",
        "# DO NOT CHANGE\n",
        "# ---------------\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "env = ArmEnv(arm, gui=False)\n",
        "tqdn = TrainDQN(env)\n",
        "# ---------------\n",
        "\n",
        "# Call your trin function here\n",
        "tqdn.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To keep track of your experiments, it is good practice to plot and check how well is your model trained based on the returns vs episodes plot. With a large number of episodes, this  plot may look very jagged making it difficult to ascertain how well you are doing. We are proving code to smoothen out the plot by. This will take a large list of returns in every episode and plot a smoothened version of the list. Feel free to use it if it helps.\n",
        "```\n",
        "import seaborn as sns\n",
        "returns = __\n",
        "smoothing = 10\n",
        "\n",
        "smoothened = [sum(returns[i:i+smoothing])/smoothing for i in range(0, len(returns), smoothing)]\n",
        "sns.lineplot(smoothened)\n",
        "```"
      ],
      "metadata": {
        "id": "jAlaOcVJsn5a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIcBDbeTRNZI"
      },
      "source": [
        "### Load your model and test its performance\n",
        "Change your model path and the goal to see how well your learnt model is performing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "x5gTRNKhRQQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc619eb-5939-49ff-c7f7-798c15f422ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/2025-05-09_23-57-06/q_network_ep_0300.pth: Avg Return = -169.38\n",
            "models/2025-05-10_00-19-44/q_network_ep_1500.pth: Avg Return = -8.27\n",
            "models/2025-05-10_00-17-56/q_network_ep_1400.pth: Avg Return = -12.78\n",
            "models/2025-05-10_00-10-21/q_network_ep_1000.pth: Avg Return = -7.10\n",
            "models/2025-05-10_00-02-56/q_network_ep_0600.pth: Avg Return = -8.73\n",
            "models/2025-05-10_00-08-28/q_network_ep_0900.pth: Avg Return = -7.13\n",
            "models/2025-05-10_00-14-03/q_network_ep_1200.pth: Avg Return = -7.87\n",
            "models/2025-05-10_00-00-58/q_network_ep_0500.pth: Avg Return = -8.99\n",
            "models/2025-05-09_23-55-18/q_network_ep_0200.pth: Avg Return = -40.81\n",
            "models/2025-05-10_00-06-39/q_network_ep_0800.pth: Avg Return = -7.46\n",
            "models/2025-05-09_23-58-54/q_network_ep_0400.pth: Avg Return = -8.27\n",
            "models/2025-05-09_23-53-28/q_network_ep_0100.pth: Avg Return = -394.39\n",
            "models/2025-05-10_00-16-00/q_network_ep_1300.pth: Avg Return = -8.01\n",
            "models/2025-05-10_00-04-49/q_network_ep_0700.pth: Avg Return = -8.01\n",
            "models/2025-05-10_00-12-11/q_network_ep_1100.pth: Avg Return = -7.41\n",
            "\n",
            "Best model: models/2025-05-10_00-10-21/q_network_ep_1000.pth with Avg Return = -7.10\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from geometry import polar2cartesian\n",
        "\n",
        "# Define fixed test goal\n",
        "test_goal = polar2cartesian(1.6, 0.25 - np.pi/2.0)\n",
        "\n",
        "# Environment setup (same as before)\n",
        "arm = Robot(\n",
        "    ArmDynamics(\n",
        "        num_links=2,\n",
        "        link_mass=0.1,\n",
        "        link_length=1,\n",
        "        joint_viscous_friction=0.1,\n",
        "        dt=0.01,\n",
        "        gravity=False\n",
        "    )\n",
        ")\n",
        "env = ArmEnv(arm, gui=False)\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "# Set your model directory path\n",
        "model_root_dir = 'models'  # Adjust to actual parent directory where all runs are saved\n",
        "\n",
        "def evaluate_model(model_path, num_episodes=5):\n",
        "    qnet = QNetwork(env).to(device)\n",
        "    qnet.load_state_dict(torch.load(model_path))\n",
        "    qnet.eval()\n",
        "\n",
        "    returns = []\n",
        "    for _ in range(num_episodes):\n",
        "        obs = env.reset(goal=test_goal)\n",
        "        done = False\n",
        "        episode_return = 0\n",
        "\n",
        "        while not done:\n",
        "            action = qnet.select_discrete_action(obs, device)\n",
        "            action = qnet.action_discrete_to_continuous(action)\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            episode_return += reward\n",
        "\n",
        "        returns.append(episode_return)\n",
        "\n",
        "    return np.mean(returns)\n",
        "\n",
        "# Loop through all saved models and evaluate\n",
        "best_model_path = None\n",
        "best_return = -float('inf')\n",
        "\n",
        "for subdir, _, files in os.walk(model_root_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.pth'):\n",
        "            model_path = os.path.join(subdir, file)\n",
        "            avg_return = evaluate_model(model_path)\n",
        "\n",
        "            print(f\"{model_path}: Avg Return = {avg_return:.2f}\")\n",
        "            if avg_return > best_return:\n",
        "                best_return = avg_return\n",
        "                best_model_path = model_path\n",
        "\n",
        "print(f\"\\nBest model: {best_model_path} with Avg Return = {best_return:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from math import dist\n",
        "from arm_env import ArmEnv\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from geometry import polar2cartesian\n",
        "\n",
        "def evaluate_model(model_path, goal, env, device):\n",
        "    qnet = QNetwork(env).to(device)\n",
        "    qnet.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    qnet.eval()\n",
        "\n",
        "    obs = env.reset(goal)\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action_id = qnet.select_discrete_action(obs, device)\n",
        "        action = qnet.action_discrete_to_continuous(action_id)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "    final_pos = info['pos_ee']\n",
        "    final_dist = np.linalg.norm(final_pos - goal)\n",
        "    return total_reward, final_dist\n",
        "\n",
        "def evaluate_all_models(models_dir):\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "    # Define arm and environment once\n",
        "    arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "            gravity=False\n",
        "        )\n",
        "    )\n",
        "    env = ArmEnv(arm, gui=False)\n",
        "\n",
        "    # Set a fixed evaluation goal\n",
        "    goal = polar2cartesian(1.6, 0.25 - np.pi / 2.0)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Traverse all subdirectories and find .pth files\n",
        "    for root, dirs, files in os.walk(models_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.pth'):\n",
        "                model_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    ret, dist = evaluate_model(model_path, goal, env, device)\n",
        "                    results.append((model_path, ret, dist))\n",
        "                    print(f\"✅ Evaluated: {file} | Return: {ret:.2f} | Distance: {dist:.4f}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Failed: {file} | Error: {e}\")\n",
        "\n",
        "    # Sort by reward descending, then distance ascending\n",
        "    results.sort(key=lambda x: (-x[1], x[2]))\n",
        "\n",
        "    print(\"\\n=== Ranked Models ===\")\n",
        "    for i, (path, ret, dist) in enumerate(results, 1):\n",
        "        print(f\"{i:2d}. {os.path.basename(path):30s} | Return: {ret:.2f} | Final Dist: {dist:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# === RUN IT ===\n",
        "models_dir = 'models'  # Update if your models are saved elsewhere\n",
        "evaluate_all_models(models_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glEpNdfFkTgZ",
        "outputId": "a94fb4b5-b355-49b6-9792-d019bdfe9627"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/geometry.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  R[0,0] = np.cos(theta)\n",
            "/content/arm_dynamics.py:113: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  right_hand[self.idx_tau_eqbm(i), 0] += (tau[i + 1] if i < self.num_links - 1 else 0.0) - tau[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Evaluated: q_network_ep_0300.pth | Return: -169.38 | Distance: 0.7078\n",
            "✅ Evaluated: q_network_ep_1500.pth | Return: -8.27 | Distance: 0.1190\n",
            "✅ Evaluated: q_network_ep_1400.pth | Return: -12.78 | Distance: 0.3224\n",
            "✅ Evaluated: q_network_ep_1000.pth | Return: -7.10 | Distance: 0.0439\n",
            "✅ Evaluated: q_network_ep_0600.pth | Return: -8.73 | Distance: 0.1073\n",
            "✅ Evaluated: q_network_ep_0900.pth | Return: -7.13 | Distance: 0.0272\n",
            "✅ Evaluated: q_network_ep_1200.pth | Return: -7.87 | Distance: 0.0482\n",
            "✅ Evaluated: q_network_ep_0500.pth | Return: -8.99 | Distance: 0.0881\n",
            "✅ Evaluated: q_network_ep_0200.pth | Return: -40.81 | Distance: 0.7694\n",
            "✅ Evaluated: q_network_ep_0800.pth | Return: -7.46 | Distance: 0.0615\n",
            "✅ Evaluated: q_network_ep_0400.pth | Return: -8.27 | Distance: 0.0687\n",
            "✅ Evaluated: q_network_ep_0100.pth | Return: -394.39 | Distance: 2.1823\n",
            "✅ Evaluated: q_network_ep_1300.pth | Return: -8.01 | Distance: 0.0619\n",
            "✅ Evaluated: q_network_ep_0700.pth | Return: -8.01 | Distance: 0.0890\n",
            "✅ Evaluated: q_network_ep_1100.pth | Return: -7.41 | Distance: 0.0365\n",
            "\n",
            "=== Ranked Models ===\n",
            " 1. q_network_ep_1000.pth          | Return: -7.10 | Final Dist: 0.0439\n",
            " 2. q_network_ep_0900.pth          | Return: -7.13 | Final Dist: 0.0272\n",
            " 3. q_network_ep_1100.pth          | Return: -7.41 | Final Dist: 0.0365\n",
            " 4. q_network_ep_0800.pth          | Return: -7.46 | Final Dist: 0.0615\n",
            " 5. q_network_ep_1200.pth          | Return: -7.87 | Final Dist: 0.0482\n",
            " 6. q_network_ep_0700.pth          | Return: -8.01 | Final Dist: 0.0890\n",
            " 7. q_network_ep_1300.pth          | Return: -8.01 | Final Dist: 0.0619\n",
            " 8. q_network_ep_1500.pth          | Return: -8.27 | Final Dist: 0.1190\n",
            " 9. q_network_ep_0400.pth          | Return: -8.27 | Final Dist: 0.0687\n",
            "10. q_network_ep_0600.pth          | Return: -8.73 | Final Dist: 0.1073\n",
            "11. q_network_ep_0500.pth          | Return: -8.99 | Final Dist: 0.0881\n",
            "12. q_network_ep_1400.pth          | Return: -12.78 | Final Dist: 0.3224\n",
            "13. q_network_ep_0200.pth          | Return: -40.81 | Final Dist: 0.7694\n",
            "14. q_network_ep_0300.pth          | Return: -169.38 | Final Dist: 0.7078\n",
            "15. q_network_ep_0100.pth          | Return: -394.39 | Final Dist: 2.1823\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('models/2025-05-10_00-10-21/q_network_ep_1000.pth',\n",
              "  np.float64(-7.100632115744406),\n",
              "  np.float64(0.043859555526526955)),\n",
              " ('models/2025-05-10_00-08-28/q_network_ep_0900.pth',\n",
              "  np.float64(-7.131600638186105),\n",
              "  np.float64(0.027188827053636833)),\n",
              " ('models/2025-05-10_00-12-11/q_network_ep_1100.pth',\n",
              "  np.float64(-7.406781019256993),\n",
              "  np.float64(0.03652380574870491)),\n",
              " ('models/2025-05-10_00-06-39/q_network_ep_0800.pth',\n",
              "  np.float64(-7.456165207726923),\n",
              "  np.float64(0.06149868143173527)),\n",
              " ('models/2025-05-10_00-14-03/q_network_ep_1200.pth',\n",
              "  np.float64(-7.867637199134296),\n",
              "  np.float64(0.048166030337082257)),\n",
              " ('models/2025-05-10_00-04-49/q_network_ep_0700.pth',\n",
              "  np.float64(-8.009303657578831),\n",
              "  np.float64(0.0889792304049865)),\n",
              " ('models/2025-05-10_00-16-00/q_network_ep_1300.pth',\n",
              "  np.float64(-8.010533446116765),\n",
              "  np.float64(0.061913299657904444)),\n",
              " ('models/2025-05-10_00-19-44/q_network_ep_1500.pth',\n",
              "  np.float64(-8.268528205853924),\n",
              "  np.float64(0.11895015977452728)),\n",
              " ('models/2025-05-09_23-58-54/q_network_ep_0400.pth',\n",
              "  np.float64(-8.271847923282126),\n",
              "  np.float64(0.06873644329850769)),\n",
              " ('models/2025-05-10_00-02-56/q_network_ep_0600.pth',\n",
              "  np.float64(-8.728298613368024),\n",
              "  np.float64(0.1073154886711518)),\n",
              " ('models/2025-05-10_00-00-58/q_network_ep_0500.pth',\n",
              "  np.float64(-8.99288935888296),\n",
              "  np.float64(0.08813795063668664)),\n",
              " ('models/2025-05-10_00-17-56/q_network_ep_1400.pth',\n",
              "  np.float64(-12.77895822328367),\n",
              "  np.float64(0.32238421080498597)),\n",
              " ('models/2025-05-09_23-55-18/q_network_ep_0200.pth',\n",
              "  np.float64(-40.81085412007899),\n",
              "  np.float64(0.7694212970023706)),\n",
              " ('models/2025-05-09_23-57-06/q_network_ep_0300.pth',\n",
              "  np.float64(-169.38228428082618),\n",
              "  np.float64(0.707846818541623)),\n",
              " ('models/2025-05-09_23-53-28/q_network_ep_0100.pth',\n",
              "  np.float64(-394.38870631611456),\n",
              "  np.float64(2.1822898119012133))]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from score import compute_score\n",
        "import os\n",
        "import torch\n",
        "from arm_env import ArmEnv\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "\n",
        "def score_model(model_path, env, device):\n",
        "    qnet = QNetwork(env).to(device)\n",
        "    qnet.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    qnet.eval()\n",
        "    try:\n",
        "        score = compute_score(qnet, env, device)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed scoring {model_path}: {e}\")\n",
        "        return -1\n",
        "    return score\n",
        "\n",
        "def find_best_model(models_dir):\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "    # Initialize arm and env\n",
        "    arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "            gravity=False\n",
        "        )\n",
        "    )\n",
        "    env = ArmEnv(arm, gui=False)\n",
        "\n",
        "    results = []\n",
        "    for root, dirs, files in os.walk(models_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.pth'):\n",
        "                path = os.path.join(root, file)\n",
        "                print(f\"Scoring model: {path}\")\n",
        "                score = score_model(path, env, device)\n",
        "                results.append((path, score))\n",
        "\n",
        "    results.sort(key=lambda x: -x[1])  # Sort by score descending\n",
        "    print(\"\\n=== Ranked Models by Total Score ===\")\n",
        "    for i, (path, score) in enumerate(results, 1):\n",
        "        print(f\"{i:2d}. {os.path.basename(path):30s} | Score: {score}\")\n",
        "\n",
        "    best_model = results[0][0] if results else None\n",
        "    print(f\"\\nBest Model for Submission: {best_model}\")\n",
        "    return best_model\n",
        "\n",
        "# Run the selection\n",
        "models_dir = 'models'\n",
        "find_best_model(models_dir)\n",
        "\n",
        "# Best model path has been found, you can now use it\n",
        "print(f\"\\nSelected Best Model Path: {best_model}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z5oShxklQgb",
        "outputId": "fe5e12fa-3ce2-4210-dd08-a769df2bed28"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scoring model: models/2025-05-09_23-57-06/q_network_ep_0300.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -42.4981628883934\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -169.38228428082618\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -251.42199280086092\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -174.33242311343605\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -227.61600373894413\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 0\n",
            "Scoring model: models/2025-05-10_00-19-44/q_network_ep_1500.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -5.250414316646425\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -8.268528205853924\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -9.224000565115565\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -19.080742613928095\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -19.391087521715946\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 1\n",
            "Scoring model: models/2025-05-10_00-17-56/q_network_ep_1400.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -23.47121598759574\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -12.77895822328367\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -8.739091364292227\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -13.298064530447574\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -17.29573573890766\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 0\n",
            "Scoring model: models/2025-05-10_00-10-21/q_network_ep_1000.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -5.773154700612433\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -7.100632115744406\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -8.403953760065752\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -11.528708590718999\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -16.722505386371605\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 1\n",
            "Scoring model: models/2025-05-10_00-02-56/q_network_ep_0600.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -9.177363925194555\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -8.728298613368024\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -7.491034435333718\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -14.725763886902532\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -17.599587909861327\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 0\n",
            "Scoring model: models/2025-05-10_00-08-28/q_network_ep_0900.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -7.388863050645571\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -7.131600638186105\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -7.089685836261717\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -11.516272365646735\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -15.90183446704497\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 0\n",
            "Scoring model: models/2025-05-10_00-14-03/q_network_ep_1200.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -5.861037876392516\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -7.867637199134296\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -7.356743608601338\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -12.091722629543899\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -16.200689382675257\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 1\n",
            "Scoring model: models/2025-05-10_00-00-58/q_network_ep_0500.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -7.889270358676254\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -8.99288935888296\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -7.340119587201586\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -17.137877291966543\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -18.00556204498945\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 0\n",
            "Scoring model: models/2025-05-09_23-55-18/q_network_ep_0200.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -205.53179114341373\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -40.81085412007899\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -52.74424892427746\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -37.90188396253652\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -43.08916853471349\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 0\n",
            "Scoring model: models/2025-05-10_00-06-39/q_network_ep_0800.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -5.732822316348689\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -7.456165207726923\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -6.883011367980306\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -11.843104062399119\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -15.977320735174196\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 2\n",
            "Scoring model: models/2025-05-09_23-58-54/q_network_ep_0400.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -5.141377039666033\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -8.271847923282126\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -9.160544156756886\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -12.39381402119193\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -18.818100667897\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 1\n",
            "Scoring model: models/2025-05-09_23-53-28/q_network_ep_0100.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -74.27665099435794\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -394.38870631611456\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -237.59463640649975\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -155.1480594853931\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -470.78303522510964\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 0\n",
            "Scoring model: models/2025-05-10_00-16-00/q_network_ep_1300.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -4.852098989720713\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1.5\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -8.010533446116765\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -7.675405596664037\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -12.303476329407863\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -16.162954149529757\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 1.5\n",
            "Scoring model: models/2025-05-10_00-04-49/q_network_ep_0700.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -10.852272189621289\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -8.009303657578831\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -7.435332511934343\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -12.61551191871408\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -16.268434153812905\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 0\n",
            "Scoring model: models/2025-05-10_00-12-11/q_network_ep_1100.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -5.352267203231812\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -7.406781019256993\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -8.21587953975512\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -11.765996176735397\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -16.356542230577823\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 1\n",
            "\n",
            "=== Ranked Models by Total Score ===\n",
            " 1. q_network_ep_0800.pth          | Score: 2\n",
            " 2. q_network_ep_1300.pth          | Score: 1.5\n",
            " 3. q_network_ep_1500.pth          | Score: 1\n",
            " 4. q_network_ep_1000.pth          | Score: 1\n",
            " 5. q_network_ep_1200.pth          | Score: 1\n",
            " 6. q_network_ep_0400.pth          | Score: 1\n",
            " 7. q_network_ep_1100.pth          | Score: 1\n",
            " 8. q_network_ep_0300.pth          | Score: 0\n",
            " 9. q_network_ep_1400.pth          | Score: 0\n",
            "10. q_network_ep_0600.pth          | Score: 0\n",
            "11. q_network_ep_0900.pth          | Score: 0\n",
            "12. q_network_ep_0500.pth          | Score: 0\n",
            "13. q_network_ep_0200.pth          | Score: 0\n",
            "14. q_network_ep_0100.pth          | Score: 0\n",
            "15. q_network_ep_0700.pth          | Score: 0\n",
            "\n",
            "Best Model for Submission: models/2025-05-10_00-06-39/q_network_ep_0800.pth\n",
            "\n",
            "Selected Best Model Path: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grading and Evaluation\n",
        "You will be evaluated on 5 different goal positions worth 1.5 points each. You must pass the best `model_path` for your network. The scoring function will run one episode for every goal position and find the total reward (aka return) for the episode. For every goal you get:\n",
        "\n",
        "* 1 Point if `easy target < total reward < hard target`\n",
        "* 1.5 Points if `hard target < total reward`"
      ],
      "metadata": {
        "id": "pUDEYLsZLSp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from score import compute_score\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# DO NOT CHANGE arm parameters\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "# ------------------\n",
        "\n",
        "env = ArmEnv(arm, gui=False)\n",
        "model_path = best_model_path # Fill in the model_path\n",
        "print(f'Model path: {model_path}')\n",
        "device = torch.device('cpu')\n",
        "qnet = QNetwork(env).to(device)\n",
        "qnet.load_state_dict(torch.load(model_path))\n",
        "qnet.eval()\n",
        "score = compute_score(qnet, env, device)"
      ],
      "metadata": {
        "id": "OhPD-u6TIxdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7b097b-e0ea-4c72-93c6-c6938c03da1f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model path: models/2025-05-10_00-10-21/q_network_ep_1000.pth\n",
            "---Computing score---\n",
            "\n",
            "Goal 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/geometry.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  R[0,0] = np.cos(theta)\n",
            "/content/arm_dynamics.py:113: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  right_hand[self.idx_tau_eqbm(i), 0] += (tau[i + 1] if i < self.num_links - 1 else 0.0) - tau[i]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward: -5.773154700612433\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -7.100632115744406\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -8.403953760065752\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -11.528708590718999\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -16.722505386371605\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: PPO with an open source RL library\n",
        "\n",
        "In this part, you will use one of the most popular open source RL libraries ([Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)) to solve the same goal reaching problem as Part 1. We will use the same `ArmEnv` gym environment. The algorithm you should choose to use is PPO."
      ],
      "metadata": {
        "id": "vkCvO0-05XK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO training\n",
        "\n",
        "We provide the code to construct parallel environments. Parallel environments can be very useful if you have good CPUs and it can speed up training."
      ],
      "metadata": {
        "id": "UBK89P2B8CgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
        "from stable_baselines3.common.vec_env.vec_monitor import VecMonitor\n",
        "from copy import deepcopy\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from arm_env import ArmEnv\n",
        "\n",
        "class EnvMaker:\n",
        "    def __init__(self,  arm, seed):\n",
        "        self.seed = seed\n",
        "        self.arm = arm\n",
        "\n",
        "    def __call__(self):\n",
        "        arm = deepcopy(self.arm)\n",
        "        env = ArmEnv(arm)\n",
        "        env.seed(self.seed)\n",
        "        return env\n",
        "\n",
        "def make_vec_env(arm, nenv, seed):\n",
        "    return VecMonitor(SubprocVecEnv([EnvMaker(arm, seed  + 100 * i) for i in range(nenv)]))\n",
        "\n",
        "# conveniet function to create a robot arm\n",
        "def make_arm():\n",
        "    arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01\n",
        "        )\n",
        "    )\n",
        "    arm.reset()\n",
        "    return arm\n"
      ],
      "metadata": {
        "id": "2RTqfmpVwMja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need to complete the code to train the policy using the [PPO class](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) from stable_baselines3. We provide the code to generate the name of the directory to save the checkpoint, an example is `ppo_models/2025-04-21_01-14-13`. Your checkpoint model should be named `ppo_network.zip`. See the [save](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO.save) function. Training should take less than 40 minutes."
      ],
      "metadata": {
        "id": "Bniz2TouwM3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.ppo import PPO\n",
        "import os\n",
        "import time\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "\n",
        "# Default parameters\n",
        "timesteps = 500000\n",
        "nenv = 8  # number of parallel environments. This can speed up training when you have good CPUs\n",
        "seed = 8\n",
        "batch_size = 2048\n",
        "\n",
        "# Generate path of the directory to save the checkpoint\n",
        "timestr = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "save_dir = os.path.join('ppo_models', timestr)\n",
        "\n",
        "# Set random seed\n",
        "set_random_seed(seed)\n",
        "\n",
        "# Create arm\n",
        "arm = make_arm()\n",
        "\n",
        "# Create parallel envs\n",
        "vec_env = make_vec_env(arm=arm, nenv=nenv, seed=seed)\n",
        "\n",
        "# ------ IMPLEMENT YOUR TRAINING CODE HERE ------------\n",
        "raise NotImplementedError\n",
        "\n",
        "# Do not forget to save your model at the end of training"
      ],
      "metadata": {
        "id": "FHoSWOnG-2sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading and evaluation\n",
        "\n",
        "The total number of points for Part 2 is 7.5. We will evaluate your trained model on 5 random goal locations. For each test, we assign points based on the distance between the end effector and the goal location at the end of the episode.\n",
        "\n",
        "- If 0 < distance < 0.05, you get 1.5 points.\n",
        "- If 0.05 <= distance < 0.1, you get 1 point.\n",
        "- If distance >= 0.1, you get 0 point.\n",
        "\n"
      ],
      "metadata": {
        "id": "f9N2falIz9rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from score import score_policy\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from render import Renderer\n",
        "import time\n",
        "\n",
        "# Set the path to your model\n",
        "model_path = 'ppo_network.zip'\n",
        "\n",
        "set_random_seed(seed=100)\n",
        "\n",
        "# Create arm robot\n",
        "arm = make_arm()\n",
        "\n",
        "# Create environment\n",
        "env = ArmEnv(arm, gui=False)\n",
        "env.seed(100)\n",
        "\n",
        "# Load and test policy\n",
        "policy = PPO.load(model_path)\n",
        "score_policy(policy, env)"
      ],
      "metadata": {
        "id": "X6eQ2mzglwd0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}