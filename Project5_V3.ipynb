{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nprimavera/Reinforcement-Learning/blob/main/Project5_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MECS6616 Spring 2025 - Project 5**"
      ],
      "metadata": {
        "id": "PxhEChyomf5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "***IMPORTANT:***\n",
        "- **Before starting, make sure to read the [Assignment Instructions](https://courseworks2.columbia.edu/courses/215046/pages/assignment-instructions) page on Courseworks to understand the workflow and submission requirements for this project.**\n",
        "\n",
        "**FOR PROJECT 5!!!**\n",
        "- Apart from the link to your notebook, you are also required to submit `q_network.pth` of Part 1 and `ppo_network.zip` (model checkpoints are loaded and saved by stable_baselines3 as zip files) of Part 2 to Coursework. You should put the link to your notebook in the comment entry"
      ],
      "metadata": {
        "id": "50kTBhSwmrJB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inY7y5CRo97q"
      },
      "source": [
        "# Project Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# There will be error messages from this command. You can ignore those error messages\n",
        "# as long as you see \"Successfully installed setuptools-65.5.0\" at the end.\n",
        "\n",
        "# After installing setuptools, a pop-up window will appear and you will be prompted\n",
        "# to restart the notebook environment. Click on the restart environment button before continuing\n",
        "\n",
        "!pip install setuptools==65.5.0"
      ],
      "metadata": {
        "id": "PTRNqFBLkRDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a631df5-54d9-4dd0-89c1-f68a2ddb6c53"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools==65.5.0 in /usr/local/lib/python3.11/dist-packages (65.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**----------------------------**\n",
        "**WAIT FOR NOTEBOOK TO RESTART**\n",
        "**----------------------------**"
      ],
      "metadata": {
        "id": "tnf22sQzqw6_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QPIiNSZ8hb8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4822f1-9fe3-40b3-cb16-3f1917c604b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mecs6616_sp24_project5'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (15/15), 9.22 KiB | 4.61 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# After running this cell, the folder 'mecs6616_sp25_project5' will show up in the file explorer on the left (click on the folder icon if it's not open)\n",
        "# It may take a few seconds to appear\n",
        "!git clone https://github.com/roamlab/mecs6616_sp24_project5.git\n",
        "!mv /content/mecs6616_sp24_project5 /content/mecs6616_sp25_project5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ise8RAQhhs3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688a9f83-8578-4f4f-d44a-49dfb7f3922f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/mecs6616_sp25_project5/arm_dynamics_base.py' -> '/content/arm_dynamics_base.py'\n",
            "'/content/mecs6616_sp25_project5/arm_dynamics.py' -> '/content/arm_dynamics.py'\n",
            "'/content/mecs6616_sp25_project5/arm_env.py' -> '/content/arm_env.py'\n",
            "'/content/mecs6616_sp25_project5/geometry.py' -> '/content/geometry.py'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5' -> '/content/mecs6616_sp24_project5'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git' -> '/content/mecs6616_sp24_project5/.git'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks' -> '/content/mecs6616_sp24_project5/.git/hooks'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/post-update.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/post-update.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/pre-merge-commit.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/pre-merge-commit.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/fsmonitor-watchman.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/fsmonitor-watchman.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/update.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/update.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/pre-push.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/pre-push.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/pre-commit.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/pre-commit.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/commit-msg.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/commit-msg.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/prepare-commit-msg.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/prepare-commit-msg.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/applypatch-msg.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/applypatch-msg.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/pre-rebase.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/pre-rebase.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/push-to-checkout.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/push-to-checkout.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/pre-applypatch.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/pre-applypatch.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/hooks/pre-receive.sample' -> '/content/mecs6616_sp24_project5/.git/hooks/pre-receive.sample'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/branches' -> '/content/mecs6616_sp24_project5/.git/branches'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/description' -> '/content/mecs6616_sp24_project5/.git/description'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/info' -> '/content/mecs6616_sp24_project5/.git/info'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/info/exclude' -> '/content/mecs6616_sp24_project5/.git/info/exclude'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/refs' -> '/content/mecs6616_sp24_project5/.git/refs'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/refs/heads' -> '/content/mecs6616_sp24_project5/.git/refs/heads'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/refs/heads/main' -> '/content/mecs6616_sp24_project5/.git/refs/heads/main'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/refs/tags' -> '/content/mecs6616_sp24_project5/.git/refs/tags'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/refs/remotes' -> '/content/mecs6616_sp24_project5/.git/refs/remotes'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/refs/remotes/origin' -> '/content/mecs6616_sp24_project5/.git/refs/remotes/origin'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/refs/remotes/origin/HEAD' -> '/content/mecs6616_sp24_project5/.git/refs/remotes/origin/HEAD'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/index' -> '/content/mecs6616_sp24_project5/.git/index'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/objects' -> '/content/mecs6616_sp24_project5/.git/objects'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/objects/pack' -> '/content/mecs6616_sp24_project5/.git/objects/pack'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/objects/pack/pack-2049da8647337dd2a4651ce334d5defad719b52a.pack' -> '/content/mecs6616_sp24_project5/.git/objects/pack/pack-2049da8647337dd2a4651ce334d5defad719b52a.pack'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/objects/pack/pack-2049da8647337dd2a4651ce334d5defad719b52a.idx' -> '/content/mecs6616_sp24_project5/.git/objects/pack/pack-2049da8647337dd2a4651ce334d5defad719b52a.idx'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/objects/info' -> '/content/mecs6616_sp24_project5/.git/objects/info'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/config' -> '/content/mecs6616_sp24_project5/.git/config'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/packed-refs' -> '/content/mecs6616_sp24_project5/.git/packed-refs'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs' -> '/content/mecs6616_sp24_project5/.git/logs'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs/refs' -> '/content/mecs6616_sp24_project5/.git/logs/refs'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs/refs/remotes' -> '/content/mecs6616_sp24_project5/.git/logs/refs/remotes'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs/refs/remotes/origin' -> '/content/mecs6616_sp24_project5/.git/logs/refs/remotes/origin'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs/refs/remotes/origin/HEAD' -> '/content/mecs6616_sp24_project5/.git/logs/refs/remotes/origin/HEAD'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs/refs/heads' -> '/content/mecs6616_sp24_project5/.git/logs/refs/heads'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs/refs/heads/main' -> '/content/mecs6616_sp24_project5/.git/logs/refs/heads/main'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/logs/HEAD' -> '/content/mecs6616_sp24_project5/.git/logs/HEAD'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/.git/HEAD' -> '/content/mecs6616_sp24_project5/.git/HEAD'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/README.md' -> '/content/mecs6616_sp24_project5/README.md'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/arm_dynamics.py' -> '/content/mecs6616_sp24_project5/arm_dynamics.py'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/arm_dynamics_base.py' -> '/content/mecs6616_sp24_project5/arm_dynamics_base.py'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/arm_env.py' -> '/content/mecs6616_sp24_project5/arm_env.py'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/geometry.py' -> '/content/mecs6616_sp24_project5/geometry.py'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/render.py' -> '/content/mecs6616_sp24_project5/render.py'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/robot.py' -> '/content/mecs6616_sp24_project5/robot.py'\n",
            "'/content/mecs6616_sp25_project5/mecs6616_sp24_project5/score.py' -> '/content/mecs6616_sp24_project5/score.py'\n",
            "'/content/mecs6616_sp25_project5/README.md' -> '/content/README.md'\n",
            "'/content/mecs6616_sp25_project5/render.py' -> '/content/render.py'\n",
            "'/content/mecs6616_sp25_project5/robot.py' -> '/content/robot.py'\n",
            "'/content/mecs6616_sp25_project5/score.py' -> '/content/score.py'\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# copy all needed files into the working directory. This is simply to make accessing files easier\n",
        "!cp -av /content/mecs6616_sp25_project5/* /content/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# There will be error messages from this command. You can ignore those error messages\n",
        "# as long as you see \"Successfully installed gym stable-baselines3\" at the end.\n",
        "\n",
        "!pip install wheel==0.38.4\n",
        "!pip install gym stable-baselines3\n",
        "!pip install shimmy>=2.0"
      ],
      "metadata": {
        "id": "qEFlC9hVkRrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b85f668a-a70f-4b8d-ad60-dd8b6cc3fc21"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wheel==0.38.4\n",
            "  Downloading wheel-0.38.4-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading wheel-0.38.4-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: wheel\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.45.1\n",
            "    Uninstalling wheel-0.45.1:\n",
            "      Successfully uninstalled wheel-0.45.1\n",
            "Successfully installed wheel-0.38.4\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.1.1)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Implement DQN\n",
        "\n",
        "For this part, you will implement DQN from scratch. You SHOULD NOT use any RL libraries."
      ],
      "metadata": {
        "id": "-KNg9fzU5Un7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-JvzRuwNsYz"
      },
      "source": [
        "## Starter Code Explanation\n",
        "In addition to code you are already familiar with from the previous project (i.e. arm dynamics, etc.) we are providing an \"Environment\" in the `ArmEnv` class. The environment \"wraps around\" the arm dynamics and provides the key functions that an RL algorithm expects: reset(...) and step(...). The implementation of `ArmEnv` follows the [OpenAI Gym](https://www.gymlibrary.dev/api/core/) API standard. It is a standard that is accepeted by many RL libraries and allows for our problem to be easily solved with various RL libraries. Take a moment to familiarize yourself with these functions! See [here](https://www.gymlibrary.dev/api/core/) for more information on the definition of the reset(...) and step(...) functions.\n",
        "\n",
        "Important notes:\n",
        "\n",
        "* The ArmEnv expects an action similar to the one used previously: a vector with a torque for every arm joint. Thus, the native action space for this environment is high-dimensional, and continuous. DQN will require an action space that is 1-dimensional and discrete. You will need to convert between these. For example, you can have an action space of [0, 1, 2,] where each number just represents the identity of an action candidate, and a conversion dictionary {0: [-0.1, -0.1], 1: [0.1, 0.1], 2: [0, 0]}. Then, when the Q network output an action 1, it will be converted into [0.1, 0.1] and used by the environment. Note that this is just an example method to implement the conversion and you do not have to follow the same procedure.\n",
        "* The observation provided by the environment will comprise the same state vector as before, to which we append the current position of the end_effector and the goal for the end-effector. Since your policy must learn to reach arbitrary goals, the goal must be provided as part of the observation. So the observation will consist of 8 values: 4 for the state, 2 for the pos_ee and 2 for the goal.\n",
        "* The maximum episode length of the environment is 200 steps. Each step is simulated for 0.01 second. This should be used for both training and testing.\n",
        "* The reward function of this environment is by default r(s, a) = - dist(pos_ee, goal)^2 where represents the negative square of L2 distance between the current position of the end-effector and the goal position."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arm Environment Example\n",
        "You are encouraged to view the `arm_env.py` file to understand the `random_goal()`, `reset()` and `step()`  functions but do not modify the file.\n",
        "\n",
        "The `env.reset()` method, will reset the arm in the vertically downwards position and set a new random goal by calling the `random_goal()` method. By understanding how the goals are set you could guide your training in that direction. You can also provide your own goal as a (2,1) array to the reset function as an argument. This could come handy later when training the model.\n",
        "\n",
        "The `env.step()` function takes an action as a (2,1) shaped array and outputs the next observation, reward, done and info. `info` is a dictionary with pos_ee and vel_ee values. This can come handy if you attempt to do some reward engineering.\n",
        "\n",
        "The cell below provides an example of random policy interacting with the ArmEnv for 50 steps (0.5 seconds)"
      ],
      "metadata": {
        "id": "gw8H0PZcSv7F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "o6r9kJ5jpeds",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "outputId": "9069617e-96de-4352-d696-3dfcb451a3ec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0YAAAMtCAYAAAC7F2GBAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZ5ZJREFUeJzt3XlYlXX+//HXYTuACriwKq4o7nsa1qSOllZjWmZmlktmpViaS2pN2jJlWaajOVozpfZNM801czRza9wXxBXJHTcwNUFQFjn37w9/nRkSDIrDDdzPx3Xd1zXc53Mf3uf+8sWe17nPjc0wDEMAAAAAYGFuZg8AAAAAAGYjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALM/D7AEKm8Ph0Llz51SuXDnZbDazxwEAAABgEsMwdPXqVYWFhcnN7fbvCZW6MDp37pzCw8PNHgMAAABAMXH69GlVqVLltmtKXRiVK1dO0s0X7+fnZ/I0AAAAAMySkpKi8PBwZyPcTqkLo18un/Pz8yOMAAAAAOTrIzbcfAEAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8ggjAAAAAJZHGAEAAACwPMIIAAAAgOURRgAAAAAsjzACAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYnkvDaMKECbrjjjtUrlw5BQUFqVu3boqPj//N4xYuXKi6devK29tbjRo10sqVK105JgAAAACLc2kYbdy4UdHR0dq2bZvWrFmjrKws3XfffUpLS8vzmC1btqhXr14aMGCA9uzZo27duqlbt246cOCAK0cFAAAAYGE2wzCMovpmP/30k4KCgrRx40bdc889ua7p2bOn0tLStGLFCue+O++8U02bNtXMmTNvWZ+RkaGMjAzn1ykpKQoPD1dycrL8/PwK/0UAAAAAKBFSUlLk7++frzYo0s8YJScnS5IqVKiQ55qtW7eqY8eOOfZ16tRJW7duzXX9hAkT5O/v79zCw8MLb2AAAAAAllBkYeRwODRs2DDdddddatiwYZ7rEhMTFRwcnGNfcHCwEhMTc10/duxYJScnO7fTp08X6twlyYYNG2Sz2XTlypV8H1O9enVNmTLld39Pm82mpUuX/u7jXf18AAAAQH4UWRhFR0frwIEDmj9/fqE+r91ul5+fX46tOOrXr59sNpuef/75Wx6Ljo6WzWZTv379in6wUsgwDI0bN06hoaHy8fFRx44ddeTIkdse8/rrr8tms+XY6tatW0QTAwAAwGxFEkZDhgzRihUrtH79elWpUuW2a0NCQpSUlJRjX1JSkkJCQlw5YpEIDw/X/Pnzdf36dee+9PR0zZs3T1WrVjVxstJl4sSJmjp1qmbOnKnt27erTJky6tSpk9LT0297XIMGDXT+/HnntmnTpiKaGAAAAGZzaRgZhqEhQ4ZoyZIlWrdunWrUqPGbx0RFRWnt2rU59q1Zs0ZRUVGuGrPING/eXOHh4Vq8eLFz3+LFi1W1alU1a9Ysx9qMjAy9+OKLCgoKkre3t+6++27t3Lkzx5qVK1eqTp068vHxUfv27XXy5MlbvuemTZv0pz/9ST4+PgoPD9eLL75427sC5uazzz5TgwYNZLfbFRoaqiFDhuS5dv/+/frzn/8sHx8fVaxYUc8++6xSU1N/9/ONHz9eoaGh2rdvX75mNQxDU6ZM0V//+ld17dpVjRs31ueff65z58795iV6Hh4eCgkJcW6VKlXK8byvv/66qlatKrvdrrCwML344ov5mgkAAADFn0vDKDo6Wl988YXmzZuncuXKKTExUYmJiTneMenTp4/Gjh3r/Hro0KFatWqVJk2apMOHD+v111/Xrl27bvsfzyXJ008/rVmzZjm//uyzz9S/f/9b1r388statGiR5syZo5iYGEVERKhTp066fPmyJOn06dN65JFH1KVLF8XGxuqZZ57RmDFjcjzHsWPH1LlzZ3Xv3l379u3TV199pU2bNhXoXM6YMUPR0dF69tlntX//fi1fvlwRERG5rk1LS1OnTp1Uvnx57dy5UwsXLtT333+f4/vl9/kMw9ALL7ygzz//XP/5z3/UuHFjSTcveatevXqe8544cUKJiYk5buDh7++v1q1b53kDj18cOXJEYWFhqlmzpnr37q2EhATnY4sWLdLkyZP18ccf68iRI1q6dKkaNWp02+cDAABACWK4kKRct1mzZjnXtG3b1ujbt2+O4xYsWGDUqVPH8PLyMho0aGB8++23+f6eycnJhiQjOTm5kF5F4ejbt6/RtWtX48KFC4bdbjdOnjxpnDx50vD29jZ++ukno2vXrs7zkJqaanh6ehpz5851Hp+ZmWmEhYUZEydONAzDMMaOHWvUr18/x/cYPXq0Icn4+eefDcMwjAEDBhjPPvtsjjX/+c9/DDc3N+P69euGYRhGtWrVjMmTJ+c5d1hYmPHqq6/m+bgkY8mSJYZhGMYnn3xilC9f3khNTXU+/u233xpubm5GYmJivp9v4cKFxhNPPGHUq1fPOHPmTI7Hp02bZvz5z3/O8/jNmzcbkoxz587l2N+jRw/jsccey/O4lStXGgsWLDD27t1rrFq1yoiKijKqVq1qpKSkGIZhGJMmTTLq1KljZGZm5vkcAAAAKF4K0gYeLo6u31yzYcOGW/b16NFDPXr0cMFE5gsMDNSDDz6o2bNnyzAMPfjggzku2ZJuvtOTlZWlu+66y7nP09NTrVq1UlxcnCQpLi5OrVu3znHcry833Lt3r/bt26e5c+c69xmGIYfDoRMnTqhevXq3nfXChQs6d+6cOnTokK/XFhcXpyZNmqhMmTLOfXfddZccDofi4+Nls9ny9XwvvfSS7Ha7tm3bdsu5GTJkiEvePbz//vud/7tx48Zq3bq1qlWrpgULFmjAgAHq0aOHpkyZopo1a6pz58564IEH1KVLF3l4uPT/hQAAAFBEivTvGOGmp59+WrNnz9acOXP09NNPu+z7pKam6rnnnlNsbKxz27t3r44cOaJatWr95vE+Pj6FOk9+n+/ee+/V2bNntXr16gJ/j19u0vFHb+AREBCgOnXq6OjRo5Ju3jgjPj5e//jHP+Tj46PBgwfrnnvuUVZWVoFnBAAAQPFDGJmgc+fOyszMVFZWljp16nTL47Vq1ZKXl5c2b97s3JeVlaWdO3eqfv36kqR69eppx44dOY7btm1bjq+bN2+uQ4cOKSIi4pbNy8vrN+csV66cqlevfsvNMPJSr1497d27N8fNHTZv3iw3NzdFRkbm+/keeughzZs3T88880yBb+9eo0YNhYSE5PgeKSkp2r59e4Fu4JGamqpjx44pNDTUuc/Hx0ddunTR1KlTtWHDBm3dulX79+8v0HwAAAAonggjE7i7uysuLk6HDh2Su7v7LY+XKVNGgwYN0qhRo7Rq1SodOnRIAwcO1LVr1zRgwABJ0vPPP68jR45o1KhRio+P17x58zR79uwczzN69Ght2bJFQ4YMUWxsrI4cOaJly5YV6FK0119/XZMmTdLUqVN15MgRxcTEaNq0abmu7d27t7y9vdW3b18dOHBA69ev1wsvvKCnnnrK+Ud78/t8Dz/8sP7v//5P/fv319dff+3c/9FHH932UjybzaZhw4bpb3/7m5YvX679+/erT58+CgsLU7du3ZzrOnTooI8++sj59ciRI7Vx40adPHlSW7Zs0cMPPyx3d3f16tVLkjR79mx9+umnOnDggI4fP64vvvhCPj4+qlatWr7PJQAAAIovPiBhkt/6Q7TvvvuuHA6HnnrqKV29elUtW7bU6tWrVb58eUlS1apVtWjRIr300kuaNm2aWrVqpXfeeSfHpXmNGzfWxo0b9eqrr+pPf/qTDMNQrVq11LNnz3zP2bdvX6Wnp2vy5MkaOXKkKlWqpEcffTTXtb6+vlq9erWGDh2qO+64Q76+vurevbs+/PDD3/V8jz76qPMcuLm56ZFHHtHFixd17Nix28788ssvKy0tTc8++6yuXLmiu+++W6tWrZK3t7dzzbFjx3Tx4kXn12fOnFGvXr106dIlBQYG6u6779a2bdsUGBgo6ealde+++66GDx+u7OxsNWrUSN98840qVqyY73MJAACA4stm5OcOCSVISkqK/P39lZyc/JvxAQAAAKD0KkgbcCkdAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxiVMv369ctxW+qCateunYYNG1Zo8xT28wEAAACuQBgVkX79+slms8lms8nT01M1atTQyy+/rPT0dLNHK/YOHjyo7t27q3r16rLZbJoyZcotayZMmKA77rhD5cqVU1BQkLp166b4+Ph8f4/58+fLZrPliMqsrCyNHj1ajRo1UpkyZRQWFqY+ffro3LlzzjUbNmxw/t/119vOnTv/yMsGAABAESKMilDnzp11/vx5HT9+XJMnT9bHH3+s8ePHmz1WsXft2jXVrFlT7777rkJCQnJds3HjRkVHR2vbtm1as2aNsrKydN999yktLe03n//kyZMaOXKk/vSnP93yfWNiYvTaa68pJiZGixcvVnx8vB566CHnmjZt2uj8+fM5tmeeeUY1atRQy5Yt/9gLBwAAQJEhjIqQ3W5XSEiIwsPD1a1bN3Xs2FFr1qxxPu5wODRhwgTVqFFDPj4+atKkib7++mvn49nZ2RowYIDz8cjISP39738v8BybN29Wu3bt5Ovrq/Lly6tTp076+eefc137888/q0+fPipfvrx8fX11//3368iRI7/7+b799lv5+/tr7ty5+Z73jjvu0Pvvv6/HH39cdrs91zWrVq1Sv3791KBBAzVp0kSzZ89WQkKCdu/efdvnzs7OVu/evfXGG2+oZs2aOR7z9/fXmjVr9NhjjykyMlJ33nmnPvroI+3evVsJCQmSJC8vL4WEhDi3ihUratmyZerfv79sNpsk6dSpU+rSpYvKly+vMmXKqEGDBlq5cmW+Xz8AAABcjzAyyYEDB7RlyxZ5eXk5902YMEGff/65Zs6cqYMHD+qll17Sk08+qY0bN0q6GU5VqlTRwoULdejQIY0bN06vvPKKFixYkO/vGxsbqw4dOqh+/fraunWrNm3apC5duig7OzvX9f369dOuXbu0fPlybd26VYZh6IEHHlBWVlaBn2/evHnq1auX5s6dq969e0v676VoJ0+ezPdryI/k5GRJUoUKFW677s0331RQUJAGDBiQ7+e12WwKCAjI9fHly5fr0qVL6t+/v3NfdHS0MjIy9MMPP2j//v167733VLZs2fy9EAAAABQJD7MHsJIVK1aobNmyunHjhjIyMuTm5qaPPvpIkpSRkaF33nlH33//vaKioiRJNWvW1KZNm/Txxx+rbdu28vT01BtvvOF8vho1amjr1q1asGCBHnvssXzNMHHiRLVs2VL/+Mc/nPsaNGiQ69ojR45o+fLl2rx5s9q0aSNJmjt3rsLDw7V06VL16NEj3883ffp0vfrqq/rmm2/Utm1b535fX19FRkbK09MzX/Pnh8Ph0LBhw3TXXXepYcOGea7btGmTPv30U8XGxubredPT0zV69Gj16tUrz7+c/Omnn6pTp06qUqWKc19CQoK6d++uRo0aSdIt70wBAADAfIRREWrfvr1mzJihtLQ0TZ48WR4eHurevbsk6ejRo7p27ZruvffeHMdkZmaqWbNmzq+nT5+uzz77TAkJCbp+/boyMzPVtGnTfM8QGxurHj165GttXFycPDw81Lp1a+e+ihUrKjIyUnFxcfl+vq+//loXLlzQ5s2bdccdd+R4rFWrVjp8+HC+58+P6OhoHThwQJs2bcpzzdWrV/XUU0/pn//8pypVqvSbz5mVlaXHHntMhmFoxowZua45c+aMVq9efcs7eC+++KIGDRqk7777Th07dlT37t3VuHHjgr0oAAAAuBSX0hWhMmXKKCIiQk2aNNFnn32m7du369NPP5UkpaamSrr5GZzY2FjndujQIefnjObPn6+RI0dqwIAB+u677xQbG6v+/fsrMzMz3zP4+PgU6mvKz/M1a9ZMgYGB+uyzz2QYRqF+/18bMmSIVqxYofXr1+d41+bXjh07ppMnT6pLly7y8PCQh4eHPv/8cy1fvlweHh46duyYc+0vUXTq1CmtWbMmz3eLZs2apYoVK+a4OYMkPfPMMzp+/Lieeuop7d+/Xy1bttS0adMK5wUDAACgUBBGJnFzc9Mrr7yiv/71r7p+/brq168vu92uhIQERURE5NjCw8MlyXlJ2+DBg9WsWTNFRETk+A/4/GjcuLHWrl2br7X16tXTjRs3tH37due+S5cuKT4+XvXr18/389WqVUvr16/XsmXL9MILLxRo3vwyDENDhgzRkiVLtG7dOtWoUeO26+vWrav9+/fniNCHHnpI7du3V2xsrPOc/xJFR44c0ffff6+KFSvm+f1nzZqlPn365HpZYHh4uJ5//nktXrxYI0aM0D//+c8//qIBAABQaAgjE/Xo0UPu7u6aPn26ypUrp5EjR+qll17SnDlzdOzYMcXExGjatGmaM2eOJKl27dratWuXVq9erR9//FGvvfZagf9WztixY7Vz504NHjxY+/bt0+HDhzVjxgxdvHjxlrW1a9dW165dNXDgQG3atEl79+7Vk08+qcqVK6tr164Fer46depo/fr1WrRoUY4/+Lpjxw7VrVtXZ8+ezXPmzMxMZ7xkZmbq7Nmzio2N1dGjR51roqOj9cUXX2jevHkqV66cEhMTlZiYqOvXrzvX9OnTR2PHjpUkeXt7q2HDhjm2gIAAlStXTg0bNpSXl5eysrL06KOPateuXZo7d66ys7Odz/vrd+nWrVunEydO6Jlnnrll/mHDhmn16tU6ceKEYmJitH79etWrV+82/1cCAABAUSOMTOTh4aEhQ4Zo4sSJSktL01tvvaXXXntNEyZMUL169dS5c2d9++23znc/nnvuOT3yyCPq2bOnWrdurUuXLmnw4MEF+p516tTRd999p71796pVq1aKiorSsmXL5OGR+8fNZs2apRYtWugvf/mLoqKiZBiGVq5c6XxXpCDPFxkZqXXr1unLL7/UiBEjJN38W0Hx8fHOu9zl5ty5c2rWrJmaNWum8+fP64MPPlCzZs1yRMiMGTOUnJysdu3aKTQ01Ll99dVXzjUJCQk6f/58vs/V2bNntXz5cp05c0ZNmzbN8bxbtmzJsfbTTz9VmzZtVLdu3VueJzs7W9HR0c7/m9apUyfHzSoAAABgPpvh6g99FLGUlBT5+/srOTk5z8+CAAAAACj9CtIGvGMEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8ggjAAAAAJZHGAEAAACwPMIIAAAAgOURRgAAAAAsjzACAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8ggjAAAAAJZHGAEAAACwPMIIAAAAgOURRgAAAAAsjzACAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAy3NpGP3www/q0qWLwsLCZLPZtHTp0tuu37Bhg2w22y1bYmKiK8cEAAAAYHEuDaO0tDQ1adJE06dPL9Bx8fHxOn/+vHMLCgpy0YQAAAAAIHm48snvv/9+3X///QU+LigoSAEBAYU/EAAAAADkolh+xqhp06YKDQ3Vvffeq82bN992bUZGhlJSUnJsAAAAAFAQxSqMQkNDNXPmTC1atEiLFi1SeHi42rVrp5iYmDyPmTBhgvz9/Z1beHh4EU4MAAAAoDSwGYZhFMk3stm0ZMkSdevWrUDHtW3bVlWrVtX//d//5fp4RkaGMjIynF+npKQoPDxcycnJ8vPz+yMjAwAAACjBUlJS5O/vn682cOlnjApDq1attGnTpjwft9vtstvtRTgRAAAAgNKmWF1Kl5vY2FiFhoaaPQYAAACAUsyl7xilpqbq6NGjzq9PnDih2NhYVahQQVWrVtXYsWN19uxZff7555KkKVOmqEaNGmrQoIHS09P1r3/9S+vWrdN3333nyjEBAAAAWJxLw2jXrl1q37698+vhw4dLkvr27avZs2fr/PnzSkhIcD6emZmpESNG6OzZs/L19VXjxo31/fff53gOAAAAAChsRXbzhaJSkA9YAQAAACi9CtIGxf4zRgAAAADgaoQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8ggjAAAAAJZHGAEAAACwPMIIAAAAgOURRgAAAAAsjzACAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8ggjAAAAAJZHGAEAAACwPMIIAAAAgOURRgAAAAAsjzACAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8ggjAAAAAJZHGAEAAACwPMIIAAAAgOURRgAAAAAsjzACAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACW59Iw+uGHH9SlSxeFhYXJZrNp6dKlv3nMhg0b1Lx5c9ntdkVERGj27NmuHBEAAAAAXBtGaWlpatKkiaZPn56v9SdOnNCDDz6o9u3bKzY2VsOGDdMzzzyj1atXu3JMAAAAABbn4conv//++3X//ffne/3MmTNVo0YNTZo0SZJUr149bdq0SZMnT1anTp1yPSYjI0MZGRnOr1NSUv7Y0AAAAAAsp1h9xmjr1q3q2LFjjn2dOnXS1q1b8zxmwoQJ8vf3d27h4eGuHhMAAABAKVOswigxMVHBwcE59gUHByslJUXXr1/P9ZixY8cqOTnZuZ0+fbooRgUAAABQirj0UrqiYLfbZbfbzR4DAAAAQAlWrN4xCgkJUVJSUo59SUlJ8vPzk4+Pj0lTAQAAACjtilUYRUVFae3atTn2rVmzRlFRUSZNBAAAAMAKXBpGqampio2NVWxsrKSbt+OOjY1VQkKCpJufD+rTp49z/fPPP6/jx4/r5Zdf1uHDh/WPf/xDCxYs0EsvveTKMQEAAABYnEvDaNeuXWrWrJmaNWsmSRo+fLiaNWumcePGSZLOnz/vjCRJqlGjhr799lutWbNGTZo00aRJk/Svf/0rz1t1AwAAAEBhsBmGYZg9RGFKSUmRv7+/kpOT5efnZ/Y4AAAAAExSkDYoVp8xAgAAAAAzEEYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8ggjAAAAAJZHGAEAAACwPMIIAAAAgOURRgAAAAAsjzACAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8ggjAAAAAJZHGAEAAACwPMIIAAAAgOURRgAAAAAsjzACAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8ggjAAAAAJZHGAEAAACwPMIIAAAAgOURRgAAAAAsjzACAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAyyuSMJo+fbqqV68ub29vtW7dWjt27Mhz7ezZs2Wz2XJs3t7eRTEmAAAAAItyeRh99dVXGj58uMaPH6+YmBg1adJEnTp10oULF/I8xs/PT+fPn3dup06dcvWYAAAAACzM5WH04YcfauDAgerfv7/q16+vmTNnytfXV5999lmex9hsNoWEhDi34OBgV48JAAAAwMJcGkaZmZnavXu3Onbs+N9v6Oamjh07auvWrXkel5qaqmrVqik8PFxdu3bVwYMH81ybkZGhlJSUHBsAAAAAFIRLw+jixYvKzs6+5R2f4OBgJSYm5npMZGSkPvvsMy1btkxffPGFHA6H2rRpozNnzuS6fsKECfL393du4eHhhf46AAAAAJRuxe6udFFRUerTp4+aNm2qtm3bavHixQoMDNTHH3+c6/qxY8cqOTnZuZ0+fbqIJwYAAABQ0nm48skrVaokd3d3JSUl5diflJSkkJCQfD2Hp6enmjVrpqNHj+b6uN1ul91u/8OzAgAAALAul75j5OXlpRYtWmjt2rXOfQ6HQ2vXrlVUVFS+niM7O1v79+9XaGioq8YEAAAAYHEufcdIkoYPH66+ffuqZcuWatWqlaZMmaK0tDT1799fktSnTx9VrlxZEyZMkCS9+eabuvPOOxUREaErV67o/fff16lTp/TMM8+4elQAAAAAFuXyMOrZs6d++uknjRs3TomJiWratKlWrVrlvCFDQkKC3Nz++8bVzz//rIEDByoxMVHly5dXixYttGXLFtWvX9/VowIAAACwKJthGIbZQxSmlJQU+fv7Kzk5WX5+fmaPAwAAAMAkBWmDYndXOgAAAAAoaoQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8ggjAAAAAJZHGAEAAACwPMIIAAAAgOURRgAAAAAsjzACAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8ggjAAAAAJZHGAEAAACwPMIIAAAAgOURRgAAAAAsjzACAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8ggjAAAAAJZHGAEAAACwPMIIAAAAgOURRgAAAAAsjzACAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWVyRhNH36dFWvXl3e3t5q3bq1duzYcdv1CxcuVN26deXt7a1GjRpp5cqVRTEmAAAAAItyeRh99dVXGj58uMaPH6+YmBg1adJEnTp10oULF3Jdv2XLFvXq1UsDBgzQnj171K1bN3Xr1k0HDhxw9agAgFIkK9uhrGyH2WMAAEoIm2EYhiu/QevWrXXHHXfoo48+kiQ5HA6Fh4frhRde0JgxY25Z37NnT6WlpWnFihXOfXfeeaeaNm2qmTNn3rI+IyNDGRkZzq9TUlIUHh6u5ORk+fn5ueAVAQCKu6SUdA2ZF6OGlf01vksDs8cBAJgkJSVF/v7++WoDl75jlJmZqd27d6tjx47//YZuburYsaO2bt2a6zFbt27NsV6SOnXqlOf6CRMmyN/f37mFh4cX3gsAAJQ4W49d0oNTN2nnyZ/19a4zupCSbvZIAIASwKVhdPHiRWVnZys4ODjH/uDgYCUmJuZ6TGJiYoHWjx07VsnJyc7t9OnThTM8AKBEcTgMzdhwTL3/tU0XUzMUGVxOy4bcpSA/b7NHAwCUAB5mD/BH2e122e12s8cAAJgo+XqWRizYq+/jkiRJjzSrrL893FC+XiX+nzkAQBFx6b8YlSpVkru7u5KSknLsT0pKUkhISK7HhISEFGg9AMDaDp5L1qAvYpRw+Zq83N30+kMN1KtVuGw2m9mjAQBKEJdeSufl5aUWLVpo7dq1zn0Oh0Nr165VVFRUrsdERUXlWC9Ja9asyXM9AMC6vtqZoIf/sUUJl6+pcoCPvh4UpSdaVyWKAAAF5vJrDIYPH66+ffuqZcuWatWqlaZMmaK0tDT1799fktSnTx9VrlxZEyZMkCQNHTpUbdu21aRJk/Tggw9q/vz52rVrlz755BNXjwoAKCHSs7I1btkBLdh1RpLUPjJQk3s2VYCvl8mTAQBKKpeHUc+ePfXTTz9p3LhxSkxMVNOmTbVq1SrnDRYSEhLk5vbfN67atGmjefPm6a9//ateeeUV1a5dW0uXLlXDhg1dPSoAoAQ4dSlNg76I0aHzKbLZpBH31tHgdhFyc+NdIgDA7+fyv2NU1Apyr3IAQMny3cFEjVi4V1fTb6hiGS/9/fFmurt2JbPHAgAUUwVpA27XAwAo9m5kO/T+d/H6eONxSVLzqgGa3ru5Qv19TJ4MAFBaEEYAgGLtwtV0vTBvj7afuCxJ6n9XdY29v568PFx6/yAAgMUQRgCAYmvHicuKnhejn65mqIyXu957tLH+0jjM7LEAAKUQYQQAKHYMw9A//3Nc762KV7bDUO2gsprxZAtFBJU1ezQAQClFGAEAipWU9CyNWrhXqw/e/GPfXZuG6Z2HG6mMnX+yAACuw78yAIBiI+58igZ9sVsnL12Tp7tN47o00JP8wVYAQBEgjAAAxcLXu8/or0v3Kz3LocoBPpreu7mahgeYPRYAwCIIIwCAqdKzsvXGNwf15Y7TkqS2dQI1pWdTlS/jZfJkAAArIYwAAKY5ffmaBs3drQNnU2SzScM61NELf46QmxuXzgEAihZhBAAwxdq4JL30VaxS0m+ovK+npjzeTG3rBJo9FgDAoggjAECRynYY+nBNvKavPyZJahoeoOm9m6tygI/JkwEArIwwAgAUmYupGXrxyz3acuySJKlvVDW9+mB9eXm4mTwZAMDqCCMAQJHYdfKyoufFKCklQ75e7nq3e2M91CTM7LEAAJBEGAEAXMwwDH266YTe/fdh3XAYqhVYRjOfbKHaweXMHg0AACfCCADgMlfTszR60T6t3J8oSfpL41C9272xytr55wcAULzwLxMAwCXiE69q0Be7dfximjzdbXr1gXrq26a6bDZuxQ0AKH4IIwBAoVuy54xeWXxA17OyFervrem9m6t51fJmjwUAQJ4IIwBAocm4ka03vzmkudsTJEl/ql1JU3o2VcWydpMnAwDg9ggjAEChOH35mqLnxWjfmWRJ0osdamtoh9pyd+PSOQBA8UcYAQD+sPXxF/TSV7G6ci1LAb6emtyzqdpHBpk9FgAA+UYYAQB+t2yHob9//6OmrT8qw5AaV/HXP3o3V5XyvmaPBgBAgRBGAIDf5VJqhoZ9Fav/HLkoSXryzqp67S/1ZfdwN3kyAAAKjjACABTY7lM/a8i8GJ1PTpePp7smPNJI3ZpVNnssAAB+N8IIAJBvhmFo9paTevvbON1wGKpZqYxmPNlCkSHlzB4NAIA/hDACAORLWsYNjV60Tyv2nZckPdAoRO91b6xy3p4mTwYAwB9HGAEAftORpKt6/ovdOvZTmjzcbBr7QD09fVd12WzcihsAUDoQRgCA21oWe1ZjF+/XtcxsBfvZNf2J5mpZvYLZYwEAUKgIIwBArjJvOPT2t4c0Z+spSVKbWhU1tVczVSprN3kyAAAKH2EEALjF2SvXFT03RrGnr0iShrSP0Ev31pG7G5fOAQBKJ8IIAJDDDz/+pKHz9+jna1ny8/bQ5J5N1aFesNljAQDgUoQRAECS5HAYmrruiP6+9ogMQ2pY2U8zerdQeAVfs0cDAMDlCCMAgC6nZWrYV7H64cefJEm9WlXV+C715e3pbvJkAAAUDcIIACwu9vQVRc+N0dkr1+Xt6aa3uzVS9xZVzB4LAIAiRRgBgEUZhqEvtp3SmysOKSvbUPWKvprxZAvVC/UzezQAAIocYQQAFnQt84bGLt6vZbHnJEmdG4RoYo/G8vP2NHkyAADMQRgBgMUcvZCqQV/s1pELqXJ3s2lM57p65k81ZLNxK24AgHURRgBgISv2ndPor/cpLTNbQeXs+uiJ5mpVo4LZYwEAYDrCCAAsIPOGQxP+HadZm09Kku6sWUFTezVTUDlvcwcDAKCYIIwAoJQ7n3xd0XNjFJNwRZL0fNtaGnlfHXm4u5k7GAAAxQhhBACl2KYjF/Xi/D26nJapct4e+vCxprq3frDZYwEAUOwQRgBQCjkchv6x4agmrflRhiHVD/XTjCebq1rFMmaPBgBAsUQYAUApc+Vapl76Klbr43+SJPVsGa43ujaQt6e7yZMBAFB8EUYAUIrsO3NFg76I0dkr12X3cNNb3RrqsZbhZo8FAECxRxgBQClgGIbm7UjQG8sPKTPboaoVfDXjyeZqEOZv9mgAAJQIhBEAlHDXM7P16pL9WrznrCTp3vrB+qBHE/n7eJo8GQAAJQdhBAAl2PGfUjXoixjFJ12Vm016uXNdPXdPTdlsNrNHAwCgRCGMAKCE+vf+8xr19T6lZtxQpbJ2ffREM91Zs6LZYwEAUCIRRgBQwmRlO/Tevw/rX5tOSJJaVa+gj55opiA/b5MnAwCg5CKMAKAESUpJ15B5Mdp58mdJ0rP31NSoTpHydHczeTIAAEo2wggASogtxy7qxS/36GJqpsrZPfR+jybq3DDE7LEAACgVCCMAKOYcDkMzfzimD1bHy2FIdUPKacaTLVSjUhmzRwMAoNQgjACgGEu+lqURC2P1fdwFSdKjLarora4N5ePlbvJkAACULoQRABRTB84ma9Dc3Tp9+bq8PNz05kMN1POOcG7FDQCACxBGAFAMfbUzQa8tO6jMGw5VKe+jmU+2UMPK/maPBQBAqUUYAUAxcj0zW+OWHdDC3WckSR3qBunDx5rK39fT5MkAACjdCCMAKCZOXkzToLkxijufIjebNOK+SA1qW0tublw6BwCAqxFGAFAMrD6YqJEL9upqxg1VKuulqY83U5uISmaPBQCAZRBGAGCiG9kOvb86Xh//cFyS1LJaeX30RHOF+HubPBkAANZCGAGASS6kpGvIl3u048RlSdKAu2tozP115enuZvJkAABYD2EEACbYdvySXvhyj366mqGydg9NfLSxHmgUavZYAABYFmEEAEXIMAx98sNxTVwdr2yHocjgcprxZHPVDCxr9mgAAFgaYQQARSQlPUsjF+zVd4eSJEmPNKusvz3cUL5e/CoGAMBs/GsMAEXg0LkUDZq7W6cuXZOXu5vGP1RfT7SqKpuNW3EDAFAcEEYA4GILd53WX5ceUMYNhyoH+GjGk83VuEqA2WMBAID/QRgBgIukZ2Xr9eUHNX/naUlSu8hATX6sqcqX8TJ5MgAA8GuEEQC4QMKlaxo0d7cOnkuRzSYN71hH0e0j5ObGpXMAABRHhBEAFLLvDyVp+IJYpaTfUIUyXpr6eDPdXbuS2WMBAIDbIIwAoJDcyHZo0pofNWPDMUlS86oBmt67uUL9fUyeDAAA/BbCCAAKwU9XM/Til3u09fglSVK/NtX1ygP15OXhZvJkAAAgPwgjAPiDdp68rOi5MbpwNUNlvNz1bvfG6tIkzOyxAABAARBGAPA7GYahTzed0IR/H1a2w1DtoLKa8WQLRQSVNXs0AABQQIQRAPwOV9Oz9PLX+/TvA4mSpK5Nw/TOw41Uxs6vVQAASiL+BQeAAjqcmKJBX8ToxMU0ebrbNO4v9fXkndVks3ErbgAASirCCAAKYHHMGb2yZL/SsxwK8/fW9N7N1axqebPHAgAAfxBhBAD5kJ6VrTdXHNK87QmSpHvqBGpKz6aqUMbL5MkAAEBhIIwA4DecvnxNg+fGaP/ZZNls0tAOtfXCn2vL3Y1L5wAAKC1c+gc2Ll++rN69e8vPz08BAQEaMGCAUlNTb3tMu3btZLPZcmzPP/+8K8cEgDytP3xBf5m2SfvPJqu8r6dm92+lYR3rEEUAAJQyLn3HqHfv3jp//rzWrFmjrKws9e/fX88++6zmzZt32+MGDhyoN9980/m1r6+vK8cEgFtkOwxN+f5HTVt3VJLUJDxA/+jdXJUDfEyeDAAAuILLwiguLk6rVq3Szp071bJlS0nStGnT9MADD+iDDz5QWFjef/zQ19dXISEhrhoNAG7rUmqGhs6P1aajFyVJfaKq6dUH68nu4W7yZAAAwFVcdind1q1bFRAQ4IwiSerYsaPc3Ny0ffv22x47d+5cVapUSQ0bNtTYsWN17dq1PNdmZGQoJSUlxwYAv9fuUz/rwambtOnoRfl4uuvvjzfVm10bEkUAAJRyLnvHKDExUUFBQTm/mYeHKlSooMTExDyPe+KJJ1StWjWFhYVp3759Gj16tOLj47V48eJc10+YMEFvvPFGoc4OwHoMw9DsLSf19rdxuuEwVCuwjGY+2UK1g8uZPRoAACgCBQ6jMWPG6L333rvtmri4uN890LPPPuv8340aNVJoaKg6dOigY8eOqVatWresHzt2rIYPH+78OiUlReHh4b/7+wOwntSMGxq9aJ++3XdekvSXxqF6t3tjlbVz404AAKyiwP/qjxgxQv369bvtmpo1ayokJEQXLlzIsf/GjRu6fPlygT4/1Lp1a0nS0aNHcw0ju90uu92e7+cDgP/1Y9JVPf/Fbh3/KU0ebjb99cF66tumumw27joHAICVFDiMAgMDFRgY+JvroqKidOXKFe3evVstWrSQJK1bt04Oh8MZO/kRGxsrSQoNDS3oqABwW8tiz2rMov26npWtED9vTe/dXC2qlTd7LAAAYAKX3XyhXr166ty5swYOHKgdO3Zo8+bNGjJkiB5//HHnHenOnj2runXraseOHZKkY8eO6a233tLu3bt18uRJLV++XH369NE999yjxo0bu2pUABaTcSNbry09oKHzY3U9K1t3R1TSty/eTRQBAGBhLr2Afu7cuRoyZIg6dOggNzc3de/eXVOnTnU+npWVpfj4eOdd57y8vPT9999rypQpSktLU3h4uLp3766//vWvrhwTgIWcvXJdg+fGaO/pK5KkF/8coaH8wVYAACzPZhiGYfYQhSklJUX+/v5KTk6Wn5+f2eMAKEY2/viThs3fo5+vZSnA11OTezZV+8ig3z4QAACUSAVpA265BKDUy3YYmrr2iKauOyLDkBpX8df0J5orvIKv2aMBAIBigjACUKpdTsvU0Pl79J8jFyVJvVtX1bgu9fmDrQAAIAfCCECptSfhZ0XPjdG55HR5e7rpnYcb6ZHmVcweCwAAFEOEEYBSxzAM/d+2U3prxSFlZRuqWamMZjzZQpEh5cweDQAAFFOEEYBSJS3jhsYu3q/le89Jku5vGKKJjzZWOW9PkycDAADFGWEEoNQ4euGqnv8iRkcvpMrDzaYx99fVgLtryGbjVtwAAOD2CCMApcI3e89p9KJ9upaZrWA/u6Y/0Vwtq1cweywAAFBCEEYASrTMGw69szJOs7eclCS1qVVRf3+8mQLL2c0dDAAAlCiEEYAS63zydQ2eG6M9CVckSdHta2n4vZFyd+PSOQAAUDCEEYASadORi3px/h5dTsuUn7eHPnysqTrWDzZ7LAAAUEIRRgBKFIfD0Efrj2ry9z/KMKSGlf30jydaqGpFX7NHAwAAJRhhBKDEOHYhVY9/slU/pWZKknq1Ctf4Lg3k7elu8mQAAKCkI4wAlBjR82KcUfT2ww3Vu3U1kycCAAClhZvZAwBAfn3Wr6XsHjd/bf2YeNXkaQAAQGlCGAEoMcICfPXPPi0lSXO2ntKWoxdNnggAAJQWhBGAEuWeOoF6onVVSdKor/fpanqWyRMBAIDSgDACUOK88kA9VSnvo7NXruudlXFmjwMAAEoBwghAiVPW7qH3H20iSfpyx2ltiL9g8kQAAKCkI4wAlEhRtSqqX5vqkqQxi/Yr+TqX1AEAgN+PMAJQYo3uXFfVK/oqMSVdb35zyOxxAABACUYYASixfLzc9UGPJrLZpEUxZ7TmUJLZIwEAgBKKMAJQorWsXkED/1RTkjR28X79nJZp8kQAAKAkIowAlHjD762jiKCyupiaoXHLD5o9DgAAKIEIIwAlnrenuyb1aCJ3N5u+2XtOK/efN3skAABQwhBGAEqFJuEBGtS2liTpr0sP6GJqhskTAQCAkoQwAlBqvNihtuqGlNPltEy9umS/DMMweyQAAFBCEEYASg0vDzdNeqyJPNxsWn0wSctiz5k9EgAAKCEIIwClSoMwf73YobYkafzyg0pKSTd5IgAAUBIQRgBKnUHtaqlRZX8lX8/S2MVcUgcAAH4bYQSg1PF0v3lJnZe7m9YdvqCFu8+YPRIAACjmCCMApVKd4HJ66d46kqS3vjmkc1eumzwRAAAozggjAKXWs/fUVLOqAbqacUOjF+3jkjoAAJAnwghAqeXuZtMHPZrI7uGm/xy5qLnbE8weCQAAFFOEEYBSrVZgWb3cua4k6Z2VcUq4dM3kiQAAQHFEGAEo9fq3qa5WNSroWma2Rn29Vw4Hl9QBAICcCCMApZ6bm00fPNpEvl7u2n7isuZsPWn2SAAAoJghjABYQtWKvhr7QD1J0nurDuv4T6kmTwQAAIoTwgiAZfRuVVV3RVRUepZDIxfuVTaX1AEAgP+PMAJgGW5uNk18tInK2j0Uk3BF//rPcbNHAgAAxQRhBMBSKgf46LW/3LykbtKaH3Uk6arJEwEAgOKAMAJgOY+1DFf7yEBl3nBoxMK9upHtMHskAABgMsIIgOXYbDa9272x/Lw9tO9MsmZsOGb2SAAAwGSEEQBLCvbz1htdG0iSpq47okPnUkyeCAAAmIkwAmBZ3ZpW1n31g5WVbWj4glhl3uCSOgAArIowAmBZNptNbz/cSOV9PXU48ao+WnfE7JEAAIBJCCMAlhZYzq6/dWskSZq+4Zj2nbli7kAAAMAUhBEAy3uwcagebByqbIehEQv2Kj0r2+yRAABAESOMAEDSW10bqlJZLx25kKrJ3/9o9jgAAKCIEUYAIKlCGS+98/DNS+r++cNx7T71s8kTAQCAokQYAcD/d1+DED3SrLIchjRy4V5dz+SSOgAArIIwAoD/Mb5LAwX72XXiYpomrj5s9jgAAKCIEEYA8D/8fT31XvfGkqRZm09q2/FLJk8EAACKAmEEAL/SLjJIj98RLkka9fVepWXcMHkiAADgaoQRAOTi1QfrqXKAj05fvq4J/44zexwAAOBihBEA5KKct6cmPnrzkrovtiVo05GLJk8EAABciTACgDzcFVFJT91ZTZL08td7lZKeZfJEAADAVQgjALiNMffXVdUKvjqXnK6/rThk9jgAAMBFCCMAuI0ydg990KOJbDZpwa4zWnc4yeyRAACACxBGAPAbWtWooKfvqiFJGrNov65cyzR5IgAAUNgIIwDIh1GdIlUzsIwuXM3Q68sPmj0OAAAoZIQRAOSDt6e7JvVoIjebtDT2nFYdSDR7JAAAUIgIIwDIp2ZVy+u5trUkSa8u2a9LqRkmTwQAAAoLYQQABTCsY23VCS6rS2mZGreMS+oAACgtCCMAKAC7h7sm9Wgqdzebvt1/Xt/sPWf2SAAAoBAQRgBQQI2q+Cu6fYQk6bVlB3TharrJEwEAgD+KMAKA32FI+wjVD/XTlWtZemXxARmGYfZIAADgDyCMAOB38PJw04c9m8jT3abv45K0OOas2SMBAIA/gDACgN+pboifhnWsI0l6/ZuDOp983eSJAADA70UYAcAf8Nw9NdUkPEBX029o9KL9XFIHAEAJRRgBwB/g4e6mST0ay8vDTT/8+JPm7zxt9kgAAOB3IIwA4A+KCCqnUfdFSpL+tuKQzvx8zeSJAABAQRFGAFAInr67hlpWK6+0zGy9/PU+ORxcUgcAQElCGAFAIXB3s+n9Hk3k7emmLccu6Yvtp8weCQAAFABhBACFpEalMhrTua4kacLKwzp5Mc3kiQAAQH4RRgBQiPpEVVdUzYq6npWtUV/vVTaX1AEAUCIQRgBQiNzcbJr4aGOV8XLXzpM/a9bmE2aPBAAA8oEwAoBCFl7BV68+WF+SNHF1vI5eSDV5IgAA8FtcFkZvv/222rRpI19fXwUEBOTrGMMwNG7cOIWGhsrHx0cdO3bUkSNHXDUiALhMr1bhuqdOoDJvODRi4V7dyHaYPRIAALgNl4VRZmamevTooUGDBuX7mIkTJ2rq1KmaOXOmtm/frjJlyqhTp05KT0931ZgA4BI2m03vdW+kct4e2nv6ij75z3GzRwIAALdhMwzDpZ8Mnj17toYNG6YrV67cdp1hGAoLC9OIESM0cuRISVJycrKCg4M1e/ZsPf7447kel5GRoYyMDOfXKSkpCg8PV3Jysvz8/ArtdQDA7/H17jMauXCvvNzd9M0LdysypJzZIwEAYBkpKSny9/fPVxsUm88YnThxQomJierYsaNzn7+/v1q3bq2tW7fmedyECRPk7+/v3MLDw4tiXADIl+7NK6tD3SBlZjs0fEGssrikDgCAYqnYhFFiYqIkKTg4OMf+4OBg52O5GTt2rJKTk53b6dOnXTonABSEzWbThEcayd/HUwfPpWj6+qNmjwQAAHJRoDAaM2aMbDbbbbfDhw+7atZc2e12+fn55dgAoDgJ8vPWm10bSJI+WndUB84mmzwRAAD4NY+CLB4xYoT69et32zU1a9b8XYOEhIRIkpKSkhQaGurcn5SUpKZNm/6u5wSA4uKhJmFadSBR/z6QqBEL9mr5C3fJ7uFu9lgAAOD/K1AYBQYGKjAw0CWD1KhRQyEhIVq7dq0zhFJSUrR9+/YC3dkOAIojm82mv3VrqB0nLis+6ar+/v0Rvdy5rtljAQCA/89lnzFKSEhQbGysEhISlJ2drdjYWMXGxio19b9/6LBu3bpasmSJpJv/0TBs2DD97W9/0/Lly7V//3716dNHYWFh6tatm6vGBIAiU7GsXW8/3FCSNHPjMe1J+NnkiQAAwC8K9I5RQYwbN05z5sxxft2sWTNJ0vr169WuXTtJUnx8vJKT/3ut/csvv6y0tDQ9++yzunLliu6++26tWrVK3t7erhoTAIpU54ah6to0TMtiz2nEwr1a+eKf5O3JJXUAAJjN5X/HqKgV5F7lAGCGK9cydd/kH3ThaoYG/qmGXn2wvtkjAQBQKpXIv2MEAFYR4OulCY80kiT9a9MJ7Tx52eSJAAAAYQQAJuhQL1iPtqgiw5BGLtyra5k3zB4JAABLI4wAwCTjutRXqL+3Tl26pvf+XbR/Aw4AAOREGAGASfy8PfVe98aSpDlbT2nL0YsmTwQAgHURRgBgonvqBOqJ1lUlSaO+3qer6VkmTwQAgDURRgBgslceqKcq5X109sp1vbMyzuxxAACwJMIIAExW1u6h9x9tIkn6csdpbYi/YPJEAABYD2EEAMVAVK2K6temuiRpzKL9Sr7OJXUAABQlwggAionRneuqekVfJaak681vDpk9DgAAlkIYAUAx4ePlrg96NJHNJi2KOaM1h5LMHgkAAMsgjACgGGlZvYIG/qmmJGns4v36OS3T5IkAALAGwggAipnh99ZRRFBZXUzN0LjlB80eBwAASyCMAKCY8fZ016QeTeTuZtM3e89p5f7zZo8EAECpRxgBQDHUJDxAg9rWkiT9dekBXUzNMHkiAABKN8IIAIqpFzvUVt2QcrqclqlXl+yXYRhmjwQAQKlFGAFAMeXl4aZJjzWRh5tNqw8maVnsObNHAgCg1CKMAKAYaxDmrxc71JYkjV9+UEkp6SZPBABA6UQYAUAxN6hdLTWq7K/k61kau5hL6gAAcAXCCACKOU/3m5fUebm7ad3hC1q4+4zZIwEAUOoQRgBQAtQJLqeX7q0jSXrrm0M6d+W6yRMBAFC6EEYAUEI8e09NNasaoKsZNzR60T4uqQMAoBARRgBQQri72fRBjyaye7jpP0cuau72BLNHAgCg1CCMAKAEqRVYVi93ritJemdlnBIuXTN5IgAASgfCCABKmP5tqqtVjQq6lpmtUV/vlcPBJXUAAPxRhBEAlDBubjZ98GgT+Xq5a/uJy5q95aTZIwEAUOIRRgBQAlWt6KuxD9STJE1cfVjHf0o1eSIAAEo2wggASqjerarqroiKSs9yaOTCvcrmkjoAAH43wggASig3N5smPtpEZe0eikm4on/957jZIwEAUGIRRgBQglUO8NFrf7l5Sd2kNT/qSNJVkycCAKBkIowAoIR7rGW42kcGKvOGQyMW7tWNbIfZIwEAUOIQRgBQwtlsNr3bvbH8vD2070yyZmw4ZvZIAACUOIQRAJQCwX7eeqNrA0nS1HVHdOhciskTAQBQshBGAFBKdGtaWffVD1ZWtqHhC2KVeYNL6gAAyC/CCABKCZvNprcfbqTyvp46nHhVH607YvZIAACUGIQRAJQigeXs+lu3RpKk6RuOad+ZK+YOBABACUEYAUAp82DjUD3YOFTZDkMjFuxVela22SMBAFDsEUYAUAq91bWhKpX10pELqZr8/Y9mjwMAQLFHGAFAKVShjJfeefjmJXX//OG4dp/62eSJAAAo3ggjACil7msQokeaVZbDkEYu3KvrmVxSBwBAXggjACjFxndpoGA/u05cTNPE1YfNHgcAgGKLMAKAUszf11PvdW8sSZq1+aS2Hb9k8kQAABRPhBEAlHLtIoP0+B3hkqRRX+9VWsYNkycCAKD4IYwAwAJefbCeKgf46PTl65rw7zizxwEAoNghjADAAsp5e2riozcvqftiW4I2Hblo8kQAABQvhBEAWMRdEZX01J3VJEkvf71XKelZJk8EAEDxQRgBgIWMub+uqlbw1bnkdP1txSGzxwEAoNggjADAQsrYPfRBjyay2aQFu85o3eEks0cCAKBYIIwAwGJa1aigp++qIUkas2i/rlzLNHkiAADMRxgBgAWN6hSpmoFldOFqhl5fftDscQAAMB1hBAAW5O3prkk9msjNJi2NPadVBxLNHgkAAFMRRgBgUc2qltdzbWtJkl5dsl+XUjNMnggAAPMQRgBgYcM61lad4LK6lJapccu4pA4AYF2EEQBYmN3DXZN6NJW7m03f7j+vb/aeM3skAABMQRgBgMU1quKv6PYRkqTXlh3QhavpJk8EAEDRI4wAABrSPkL1Q/105VqWXll8QIZhmD0SAABFijACAMjLw00f9mwiT3ebvo9L0uKYs2aPBABAkSKMAACSpLohfhrWsY4k6fVvDup88nWTJwIAoOgQRgAAp+fuqakm4QG6mn5Doxft55I6AIBlEEYAACcPdzdN6tFYXh5u+uHHnzR/52mzRwIAoEgQRgCAHCKCymnUfZGSpL+tOKQzP18zeSIAAFyPMAIA3OLpu2uoZbXySsvM1stf75PDwSV1AIDSjTACANzC3c2m93s0kbenm7Ycu6Qvtp8yeyQAAFyKMAIA5KpGpTIa07muJGnCysM6eTHN5IkAAHAdwggAkKc+UdUVVbOirmdla9TXe5XNJXUAgFKKMAIA5MnNzaaJjzZWGS937Tz5s2ZtPmH2SAAAuARhBAC4rfAKvnr1wfqSpImr43X0QqrJEwEAUPgIIwDAb+rVKlz31AlU5g2HRizcqxvZDrNHAgCgUBFGAIDfZLPZ9F73Rirn7aG9p6/o4x+Omz0SAACFijACAORLqL+PxndpIEma8v2POpyYYvJEAAAUHsIIAJBv3ZtXVoe6QcrKNjRiwV5lcUkdAKCUIIwAAPlms9k04ZFGKu/rqeZVy3P7bgBAqeFh9gAAgJIlyM9b60a0U/kyXmaPAgBAoeEdIwBAgRFFAIDShjACAAAAYHmEEQAAAADLI4wAAAAAWJ7Lwujtt99WmzZt5Ovrq4CAgHwd069fP9lsthxb586dXTUiAAAAAEhy4V3pMjMz1aNHD0VFRenTTz/N93GdO3fWrFmznF/b7XZXjAcAAAAATi4LozfeeEOSNHv27AIdZ7fbFRIS4oKJAAAAACB3xe4zRhs2bFBQUJAiIyM1aNAgXbp06bbrMzIylJKSkmMDAAAAgIIoVmHUuXNnff7551q7dq3ee+89bdy4Uffff7+ys7PzPGbChAny9/d3buHh4UU4MQAAAIDSoEBhNGbMmFtujvDr7fDhw797mMcff1wPPfSQGjVqpG7dumnFihXauXOnNmzYkOcxY8eOVXJysnM7ffr07/7+AAAAAKypQJ8xGjFihPr163fbNTVr1vwj89zyXJUqVdLRo0fVoUOHXNfY7XZu0AAAAADgDylQGAUGBiowMNBVs9zizJkzunTpkkJDQ4vsewIAAACwHpd9xighIUGxsbFKSEhQdna2YmNjFRsbq9TUVOeaunXrasmSJZKk1NRUjRo1Stu2bdPJkye1du1ade3aVREREerUqZOrxgQAAAAA192ue9y4cZozZ47z62bNmkmS1q9fr3bt2kmS4uPjlZycLElyd3fXvn37NGfOHF25ckVhYWG677779NZbb3GpHAAAAACXshmGYZg9RGFKSUmRv7+/kpOT5efnZ/Y4AAAAAExSkDYoVrfrBgAAAAAzEEYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8ggjAAAAAJZHGAEAAACwPMIIAAAAgOURRgAAAAAsjzACAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8ggjAABKoclrftTUtUdyfWzq2iOavObHIp4IAIo3wggAgFLI3c2mD3OJo6lrj+jDNT/K3c1m0mQAUDx5mD0AAAAofC92qC1J+vD/vzP0Yofazigafm8d5+MAgJsIIwAASqn/jaOP1h1VZraDKAKAPHApHQAApdiLHWrLy91NmdkOebm7EUUAkAfCCACAUmzq2iPOKMrMduR5QwYAsDoupQMAoJT69WeKfvlaEu8cAcCvEEYAAJRCud1oIbcbMgAAbiKMAAAohbIdRq43Wvjl62yHYcZYAFBs2QzDKFW/GVNSUuTv76/k5GT5+fmZPQ4AAAAAkxSkDbj5AgAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8ggjAAAAAJZHGAEAAACwPMIIAAAAgOURRgAAAAAsjzACAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACW57IwOnnypAYMGKAaNWrIx8dHtWrV0vjx45WZmXnb49LT0xUdHa2KFSuqbNmy6t69u5KSklw1JgAAAAC4LowOHz4sh8Ohjz/+WAcPHtTkyZM1c+ZMvfLKK7c97qWXXtI333yjhQsXauPGjTp37pweeeQRV40JAAAAALIZhmEU1Td7//33NWPGDB0/fjzXx5OTkxUYGKh58+bp0UcflXQzsOrVq6etW7fqzjvv/M3vkZKSIn9/fyUnJ8vPz69Q5wcAAABQchSkDYr0M0bJycmqUKFCno/v3r1bWVlZ6tixo3Nf3bp1VbVqVW3dujXXYzIyMpSSkpJjAwAAAICCKLIwOnr0qKZNm6bnnnsuzzWJiYny8vJSQEBAjv3BwcFKTEzM9ZgJEybI39/fuYWHhxfm2AAAAAAsoMBhNGbMGNlstttuhw8fznHM2bNn1blzZ/Xo0UMDBw4stOElaezYsUpOTnZup0+fLtTnL6mqV6+uKVOmFOiYfv36qVu3bs6v27Vrp2HDhhXqXAAAAEBxVOAwGjFihOLi4m671axZ07n+3Llzat++vdq0aaNPPvnkts8dEhKizMxMXblyJcf+pKQkhYSE5HqM3W6Xn59fjq24SkxM1NChQxURESFvb28FBwfrrrvu0owZM3Tt2jWzx7vF4sWL9dZbb+VrbWmIqOnTp6t69ery9vZW69attWPHjt885sqVK4qOjlZoaKjsdrvq1KmjlStXOh/Pzs7Wa6+9luPujG+99ZaK8KN9AAAAyAePgh4QGBiowMDAfK09e/as2rdvrxYtWmjWrFlyc7t9h7Vo0UKenp5au3atunfvLkmKj49XQkKCoqKiCjpqsXL8+HHdddddCggI0DvvvKNGjRrJbrdr//79+uSTT1S5cmU99NBDZo+Zw+0+D1bafPXVVxo+fLhmzpyp1q1ba8qUKerUqZPi4+MVFBSU6zGZmZm69957FRQUpK+//lqVK1fWqVOnclwK+t5772nGjBmaM2eOGjRooF27dql///7y9/fXiy++WESvDgAAAL/JcJEzZ84YERERRocOHYwzZ84Y58+fd27/uyYyMtLYvn27c9/zzz9vVK1a1Vi3bp2xa9cuIyoqyoiKisr3901OTjYkGcnJyYX6ev6oTp06GVWqVDFSU1NzfdzhcDj/96lTp4yHHnrIKFOmjFGuXDmjR48eRmJiovPxo0ePGg899JARFBRklClTxmjZsqWxZs2aHM9XrVo1Y/LkyXnOc+PGDeOll14y/P39jQoVKhijRo0y+vTpY3Tt2tW5pm3btsbQoUOdX0+fPt2IiIgw7Ha7ERQUZHTv3t0wDMPo27evISnHduLECePGjRvG008/bVSvXt3w9vY26tSpY0yZMiXHHH379jW6du1qvP/++0ZISIhRoUIFY/DgwUZmZqZzTXp6uvHyyy8bVapUMby8vIxatWoZ//rXv5yP79+/3+jcubNRpkwZIygoyHjyySeNn376Kc/XnptWrVoZ0dHRzq+zs7ONsLAwY8KECXkeM2PGDKNmzZo5Zv21Bx980Hj66adz7HvkkUeM3r17F2g+AAAAFFxB2sBlN19Ys2aNjh49qrVr16pKlSoKDQ11br/IyspSfHx8jsvIJk+erL/85S/q3r277rnnHoWEhGjx4sWuGrNIXLp0Sd99952io6NVpkyZXNfYbDZJksPhUNeuXXX58mVt3LhRa9as0fHjx9WzZ0/n2tTUVD3wwANau3at9uzZo86dO6tLly5KSEjI90yTJk3S7Nmz9dlnn2nTpk26fPmylixZkuf6Xbt26cUXX9Sbb76p+Ph4rVq1Svfcc48k6e9//7uioqI0cOBAnT9/XufPn1d4eLgcDoeqVKmihQsX6tChQxo3bpxeeeUVLViwIMdzr1+/XseOHdP69es1Z84czZ49W7Nnz3Y+3qdPH3355ZeaOnWq4uLi9PHHH6ts2bKSbl7K9uc//1nNmjXTrl27tGrVKiUlJemxxx5zHj979mzn+c1NZmamdu/eneNuiG5uburYsWOed0OUpOXLlysqKkrR0dEKDg5Ww4YN9c477yg7O9u5pk2bNlq7dq1+/PFHSdLevXu1adMm3X///Xk+LwAAAExQBKFWpIrjO0bbtm0zJBmLFy/Osb9ixYpGmTJljDJlyhgvv/yyYRiG8d133xnu7u5GQkKCc93BgwcNScaOHTvy/B4NGjQwpk2b5vz6t94xCg0NNSZOnOj8Oisry6hSpUqe7xgtWrTI8PPzM1JSUnJ9vl+/u5SX6Oho5ztNhnHzHaNq1aoZN27ccO7r0aOH0bNnT8MwDCM+Pt6QdMs7Yr946623jPvuuy/HvtOnTxuSjPj4eMMwDGPx4sVGZGRknjOdPXvWkGRs2bIlx/5Ro0YZrVq1yvO4yMhIw263G08//bSxa9cuY/78+UaFChWM119/3bkmOzvbGD16tGGz2QwPDw/DZrMZ77zzTp7PCQAAgMJTLN4xwm/bsWOHYmNj1aBBA2VkZEiS4uLiFB4enuO24/Xr11dAQIDi4uIk3XzHaOTIkapXr54CAgJUtmxZxcXF5fsdo+TkZJ0/f16tW7d27vPw8FDLli3zPObee+9VtWrVVLNmTT311FOaO3duvm4YMX36dLVo0UKBgYEqW7asPvnkk1vmbNCggdzd3Z1fh4aG6sKFC5Kk2NhYubu7q23btrk+/969e7V+/XqVLVvWudWtW1eSdOzYMUnSww8/fMudEguDw+FQUFCQPvnkE7Vo0UI9e/bUq6++qpkzZzrXLFiwQHPnztW8efMUExOjOXPm6IMPPtCcOXMKfR4AAAD8fgW++QIKLiIiQjabTfHx8Tn2/3L3Ph8fnwI938iRI7VmzRp98MEHioiIkI+Pjx599FFlZmYW2sy/Vq5cOcXExGjDhg367rvvNG7cOL3++uvauXPnLX936hfz58/XyJEjNWnSJEVFRalcuXJ6//33tX379hzrPD09c3xts9nkcDgk/fa5SU1NVZcuXfTee+/d8tj/XrZ5O5UqVZK7u7uSkpJy7L/d3RB/eX5PT88cUVevXj0lJiYqMzNTXl5eGjVqlMaMGaPHH39cktSoUSOdOnVKEyZMUN++ffM1HwAAAFyPd4yKQMWKFXXvvffqo48+Ulpa2m3X1qtXT6dPn87x95gOHTqkK1euqH79+pKkzZs3q1+/fnr44YfVqFEjhYSE6OTJk/mex9/fX6GhoTkC5caNG9q9e/dtj/Pw8FDHjh01ceJE7du3TydPntS6deskSV5eXjk+W/PLnG3atNHgwYPVrFkzRUREON/Fya9GjRrJ4XBo48aNuT7evHlzHTx4UNWrV1dERESOLa/Pc/2al5eXWrRoobVr1zr3ORwOrV279rZ3Q7zrrrt09OhRZ8RJ0o8//qjQ0FB5eXlJkq5du3bL3Rjd3d1zHAMAAADzEUZF5B//+Idu3Lihli1b6quvvlJcXJzi4+P1xRdf6PDhw853HTp27KhGjRqpd+/eiomJ0Y4dO9SnTx+1bdvWealb7dq1tXjxYsXGxmrv3r164oknCvwf2kOHDtW7776rpUuX6vDhwxo8ePAtfz/qf61YsUJTp05VbGysTp06pc8//1wOh0ORkZGSbv5B2e3bt+vkyZO6ePGiHA6HateurV27dmn16tX68ccf9dprr2nnzp0FmrN69erq27evnn76aS1dulQnTpzQhg0bnDdwiI6O1uXLl9WrVy/t3LlTx44d0+rVq9W/f39nqC1ZssR5eV1ehg8frn/+85+aM2eO4uLiNGjQIKWlpal///7ONX369NHYsWOdXw8aNEiXL1/W0KFD9eOPP+rbb7/VO++8o+joaOeaLl266O2339a3336rkydPasmSJfrwww/18MMPF+g8AAAAwLW4lK6I1KpVS3v27NE777yjsWPH6syZM7Lb7apfv75GjhypwYMHS7p5GdmyZcv0wgsv6J577pGbm5s6d+6sadOmOZ/rww8/1NNPP602bdqoUqVKGj16tFJSUgo0z4gRI3T+/Hn17dtXbm5uevrpp/Xwww8rOTk51/UBAQFavHixXn/9daWnp6t27dr68ssv1aBBA0k3L+/r27ev6tevr+vXr+vEiRN67rnntGfPHvXs2VM2m029evXS4MGD9e9//7tAs86YMUOvvPKKBg8erEuXLqlq1ap65ZVXJElhYWHavHmzRo8erfvuu08ZGRmqVq2aOnfu7HynJjk5+ZbLGH+tZ8+e+umnnzRu3DglJiaqadOmWrVqlYKDg51rEhIScrz7Ex4ertWrV+ull15S48aNVblyZQ0dOlSjR492rpk2bZpee+01DR48WBcuXFBYWJiee+45jRs3rkDnAAAAAK5lMwzDMHuIwpSSkiJ/f38lJyfLz8/P7HEAAAAAmKQgbcCldAAAAAAsjzACAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABYHmEEAAAAwPIIIwAAAACWRxgBAAAAsDzCCAAAAIDlEUYAAAAALI8wAgAAAGB5hBEAAAAAyyOMAAAAAFgeYQQAAADA8jzMHqCwGYYhSUpJSTF5EgAAAABm+qUJfmmE2yl1YXT16lVJUnh4uMmTAAAAACgOrl69Kn9//9uusRn5yacSxOFw6Ny5cypXrpxsNpvZ4yglJUXh4eE6ffq0/Pz8zB6n1OH8uhbn17U4v67F+XUtzq9rcX5di/PrWsXp/BqGoatXryosLExubrf/FFGpe8fIzc1NVapUMXuMW/j5+Zn+g1GacX5di/PrWpxf1+L8uhbn17U4v67F+XWt4nJ+f+udol9w8wUAAAAAlkcYAQAAALA8wsjF7Ha7xo8fL7vdbvYopRLn17U4v67F+XUtzq9rcX5di/PrWpxf1yqp57fU3XwBAAAAAAqKd4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGhejkyZMaMGCAatSoIR8fH9WqVUvjx49XZmbmbY9LT09XdHS0KlasqLJly6p79+5KSkoqoqlLlrfffltt2rSRr6+vAgIC8nVMv379ZLPZcmydO3d27aAl1O85v4ZhaNy4cQoNDZWPj486duyoI0eOuHbQEury5cvq3bu3/Pz8FBAQoAEDBig1NfW2x7Rr1+6Wn9/nn3++iCYu/qZPn67q1avL29tbrVu31o4dO267fuHChapbt668vb3VqFEjrVy5sogmLZkKcn5nz559y8+qt7d3EU5bsvzwww/q0qWLwsLCZLPZtHTp0t88ZsOGDWrevLnsdrsiIiI0e/Zsl89ZUhX0/G7YsOGWn1+bzabExMSiGbgEmTBhgu644w6VK1dOQUFB6tatm+Lj43/zuJLw+5cwKkSHDx+Ww+HQxx9/rIMHD2ry5MmaOXOmXnnlldse99JLL+mbb77RwoULtXHjRp07d06PPPJIEU1dsmRmZqpHjx4aNGhQgY7r3Lmzzp8/79y+/PJLF01Ysv2e8ztx4kRNnTpVM2fO1Pbt21WmTBl16tRJ6enpLpy0ZOrdu7cOHjyoNWvWaMWKFfrhhx/07LPP/uZxAwcOzPHzO3HixCKYtvj76quvNHz4cI0fP14xMTFq0qSJOnXqpAsXLuS6fsuWLerVq5cGDBigPXv2qFu3burWrZsOHDhQxJOXDAU9v5Lk5+eX42f11KlTRThxyZKWlqYmTZpo+vTp+Vp/4sQJPfjgg2rfvr1iY2M1bNgwPfPMM1q9erWLJy2ZCnp+fxEfH5/jZzgoKMhFE5ZcGzduVHR0tLZt26Y1a9YoKytL9913n9LS0vI8psT8/jXgUhMnTjRq1KiR5+NXrlwxPD09jYULFzr3xcXFGZKMrVu3FsWIJdKsWbMMf3//fK3t27ev0bVrV5fOU9rk9/w6HA4jJCTEeP/99537rly5YtjtduPLL7904YQlz6FDhwxJxs6dO537/v3vfxs2m804e/Zsnse1bdvWGDp0aBFMWPK0atXKiI6Odn6dnZ1thIWFGRMmTMh1/WOPPWY8+OCDOfa1bt3aeO6551w6Z0lV0PNbkN/LyEmSsWTJktuuefnll40GDRrk2NezZ0+jU6dOLpysdMjP+V2/fr0hyfj555+LZKbS5MKFC4YkY+PGjXmuKSm/f3nHyMWSk5NVoUKFPB/fvXu3srKy1LFjR+e+unXrqmrVqtq6dWtRjGgJGzZsUFBQkCIjIzVo0CBdunTJ7JFKhRMnTigxMTHHz6+/v79at27Nz++vbN26VQEBAWrZsqVzX8eOHeXm5qbt27ff9ti5c+eqUqVKatiwocaOHatr1665etxiLzMzU7t3787xs+fm5qaOHTvm+bO3devWHOslqVOnTvys5uL3nF9JSk1NVbVq1RQeHq6uXbvq4MGDRTGuJfDzWzSaNm2q0NBQ3Xvvvdq8ebPZ45QIycnJknTb/94tKT+/HmYPUJodPXpU06ZN0wcffJDnmsTERHl5ed3yeY7g4GCuay0knTt31iOPPKIaNWro2LFjeuWVV3T//fdr69atcnd3N3u8Eu2Xn9Hg4OAc+/n5vVViYuItl2R4eHioQoUKtz1XTzzxhKpVq6awsDDt27dPo0ePVnx8vBYvXuzqkYu1ixcvKjs7O9efvcOHD+d6TGJiIj+r+fR7zm9kZKQ+++wzNW7cWMnJyfrggw/Upk0bHTx4UFWqVCmKsUu1vH5+U1JSdP36dfn4+Jg0WekQGhqqmTNnqmXLlsrIyNC//vUvtWvXTtu3b1fz5s3NHq/YcjgcGjZsmO666y41bNgwz3Ul5fcv7xjlw5gxY3L9QN7/br/+h+Ls2bPq3LmzevTooYEDB5o0ecnwe85vQTz++ON66KGH1KhRI3Xr1k0rVqzQzp07tWHDhsJ7EcWYq8+v1bn6/D777LPq1KmTGjVqpN69e+vzzz/XkiVLdOzYsUJ8FcAfFxUVpT59+qhp06Zq27atFi9erMDAQH388cdmjwb8psjISD333HNq0aKF2rRpo88++0xt2rTR5MmTzR6tWIuOjtaBAwc0f/58s0cpFLxjlA8jRoxQv379brumZs2azv997tw5tW/fXm3atNEnn3xy2+NCQkKUmZmpK1eu5HjXKCkpSSEhIX9k7BKjoOf3j6pZs6YqVaqko0ePqkOHDoX2vMWVK8/vLz+jSUlJCg0Nde5PSkpS06ZNf9dzljT5Pb8hISG3fGj9xo0bunz5coH+f71169aSbr4jXatWrQLPW1pUqlRJ7u7ut9zB83a/O0NCQgq03sp+z/n9NU9PTzVr1kxHjx51xYiWk9fPr5+fH+8WuUirVq20adMms8cotoYMGeK8kdBvvStcUn7/Ekb5EBgYqMDAwHytPXv2rNq3b68WLVpo1qxZcnO7/ZtyLVq0kKenp9auXavu3btLunlHlISEBEVFRf3h2UuCgpzfwnDmzBldunQpx3/Il2auPL81atRQSEiI1q5d6wyhlJQUbd++vcB3Diyp8nt+o6KidOXKFe3evVstWrSQJK1bt04Oh8MZO/kRGxsrSZb5+c2Ll5eXWrRoobVr16pbt26Sbl7SsXbtWg0ZMiTXY6KiorR27VoNGzbMuW/NmjWW+V1bEL/n/P5adna29u/frwceeMCFk1pHVFTULbc35ufXtWJjYy3/uzY3hmHohRde0JIlS7RhwwbVqFHjN48pMb9/zb77Q2ly5swZIyIiwujQoYNx5swZ4/z5887tf9dERkYa27dvd+57/vnnjapVqxrr1q0zdu3aZURFRRlRUVFmvIRi79SpU8aePXuMN954wyhbtqyxZ88eY8+ePcbVq1edayIjI43FixcbhmEYV69eNUaOHGls3brVOHHihPH9998bzZs3N2rXrm2kp6eb9TKKrYKeX8MwjHfffdcICAgwli1bZuzbt8/o2rWrUaNGDeP69etmvIRirXPnzkazZs2M7du3G5s2bTJq165t9OrVy/n4r38/HD161HjzzTeNXbt2GSdOnDCWLVtm1KxZ07jnnnvMegnFyvz58w273W7Mnj3bOHTokPHss88aAQEBRmJiomEYhvHUU08ZY8aMca7fvHmz4eHhYXzwwQdGXFycMX78eMPT09PYv3+/WS+hWCvo+X3jjTeM1atXG8eOHTN2795tPP7444a3t7dx8OBBs15CsXb16lXn71hJxocffmjs2bPHOHXqlGEYhjFmzBjjqaeecq4/fvy44evra4waNcqIi4szpk+fbri7uxurVq0y6yUUawU9v5MnTzaWLl1qHDlyxNi/f78xdOhQw83Nzfj+++/NegnF1qBBgwx/f39jw4YNOf5b99q1a841JfX3L2FUiGbNmmVIynX7xYkTJwxJxvr16537rl+/bgwePNgoX7684evrazz88MM5Ygr/1bdv31zP7/+eT0nGrFmzDMMwjGvXrhn33XefERgYaHh6ehrVqlUzBg4c6PyHHTkV9Pwaxs1bdr/22mtGcHCwYbfbjQ4dOhjx8fFFP3wJcOnSJaNXr15G2bJlDT8/P6N///45ovPXvx8SEhKMe+65x6hQoYJht9uNiIgIY9SoUUZycrJJr6D4mTZtmlG1alXDy8vLaNWqlbFt2zbnY23btjX69u2bY/2CBQuMOnXqGF5eXkaDBg2Mb7/9tognLlkKcn6HDRvmXBscHGw88MADRkxMjAlTlwy/3B7619sv57Rv375G27ZtbzmmadOmhpeXl1GzZs0cv4uRU0HP73vvvWfUqlXL8Pb2NipUqGC0a9fOWLdunTnDF3N5/bfu//48ltTfvzbDMAxXviMFAAAAAMUdd6UDAAAAYHmEEQAAAADLI4wAAAAAWB5hBAAAAMDyCCMAAAAAlkcYAQAAALA8wggAAACA5RFGAAAAACyPMAIAAABgeYQRAAAAAMsjjAAAAABY3v8DfqfafKzY8WAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "import numpy as np\n",
        "\n",
        "# DO NOT CHANGE arm parameters\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "# ------------------\n",
        "\n",
        "\n",
        "# Arm environment - An “Environment” is a way to standardize a problem for RL libraries\n",
        "\"\"\"\n",
        "class ArmEnv(gym.Env):\n",
        "    - def __init__(self, arm, gui=False):\n",
        "        - set obs space, goal, action space\n",
        "        - step duration = 0.01\n",
        "        - episode length = 200\n",
        "\n",
        "    - def set_goal(self, goal):\n",
        "        - call this function to set the goal for arm during testing\n",
        "\n",
        "    - def step(self, action):\n",
        "        - takes an action as a (2,1) shaped array\n",
        "        - executes an action and returns obs, reward, done, info\n",
        "            - Action space: continuous torques --> must map from discrete action indices manually\n",
        "        - tells you the next observation\n",
        "        - In this project, observation = state + goal information (+ ee position)\n",
        "            - Obs space - 8 values: 4 for the state, 2 for the pos_ee and 2 for the goal\n",
        "        - info is a dictionary with pos_ee and vel_ee values --> use for reward engineering\n",
        "        - computes the reward function: negative squared distance between end-effector and goal\n",
        "            - reward = - dist(pos_ee, goal)^2\n",
        "        - computes the next step of the robotic arm - advances the environment, checks if episode is “done”\n",
        "        - Needs a continuous action, so always convert discrete → continuous\n",
        "\n",
        "    - def reset(self, goal=None):\n",
        "        - resets the robotic arm in a vertically downwards pos\n",
        "        - starts a new episode\n",
        "        - randomizes goal if none given\n",
        "\n",
        "    - def get_obs(self):\n",
        "        - returns the current observation of the robotic arm\n",
        "        - uses the state and pos_ee to compute the observation\n",
        "\n",
        "    - def seed(self, seed=None):\n",
        "        - for repeatability stochasticity\n",
        "\"\"\"\n",
        "env = ArmEnv(arm, gui=True)\n",
        "\n",
        "# Passing our own defined goal to the reset function\n",
        "#goal = np.array([[0.5], [-1.5]])\n",
        "#obs = env.reset(goal)\n",
        "\n",
        "# Resetting the environment without the goal will set a random goal position\n",
        "obs = env.reset() # reset the arm in the vertically downwards position and set a new random goal by calling the random_goal() method\n",
        "\n",
        "# random policy interacting with the ArmEnv for 50 steps (0.5 seconds)\n",
        "for _ in range(50):\n",
        "    rand_action = np.random.uniform(-1.5, 1.5, (2,1))\n",
        "    obs, reward, done, info = env.step(rand_action) # takes an action as a (2,1) shaped array and outputs the next observation, reward, done and info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jmXTT_ngdqG"
      },
      "source": [
        "### QNetwork\n",
        "This class defines the architecture of your network. You must fill in the __init__(...) function which defines your network, and the forward(...) function which performs the forward pass.\n",
        "\n",
        "Your action space should be discrete, with whatever cardinality you decide. The size of the output layer of your Q-Network should thus be the same as the cardinality of your action space. When selecting an action, a policy must choose the one that has the highest estimated Q-value for the current state. As part of the QNetwork class, we are providing the function select_discrete_action(...) which does exactly that.\n",
        "\n",
        "The arm environment itself however expects a 2-dimensional, continuous action vector. Therefore, when it comes to send an action to the environment, you must provide the kind of action the environment expects. It is your job to determine how to convert between the discrete action space of your Q-Network and the continuous action space of the arm. You do this by filling in the action_discrete_to_continuous(...) function in your QNetwork. You can expect to call the step function of the environment like this:\n",
        "\n",
        "```\n",
        "self.env.step(self.q_network.action_discrete_to_continuous(discrete_action))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "7UyguLRKgf_I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "Use DQN (Deep Q-Network) and PPO (Proximal Policy Optimization) to solve a robot reaching task with a 2-link arm\n",
        "\n",
        "You will build and train a neural network to control the arm to reach a goal position\n",
        "    - Build your own agent with discrete actions → continuous control\n",
        "    - It needs to choose actions and convert discrete actions to continuous torques for the robot\n",
        "\n",
        "Goal: Learn a Q-function that estimates expected rewards for each action\n",
        "\n",
        "Input: the state (positions, velocities, and goal info)\n",
        "Output: a Q-value for each discrete action\n",
        "\n",
        "Loss function:  L = Q(s_t, a_t) - [r_t + 𝛾 * max_(a’) * Q(s_t+1, a’)]\n",
        "\n",
        "    - Q(s,a): Value of taking action a in state s\n",
        "    - γ (gamma): Discount factor for future rewards\n",
        "    - Loss function = what i knew before - what i just learned\n",
        "\"\"\"\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self, env):\n",
        "\n",
        "    super(QNetwork, self).__init__()\n",
        "\n",
        "    \"\"\"\n",
        "    Goal: Define network architecture (input layer, hidden layers, output layer)\n",
        "\n",
        "    Input size = length of observation (state + goal info)\n",
        "    Output size = number of discrete actions you define\n",
        "    Some hidden layers (usually 2-3 fully connected layers)\n",
        "    \"\"\"\n",
        "\n",
        "    obs_dim = env.observation_space.shape[0]  # Obs space - 8 values: 4 for the state, 2 for the pos_ee and 2 for the goal\n",
        "    num_actions = 9  # Choose the number of discrete actions\n",
        "\n",
        "    self.fc1 = nn.Linear(obs_dim, 128)   # first hidden layer\n",
        "    self.fc2 = nn.Linear(128, 128)       # second hidden layer\n",
        "    self.fc3 = nn.Linear(128, num_actions) # output layer (one q-value per action)\n",
        "\n",
        "  def forward(self, x, device):\n",
        "\n",
        "    \"\"\"Define how the input flows through the network --> forward pass\"\"\"\n",
        "\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    q_values = self.fc3(x)\n",
        "\n",
        "    return q_values\n",
        "\n",
        "  def select_discrete_action(self, obs, device):\n",
        "\n",
        "    \"\"\"\n",
        "    Run a forward pass\n",
        "    Pick the action (index) with the highest Q-value\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert obs to PyTorch tensor before reshaping\n",
        "    obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
        "\n",
        "    # Put the observation through the network to estimate q values for all possible discrete actions\n",
        "    est_q_vals = self.forward(obs.reshape((1,) + obs.shape), device)\n",
        "\n",
        "    # Choose the discrete action with the highest estimated q value\n",
        "    discrete_action = torch.argmax(est_q_vals, dim=1).tolist()[0]\n",
        "\n",
        "    return discrete_action\n",
        "\n",
        "  def action_discrete_to_continuous(self, discrete_action):\n",
        "\n",
        "    \"\"\"\n",
        "    Convert between the discrete action space of your Q-Network and the continuous action space of the arm\n",
        "\n",
        "    Conversion dictionary\n",
        "\n",
        "    Map a discrete action index to a real torque [μ_1, μ_2]\n",
        "\n",
        "    Mapping:\n",
        "        - Choose a small number of discrete actions\n",
        "        - Manually define what each discrete action means in terms of torques\n",
        "    \"\"\"\n",
        "\n",
        "    # Conversion dictionary\n",
        "    torque_candidates = {\n",
        "        0: [-0.1, -0.1],\n",
        "        1: [-0.1,  0.0],\n",
        "        2: [-0.1,  0.1],\n",
        "        3: [ 0.0, -0.1],\n",
        "        4: [ 0.0,  0.0],  # \"do nothing\" - no torque\n",
        "        5: [ 0.0,  0.1],\n",
        "        6: [ 0.1, -0.1],\n",
        "        7: [ 0.1,  0.0],\n",
        "        8: [ 0.1,  0.1]\n",
        "    }\n",
        "    torque = torque_candidates[discrete_action]\n",
        "\n",
        "    return torch.tensor(torque, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify QNetwork\n",
        "env = ArmEnv(arm)\n",
        "device = torch.device('cpu')\n",
        "\n",
        "qnet = QNetwork(env)\n",
        "obs = env.reset()\n",
        "obs = torch.tensor(obs, dtype=torch.float32)\n",
        "\n",
        "discrete_action = qnet.select_discrete_action(obs, device)\n",
        "continuous_action = qnet.action_discrete_to_continuous(discrete_action)\n",
        "\n",
        "print(\"Discrete action:\", discrete_action)\n",
        "print(\"Continuous torque:\", continuous_action)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_mOn1whxqEE",
        "outputId": "61a6d01c-9453-4f15-8f54-115084ddb230"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete action: 0\n",
            "Continuous torque: tensor([-0.1000, -0.1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide you with code to use the replay buffer in your RL implementation. You do not need to change the ReplayBuffer class.\n",
        "```\n",
        "rb = ReplayBuffer()\n",
        "```\n",
        "After creating a ReplayBuffer object you can add samples in the buffer using `put()`:\n",
        "```\n",
        "rb.put((obs, action, reward, next_obs, done))\n",
        "```\n",
        "Take random samples from the buffer using:\n",
        "```\n",
        "obs, actions, rewards, next_obses, dones = rb.sample(batch_size)\n",
        "```\n"
      ],
      "metadata": {
        "id": "IUjAeQcPdsGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q7NytRAXtYkE"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, buffer_limit):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done_mask = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append(a)\n",
        "            r_lst.append(r)\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append(done_mask)\n",
        "\n",
        "        return np.array(s_lst), np.array(a_lst), \\\n",
        "               np.array(r_lst), np.array(s_prime_lst), \\\n",
        "               np.array(done_mask_lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TrainDQN\n",
        "Here, you must fill in the train(...) function that actually trains your network.\n",
        "\n",
        "We are providing a helper function called save_model(...) that will save the current Q-network. Use this as you see fit.\n",
        "\n",
        "To set one network equal to another one, you can use code like this:\n",
        "```\n",
        "target_network.load_state_dict(self.q_network.state_dict())\n",
        "```\n",
        "\n",
        "If you would like to be graded with a specific seed for the random number generators, make sure to change the default seed in the initialization of the TrainDQN class.\n",
        "\n",
        "The time taken to train the model will depend mainly on how big is your model architecture and the number of episodes you run the training for. As a reference, the time taken to train a model on 1500 episodes, which passed all evaluation metrics was about an hour.\n",
        "* Reference value for clipping the gradient value as mentioned in class: 0.2\n",
        "* Reference value for a typical size of Replay Buffer: >10k\n",
        "* Reference value for batch size while training: 64 - 512\n",
        "\n",
        "Note that these are just reference values and larger is not always better as it may slow things down.\n",
        "\n",
        "It is good practice in RL to ensure simpler things are working before complicating environments or training techniques.\n",
        "\n",
        "If you think your training method is not working at all, you could pass a fixed goal to the `env.reset()` method during the training loop to ensure that your model is learning."
      ],
      "metadata": {
        "id": "pxVawoBLe3bd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EwS8xVR7tbeQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "import numpy as np\n",
        "from math import dist\n",
        "\n",
        "\n",
        "def linear_decay(initial, final, current_step, decay_steps):\n",
        "    return max(final, initial - (initial - final) * current_step / decay_steps)\n",
        "\n",
        "def mse_loss(pred, target):\n",
        "    return ((pred - target) ** 2).mean()\n",
        "\n",
        "\n",
        "class TrainDQN:\n",
        "\n",
        "    def __init__(self, env, seed=0):\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        self.env = env\n",
        "        self.device = torch.device('cpu')\n",
        "        self.q_network = QNetwork(env).to(self.device)\n",
        "        self.target_network = QNetwork(env).to(self.device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        # Adam optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=1e-3)\n",
        "\n",
        "    def save_model(self, episode_num, save_dir='models'):\n",
        "        timestr = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        model_dir = os.path.join(save_dir, timestr)\n",
        "        if not os.path.exists(os.path.join(model_dir)):\n",
        "          os.makedirs(os.path.join(model_dir))\n",
        "        savepath = os.path.join(model_dir, f'q_network_ep_{episode_num:04d}.pth')\n",
        "        torch.save(self.q_network.state_dict(), savepath)\n",
        "        print(f'model saved to {savepath}\\n')\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        # Initialize constants\n",
        "        num_episodes = 1500\n",
        "        batch_size = 128\n",
        "        buffer_size = 10000\n",
        "        gamma = 0.99\n",
        "        epsilon_start = 1.0\n",
        "        epsilon_end = 0.05\n",
        "        epsilon_decay = 50000\n",
        "        target_update_freq = 20\n",
        "\n",
        "        rb = ReplayBuffer(buffer_size)\n",
        "        total_steps = 0\n",
        "\n",
        "        for episode in range(1, num_episodes + 1):\n",
        "\n",
        "            obs = self.env.reset()  # start new episode and randomize goal if given none\n",
        "            obs = torch.tensor(obs, dtype=torch.float32)  # convert obs to a tensor\n",
        "            episode_reward = 0\n",
        "\n",
        "            for step in range(200):  # max 200 steps\n",
        "\n",
        "                epsilon = linear_decay(epsilon_start, epsilon_end, total_steps, epsilon_decay)\n",
        "\n",
        "                \"\"\"\n",
        "                Q-learning:\n",
        "                    - q(s,a)=(1-alpha)q(s,a) + alpha[r(s,a)+gamma*v(s')]\n",
        "                    - q(s,a)=(1-alpha)q(s,a) + alpha[r(s,a)+gamma*max_a'(q(s',a'))]\n",
        "                    - use a greedy policy to choose the best action\n",
        "                    - update the table\n",
        "                    - after a few iterations, the agent figures out that if they follow a good path, they will get good results\n",
        "                    - policy is exploring what it knows, but it has stopped exploring for an optimal path\n",
        "                    - solution: add random exploration with ε probability at each step --> ε-greedy policy\n",
        "                    - provable to converge to real q* as long as policy allows for some random \"exploration\"\n",
        "                    - can turn off exploration once we believe the algorithm has converged and start \"exploiting\" the policy\n",
        "\n",
        "                \"\"\"\n",
        "                # ε-greedy action selection - use a greedy policy to choose the best action\n",
        "                if random.random() < epsilon:\n",
        "                    discrete_action = random.randint(0, self.q_network.fc3.out_features - 1)\n",
        "                else:\n",
        "                    discrete_action = self.q_network.select_discrete_action(obs, self.device)\n",
        "\n",
        "                continuous_action = self.q_network.action_discrete_to_continuous(discrete_action)\n",
        "                next_obs, reward, done, info = self.env.step(continuous_action.numpy())\n",
        "                next_obs = torch.tensor(next_obs, dtype=torch.float32)\n",
        "\n",
        "                done_mask = 0.0 if done else 1.0  # 0 if terminal state, 1 otherwise\n",
        "\n",
        "                # Store transition\n",
        "                transition = (obs.numpy(), discrete_action, reward, next_obs.numpy(), done_mask)\n",
        "                rb.put(transition)  # After creating a ReplayBuffer object you can add samples in the buffer using put()\n",
        "\n",
        "                episode_reward += reward\n",
        "                obs = next_obs\n",
        "                total_steps += 1\n",
        "\n",
        "                # Start training when buffer has enough samples\n",
        "                if len(rb.buffer) >= batch_size:\n",
        "                    state_batch, action_batch, reward_batch, next_state_batch, done_batch = rb.sample(batch_size)\n",
        "\n",
        "                    # Convert to tensors\n",
        "                    state_batch = torch.tensor(state_batch, dtype=torch.float32)\n",
        "                    action_batch = torch.tensor(action_batch, dtype=torch.long).unsqueeze(1)\n",
        "                    reward_batch = torch.tensor(reward_batch, dtype=torch.float32).unsqueeze(1)\n",
        "                    next_state_batch = torch.tensor(next_state_batch, dtype=torch.float32)\n",
        "                    done_batch = torch.tensor(done_batch, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "                    # Q(s,a)\n",
        "                    q_values = self.q_network(state_batch, self.device).gather(1, action_batch)\n",
        "\n",
        "                    # Target Q: r + gamma * max(Q(s',a'))\n",
        "                    with torch.no_grad():\n",
        "                        next_q_values = self.target_network(next_state_batch, self.device).max(1)[0].unsqueeze(1)\n",
        "                        target_q_values = reward_batch + gamma * next_q_values * done_batch\n",
        "\n",
        "                    # Loss (MSE)\n",
        "                    loss = mse_loss(q_values, target_q_values)\n",
        "\n",
        "                    # Backpropagation\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=0.2)  # Gradient clipping\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                    # Print loss every 100 training steps\n",
        "                    #if total_steps % 100 == 0:\n",
        "                        #print(f\"Step {total_steps} | Loss: {loss.item():.6f}\")\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            # Update target network\n",
        "            if episode % target_update_freq == 0:\n",
        "                self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "                print(f\"Episode {episode} / {num_episodes} | Target network updated.\")\n",
        "\n",
        "            # Save model every 100 episodes\n",
        "            if episode % 100 == 0:\n",
        "                self.save_model(episode)\n",
        "\n",
        "            print(f\"Episode {episode} / {num_episodes} | Loss: {loss.item():.6f} | Reward: {episode_reward:.2f} | Epsilon: {epsilon:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "sEHSV1Q1BT1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaca8ae7-3718-469a-dc8f-7f6cd8bde058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/geometry.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  R[0,0] = np.cos(theta)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 200 | Loss: 0.000389\n",
            "Episode 1 | Reward: -32.73 | Epsilon: 0.996\n",
            "Step 300 | Loss: 0.000267\n",
            "Step 400 | Loss: 0.000163\n",
            "Episode 2 | Reward: -9.08 | Epsilon: 0.992\n",
            "Step 500 | Loss: 0.000244\n",
            "Step 600 | Loss: 0.000238\n",
            "Episode 3 | Reward: -43.09 | Epsilon: 0.989\n",
            "Step 700 | Loss: 0.000146\n",
            "Step 800 | Loss: 0.000135\n",
            "Episode 4 | Reward: -107.90 | Epsilon: 0.985\n",
            "Step 900 | Loss: 0.000147\n",
            "Step 1000 | Loss: 0.000082\n",
            "Episode 5 | Reward: -11.74 | Epsilon: 0.981\n",
            "Step 1100 | Loss: 0.000126\n",
            "Step 1200 | Loss: 0.000249\n",
            "Episode 6 | Reward: -9.14 | Epsilon: 0.977\n",
            "Step 1300 | Loss: 0.000098\n",
            "Step 1400 | Loss: 0.000427\n",
            "Episode 7 | Reward: -137.44 | Epsilon: 0.973\n",
            "Step 1500 | Loss: 0.000362\n",
            "Step 1600 | Loss: 0.000130\n",
            "Episode 8 | Reward: -122.44 | Epsilon: 0.970\n",
            "Step 1700 | Loss: 0.000370\n",
            "Step 1800 | Loss: 0.000246\n",
            "Episode 9 | Reward: -116.64 | Epsilon: 0.966\n",
            "Step 1900 | Loss: 0.000434\n",
            "Step 2000 | Loss: 0.001282\n",
            "Episode 10 | Reward: -226.56 | Epsilon: 0.962\n",
            "Step 2100 | Loss: 0.000321\n",
            "Step 2200 | Loss: 0.000243\n",
            "Episode 11 | Reward: -46.00 | Epsilon: 0.958\n",
            "Step 2300 | Loss: 0.000413\n",
            "Step 2400 | Loss: 0.000198\n",
            "Episode 12 | Reward: -89.55 | Epsilon: 0.954\n",
            "Step 2500 | Loss: 0.000232\n",
            "Step 2600 | Loss: 0.000228\n",
            "Episode 13 | Reward: -56.91 | Epsilon: 0.951\n",
            "Step 2700 | Loss: 0.000547\n",
            "Step 2800 | Loss: 0.000142\n",
            "Episode 14 | Reward: -118.01 | Epsilon: 0.947\n",
            "Step 2900 | Loss: 0.000260\n",
            "Step 3000 | Loss: 0.000173\n",
            "Episode 15 | Reward: -13.66 | Epsilon: 0.943\n",
            "Step 3100 | Loss: 0.000166\n",
            "Step 3200 | Loss: 0.000141\n",
            "Episode 16 | Reward: -47.62 | Epsilon: 0.939\n",
            "Step 3300 | Loss: 0.000149\n",
            "Step 3400 | Loss: 0.000170\n",
            "Episode 17 | Reward: -14.58 | Epsilon: 0.935\n",
            "Step 3500 | Loss: 0.000313\n",
            "Step 3600 | Loss: 0.000202\n",
            "Episode 18 | Reward: -36.74 | Epsilon: 0.932\n",
            "Step 3700 | Loss: 0.000159\n",
            "Step 3800 | Loss: 0.000112\n",
            "Episode 19 | Reward: -13.88 | Epsilon: 0.928\n",
            "Step 3900 | Loss: 0.000129\n",
            "Step 4000 | Loss: 0.000150\n",
            "Episode 20 | Target network updated.\n",
            "Episode 20 | Reward: -36.82 | Epsilon: 0.924\n",
            "Step 4100 | Loss: 0.001037\n",
            "Step 4200 | Loss: 0.009530\n",
            "Episode 21 | Reward: -41.86 | Epsilon: 0.920\n",
            "Step 4300 | Loss: 0.001314\n",
            "Step 4400 | Loss: 0.000346\n",
            "Episode 22 | Reward: -105.33 | Epsilon: 0.916\n",
            "Step 4500 | Loss: 0.000321\n",
            "Step 4600 | Loss: 0.000643\n",
            "Episode 23 | Reward: -91.98 | Epsilon: 0.913\n",
            "Step 4700 | Loss: 0.000414\n",
            "Step 4800 | Loss: 0.000327\n",
            "Episode 24 | Reward: -85.21 | Epsilon: 0.909\n",
            "Step 4900 | Loss: 0.000311\n",
            "Step 5000 | Loss: 0.000869\n",
            "Episode 25 | Reward: -28.27 | Epsilon: 0.905\n",
            "Step 5100 | Loss: 0.003106\n",
            "Step 5200 | Loss: 0.001230\n",
            "Episode 26 | Reward: -21.50 | Epsilon: 0.901\n",
            "Step 5300 | Loss: 0.000379\n",
            "Step 5400 | Loss: 0.000402\n",
            "Episode 27 | Reward: -87.10 | Epsilon: 0.897\n",
            "Step 5500 | Loss: 0.000376\n",
            "Step 5600 | Loss: 0.001696\n",
            "Episode 28 | Reward: -60.71 | Epsilon: 0.894\n",
            "Step 5700 | Loss: 0.000699\n",
            "Step 5800 | Loss: 0.000395\n",
            "Episode 29 | Reward: -27.84 | Epsilon: 0.890\n",
            "Step 5900 | Loss: 0.000436\n",
            "Step 6000 | Loss: 0.000443\n",
            "Episode 30 | Reward: -56.95 | Epsilon: 0.886\n",
            "Step 6100 | Loss: 0.000641\n",
            "Step 6200 | Loss: 0.000749\n",
            "Episode 31 | Reward: -81.22 | Epsilon: 0.882\n",
            "Step 6300 | Loss: 0.000308\n",
            "Step 6400 | Loss: 0.000410\n",
            "Episode 32 | Reward: -67.33 | Epsilon: 0.878\n",
            "Step 6500 | Loss: 0.000495\n",
            "Step 6600 | Loss: 0.000950\n",
            "Episode 33 | Reward: -25.57 | Epsilon: 0.875\n",
            "Step 6700 | Loss: 0.000449\n",
            "Step 6800 | Loss: 0.000605\n",
            "Episode 34 | Reward: -162.01 | Epsilon: 0.871\n",
            "Step 6900 | Loss: 0.001853\n",
            "Step 7000 | Loss: 0.000456\n",
            "Episode 35 | Reward: -77.34 | Epsilon: 0.867\n",
            "Step 7100 | Loss: 0.000354\n",
            "Step 7200 | Loss: 0.000730\n",
            "Episode 36 | Reward: -80.86 | Epsilon: 0.863\n",
            "Step 7300 | Loss: 0.004566\n",
            "Step 7400 | Loss: 0.000553\n",
            "Episode 37 | Reward: -6.29 | Epsilon: 0.859\n",
            "Step 7500 | Loss: 0.000278\n",
            "Step 7600 | Loss: 0.000711\n",
            "Episode 38 | Reward: -98.35 | Epsilon: 0.856\n",
            "Step 7700 | Loss: 0.000278\n",
            "Step 7800 | Loss: 0.000596\n",
            "Episode 39 | Reward: -67.25 | Epsilon: 0.852\n",
            "Step 7900 | Loss: 0.000208\n",
            "Step 8000 | Loss: 0.000508\n",
            "Episode 40 | Target network updated.\n",
            "Episode 40 | Reward: -82.33 | Epsilon: 0.848\n",
            "Step 8100 | Loss: 0.001141\n",
            "Step 8200 | Loss: 0.006522\n",
            "Episode 41 | Reward: -19.22 | Epsilon: 0.844\n",
            "Step 8300 | Loss: 0.000983\n",
            "Step 8400 | Loss: 0.000812\n",
            "Episode 42 | Reward: -64.00 | Epsilon: 0.840\n",
            "Step 8500 | Loss: 0.001269\n",
            "Step 8600 | Loss: 0.001578\n",
            "Episode 43 | Reward: -46.70 | Epsilon: 0.837\n",
            "Step 8700 | Loss: 0.003119\n",
            "Step 8800 | Loss: 0.000378\n",
            "Episode 44 | Reward: -122.28 | Epsilon: 0.833\n",
            "Step 8900 | Loss: 0.000848\n",
            "Step 9000 | Loss: 0.003371\n",
            "Episode 45 | Reward: -84.01 | Epsilon: 0.829\n",
            "Step 9100 | Loss: 0.000892\n",
            "Step 9200 | Loss: 0.000291\n",
            "Episode 46 | Reward: -39.43 | Epsilon: 0.825\n",
            "Step 9300 | Loss: 0.003072\n",
            "Step 9400 | Loss: 0.001360\n",
            "Episode 47 | Reward: -46.00 | Epsilon: 0.821\n",
            "Step 9500 | Loss: 0.000978\n",
            "Step 9600 | Loss: 0.000955\n",
            "Episode 48 | Reward: -52.61 | Epsilon: 0.818\n",
            "Step 9700 | Loss: 0.001224\n",
            "Step 9800 | Loss: 0.000784\n",
            "Episode 49 | Reward: -107.95 | Epsilon: 0.814\n",
            "Step 9900 | Loss: 0.000673\n",
            "Step 10000 | Loss: 0.000970\n",
            "Episode 50 | Reward: -155.63 | Epsilon: 0.810\n",
            "Step 10100 | Loss: 0.000415\n",
            "Step 10200 | Loss: 0.000822\n",
            "Episode 51 | Reward: -46.87 | Epsilon: 0.806\n",
            "Step 10300 | Loss: 0.000727\n",
            "Step 10400 | Loss: 0.000626\n",
            "Episode 52 | Reward: -159.56 | Epsilon: 0.802\n",
            "Step 10500 | Loss: 0.002813\n",
            "Step 10600 | Loss: 0.000923\n",
            "Episode 53 | Reward: -129.61 | Epsilon: 0.799\n",
            "Step 10700 | Loss: 0.012522\n",
            "Step 10800 | Loss: 0.001422\n",
            "Episode 54 | Reward: -41.36 | Epsilon: 0.795\n",
            "Step 10900 | Loss: 0.000927\n",
            "Step 11000 | Loss: 0.002076\n",
            "Episode 55 | Reward: -141.21 | Epsilon: 0.791\n",
            "Step 11100 | Loss: 0.000987\n",
            "Step 11200 | Loss: 0.002889\n",
            "Episode 56 | Reward: -261.86 | Epsilon: 0.787\n",
            "Step 11300 | Loss: 0.000639\n",
            "Step 11400 | Loss: 0.011556\n",
            "Episode 57 | Reward: -55.75 | Epsilon: 0.783\n",
            "Step 11500 | Loss: 0.001344\n",
            "Step 11600 | Loss: 0.000978\n",
            "Episode 58 | Reward: -88.02 | Epsilon: 0.780\n",
            "Step 11700 | Loss: 0.001458\n",
            "Step 11800 | Loss: 0.001565\n",
            "Episode 59 | Reward: -53.75 | Epsilon: 0.776\n",
            "Step 11900 | Loss: 0.001196\n",
            "Step 12000 | Loss: 0.004532\n",
            "Episode 60 | Target network updated.\n",
            "Episode 60 | Reward: -63.47 | Epsilon: 0.772\n",
            "Step 12100 | Loss: 0.014315\n",
            "Step 12200 | Loss: 0.002091\n",
            "Episode 61 | Reward: -83.51 | Epsilon: 0.768\n",
            "Step 12300 | Loss: 0.002336\n",
            "Step 12400 | Loss: 0.001401\n",
            "Episode 62 | Reward: -40.32 | Epsilon: 0.764\n",
            "Step 12500 | Loss: 0.002497\n",
            "Step 12600 | Loss: 0.002089\n",
            "Episode 63 | Reward: -95.84 | Epsilon: 0.761\n",
            "Step 12700 | Loss: 0.002262\n",
            "Step 12800 | Loss: 0.010515\n",
            "Episode 64 | Reward: -86.52 | Epsilon: 0.757\n",
            "Step 12900 | Loss: 0.001363\n",
            "Step 13000 | Loss: 0.001901\n",
            "Episode 65 | Reward: -66.79 | Epsilon: 0.753\n",
            "Step 13100 | Loss: 0.001080\n",
            "Step 13200 | Loss: 0.000722\n",
            "Episode 66 | Reward: -130.41 | Epsilon: 0.749\n",
            "Step 13300 | Loss: 0.001225\n",
            "Step 13400 | Loss: 0.001539\n",
            "Episode 67 | Reward: -42.90 | Epsilon: 0.745\n",
            "Step 13500 | Loss: 0.003025\n",
            "Step 13600 | Loss: 0.018112\n",
            "Episode 68 | Reward: -83.82 | Epsilon: 0.742\n",
            "Step 13700 | Loss: 0.001955\n",
            "Step 13800 | Loss: 0.001998\n",
            "Episode 69 | Reward: -50.08 | Epsilon: 0.738\n",
            "Step 13900 | Loss: 0.001628\n",
            "Step 14000 | Loss: 0.002375\n",
            "Episode 70 | Reward: -20.13 | Epsilon: 0.734\n",
            "Step 14100 | Loss: 0.001081\n",
            "Step 14200 | Loss: 0.002012\n",
            "Episode 71 | Reward: -39.84 | Epsilon: 0.730\n",
            "Step 14300 | Loss: 0.001973\n",
            "Step 14400 | Loss: 0.013446\n",
            "Episode 72 | Reward: -221.48 | Epsilon: 0.726\n",
            "Step 14500 | Loss: 0.003336\n",
            "Step 14600 | Loss: 0.005212\n",
            "Episode 73 | Reward: -165.57 | Epsilon: 0.723\n",
            "Step 14700 | Loss: 0.013313\n",
            "Step 14800 | Loss: 0.147446\n",
            "Episode 74 | Reward: -168.98 | Epsilon: 0.719\n",
            "Step 14900 | Loss: 0.002185\n",
            "Step 15000 | Loss: 0.025575\n",
            "Episode 75 | Reward: -225.26 | Epsilon: 0.715\n",
            "Step 15100 | Loss: 0.009559\n",
            "Step 15200 | Loss: 0.002206\n",
            "Episode 76 | Reward: -96.83 | Epsilon: 0.711\n",
            "Step 15300 | Loss: 0.001705\n",
            "Step 15400 | Loss: 0.003371\n",
            "Episode 77 | Reward: -103.88 | Epsilon: 0.707\n",
            "Step 15500 | Loss: 0.011083\n",
            "Step 15600 | Loss: 0.121034\n",
            "Episode 78 | Reward: -117.77 | Epsilon: 0.704\n",
            "Step 15700 | Loss: 0.001970\n",
            "Step 15800 | Loss: 0.053750\n",
            "Episode 79 | Reward: -175.53 | Epsilon: 0.700\n",
            "Step 15900 | Loss: 0.003955\n",
            "Step 16000 | Loss: 0.006509\n",
            "Episode 80 | Target network updated.\n",
            "Episode 80 | Reward: -104.49 | Epsilon: 0.696\n",
            "Step 16100 | Loss: 0.004157\n",
            "Step 16200 | Loss: 0.015716\n",
            "Episode 81 | Reward: -129.51 | Epsilon: 0.692\n",
            "Step 16300 | Loss: 0.552735\n",
            "Step 16400 | Loss: 0.004675\n",
            "Episode 82 | Reward: -118.21 | Epsilon: 0.688\n",
            "Step 16500 | Loss: 0.005314\n",
            "Step 16600 | Loss: 0.010890\n",
            "Episode 83 | Reward: -135.85 | Epsilon: 0.685\n",
            "Step 16700 | Loss: 0.005851\n",
            "Step 16800 | Loss: 0.012413\n",
            "Episode 84 | Reward: -120.76 | Epsilon: 0.681\n",
            "Step 16900 | Loss: 0.392034\n",
            "Step 17000 | Loss: 0.253877\n",
            "Episode 85 | Reward: -137.98 | Epsilon: 0.677\n",
            "Step 17100 | Loss: 0.002123\n",
            "Step 17200 | Loss: 0.005669\n",
            "Episode 86 | Reward: -86.44 | Epsilon: 0.673\n",
            "Step 17300 | Loss: 0.176756\n",
            "Step 17400 | Loss: 0.007076\n",
            "Episode 87 | Reward: -121.69 | Epsilon: 0.669\n",
            "Step 17500 | Loss: 0.003938\n",
            "Step 17600 | Loss: 0.034959\n",
            "Episode 88 | Reward: -151.45 | Epsilon: 0.666\n",
            "Step 17700 | Loss: 0.171245\n",
            "Step 17800 | Loss: 0.008634\n",
            "Episode 89 | Reward: -163.22 | Epsilon: 0.662\n",
            "Step 17900 | Loss: 0.067734\n",
            "Step 18000 | Loss: 0.014129\n",
            "Episode 90 | Reward: -85.26 | Epsilon: 0.658\n",
            "Step 18100 | Loss: 0.004015\n",
            "Step 18200 | Loss: 0.002095\n",
            "Episode 91 | Reward: -57.76 | Epsilon: 0.654\n",
            "Step 18300 | Loss: 0.005083\n",
            "Step 18400 | Loss: 0.179224\n",
            "Episode 92 | Reward: -67.65 | Epsilon: 0.650\n",
            "Step 18500 | Loss: 0.788033\n",
            "Step 18600 | Loss: 0.004679\n",
            "Episode 93 | Reward: -44.34 | Epsilon: 0.647\n",
            "Step 18700 | Loss: 0.004205\n",
            "Step 18800 | Loss: 0.075243\n",
            "Episode 94 | Reward: -42.67 | Epsilon: 0.643\n",
            "Step 18900 | Loss: 0.376837\n",
            "Step 19000 | Loss: 0.034862\n",
            "Episode 95 | Reward: -25.24 | Epsilon: 0.639\n",
            "Step 19100 | Loss: 0.004142\n",
            "Step 19200 | Loss: 0.030634\n",
            "Episode 96 | Reward: -99.49 | Epsilon: 0.635\n",
            "Step 19300 | Loss: 0.409914\n",
            "Step 19400 | Loss: 0.198807\n",
            "Episode 97 | Reward: -154.30 | Epsilon: 0.631\n",
            "Step 19500 | Loss: 0.010406\n",
            "Step 19600 | Loss: 0.004488\n",
            "Episode 98 | Reward: -5.40 | Epsilon: 0.628\n",
            "Step 19700 | Loss: 0.030818\n",
            "Step 19800 | Loss: 0.247757\n",
            "Episode 99 | Reward: -72.81 | Epsilon: 0.624\n",
            "Step 19900 | Loss: 0.273178\n",
            "Step 20000 | Loss: 0.009970\n",
            "Episode 100 | Target network updated.\n",
            "model saved to models/2025-04-29_03-47-05/q_network_ep_0100.pth\n",
            "\n",
            "Episode 100 | Reward: -118.58 | Epsilon: 0.620\n",
            "Step 20100 | Loss: 0.006395\n",
            "Step 20200 | Loss: 0.077359\n",
            "Episode 101 | Reward: -103.00 | Epsilon: 0.616\n",
            "Step 20300 | Loss: 0.007947\n",
            "Step 20400 | Loss: 0.953162\n",
            "Episode 102 | Reward: -54.16 | Epsilon: 0.612\n",
            "Step 20500 | Loss: 0.384616\n",
            "Step 20600 | Loss: 0.007368\n",
            "Episode 103 | Reward: -123.18 | Epsilon: 0.609\n",
            "Step 20700 | Loss: 0.007150\n",
            "Step 20800 | Loss: 0.005112\n",
            "Episode 104 | Reward: -29.37 | Epsilon: 0.605\n",
            "Step 20900 | Loss: 0.277197\n",
            "Step 21000 | Loss: 0.002376\n",
            "Episode 105 | Reward: -145.09 | Epsilon: 0.601\n",
            "Step 21100 | Loss: 0.006149\n",
            "Step 21200 | Loss: 0.010887\n",
            "Episode 106 | Reward: -237.94 | Epsilon: 0.597\n",
            "Step 21300 | Loss: 0.008441\n",
            "Step 21400 | Loss: 0.005541\n",
            "Episode 107 | Reward: -41.73 | Epsilon: 0.593\n",
            "Step 21500 | Loss: 0.006107\n",
            "Step 21600 | Loss: 0.009894\n",
            "Episode 108 | Reward: -65.14 | Epsilon: 0.590\n",
            "Step 21700 | Loss: 0.750662\n",
            "Step 21800 | Loss: 0.018057\n",
            "Episode 109 | Reward: -29.27 | Epsilon: 0.586\n",
            "Step 21900 | Loss: 0.005600\n",
            "Step 22000 | Loss: 0.005151\n",
            "Episode 110 | Reward: -75.09 | Epsilon: 0.582\n",
            "Step 22100 | Loss: 0.004138\n",
            "Step 22200 | Loss: 0.358460\n",
            "Episode 111 | Reward: -85.06 | Epsilon: 0.578\n",
            "Step 22300 | Loss: 0.003608\n",
            "Step 22400 | Loss: 0.004791\n",
            "Episode 112 | Reward: -174.93 | Epsilon: 0.574\n",
            "Step 22500 | Loss: 0.013479\n",
            "Step 22600 | Loss: 0.009023\n",
            "Episode 113 | Reward: -146.62 | Epsilon: 0.571\n",
            "Step 22700 | Loss: 0.007189\n",
            "Step 22800 | Loss: 0.057148\n",
            "Episode 114 | Reward: -63.58 | Epsilon: 0.567\n",
            "Step 22900 | Loss: 0.025555\n",
            "Step 23000 | Loss: 0.007051\n",
            "Episode 115 | Reward: -111.51 | Epsilon: 0.563\n",
            "Step 23100 | Loss: 0.003284\n",
            "Step 23200 | Loss: 0.005984\n",
            "Episode 116 | Reward: -67.68 | Epsilon: 0.559\n",
            "Step 23300 | Loss: 0.366590\n",
            "Step 23400 | Loss: 0.650597\n",
            "Episode 117 | Reward: -102.22 | Epsilon: 0.555\n",
            "Step 23500 | Loss: 0.008697\n",
            "Step 23600 | Loss: 0.010138\n",
            "Episode 118 | Reward: -86.37 | Epsilon: 0.552\n",
            "Step 23700 | Loss: 0.053672\n",
            "Step 23800 | Loss: 0.611088\n",
            "Episode 119 | Reward: -57.13 | Epsilon: 0.548\n",
            "Step 23900 | Loss: 0.332073\n",
            "Step 24000 | Loss: 1.224184\n",
            "Episode 120 | Target network updated.\n",
            "Episode 120 | Reward: -52.99 | Epsilon: 0.544\n",
            "Step 24100 | Loss: 0.048307\n",
            "Step 24200 | Loss: 0.017299\n",
            "Episode 121 | Reward: -184.53 | Epsilon: 0.540\n",
            "Step 24300 | Loss: 0.153133\n",
            "Step 24400 | Loss: 0.012104\n",
            "Episode 122 | Reward: -167.21 | Epsilon: 0.536\n",
            "Step 24500 | Loss: 0.079876\n",
            "Step 24600 | Loss: 0.151159\n",
            "Episode 123 | Reward: -62.90 | Epsilon: 0.533\n",
            "Step 24700 | Loss: 0.007535\n",
            "Step 24800 | Loss: 0.429749\n",
            "Episode 124 | Reward: -90.69 | Epsilon: 0.529\n",
            "Step 24900 | Loss: 0.008856\n",
            "Step 25000 | Loss: 0.005257\n",
            "Episode 125 | Reward: -54.27 | Epsilon: 0.525\n",
            "Step 25100 | Loss: 0.006324\n",
            "Step 25200 | Loss: 0.005522\n",
            "Episode 126 | Reward: -211.48 | Epsilon: 0.521\n",
            "Step 25300 | Loss: 0.204126\n",
            "Step 25400 | Loss: 0.005428\n",
            "Episode 127 | Reward: -28.36 | Epsilon: 0.517\n",
            "Step 25500 | Loss: 0.010807\n",
            "Step 25600 | Loss: 0.003339\n",
            "Episode 128 | Reward: -76.97 | Epsilon: 0.514\n",
            "Step 25700 | Loss: 0.401913\n",
            "Step 25800 | Loss: 0.004751\n",
            "Episode 129 | Reward: -24.54 | Epsilon: 0.510\n",
            "Step 25900 | Loss: 0.938112\n",
            "Step 26000 | Loss: 0.381047\n",
            "Episode 130 | Reward: -196.41 | Epsilon: 0.506\n",
            "Step 26100 | Loss: 0.008418\n",
            "Step 26200 | Loss: 0.078147\n",
            "Episode 131 | Reward: -65.11 | Epsilon: 0.502\n",
            "Step 26300 | Loss: 0.111059\n",
            "Step 26400 | Loss: 0.009428\n",
            "Episode 132 | Reward: -279.87 | Epsilon: 0.498\n",
            "Step 26500 | Loss: 0.005917\n",
            "Step 26600 | Loss: 0.160678\n",
            "Episode 133 | Reward: -101.90 | Epsilon: 0.495\n",
            "Step 26700 | Loss: 0.004965\n",
            "Step 26800 | Loss: 0.009362\n",
            "Episode 134 | Reward: -70.06 | Epsilon: 0.491\n",
            "Step 26900 | Loss: 0.007523\n",
            "Step 27000 | Loss: 0.150525\n",
            "Episode 135 | Reward: -129.89 | Epsilon: 0.487\n",
            "Step 27100 | Loss: 0.006741\n",
            "Step 27200 | Loss: 0.005412\n",
            "Episode 136 | Reward: -208.87 | Epsilon: 0.483\n",
            "Step 27300 | Loss: 0.106459\n",
            "Step 27400 | Loss: 0.007673\n",
            "Episode 137 | Reward: -200.74 | Epsilon: 0.479\n",
            "Step 27500 | Loss: 0.004708\n",
            "Step 27600 | Loss: 0.009743\n",
            "Episode 138 | Reward: -150.32 | Epsilon: 0.476\n",
            "Step 27700 | Loss: 0.005010\n",
            "Step 27800 | Loss: 0.020388\n",
            "Episode 139 | Reward: -173.49 | Epsilon: 0.472\n",
            "Step 27900 | Loss: 0.010656\n",
            "Step 28000 | Loss: 0.194342\n",
            "Episode 140 | Target network updated.\n",
            "Episode 140 | Reward: -49.75 | Epsilon: 0.468\n",
            "Step 28100 | Loss: 0.016371\n",
            "Step 28200 | Loss: 0.017515\n",
            "Episode 141 | Reward: -89.37 | Epsilon: 0.464\n",
            "Step 28300 | Loss: 0.102931\n",
            "Step 28400 | Loss: 0.011909\n",
            "Episode 142 | Reward: -96.88 | Epsilon: 0.460\n",
            "Step 28500 | Loss: 0.879650\n",
            "Step 28600 | Loss: 0.018244\n",
            "Episode 143 | Reward: -100.33 | Epsilon: 0.457\n",
            "Step 28700 | Loss: 0.008010\n",
            "Step 28800 | Loss: 0.451755\n",
            "Episode 144 | Reward: -112.56 | Epsilon: 0.453\n",
            "Step 28900 | Loss: 0.005907\n",
            "Step 29000 | Loss: 0.006777\n",
            "Episode 145 | Reward: -203.69 | Epsilon: 0.449\n",
            "Step 29100 | Loss: 0.010786\n",
            "Step 29200 | Loss: 0.356068\n",
            "Episode 146 | Reward: -195.13 | Epsilon: 0.445\n",
            "Step 29300 | Loss: 0.948144\n",
            "Step 29400 | Loss: 0.026061\n",
            "Episode 147 | Reward: -166.30 | Epsilon: 0.441\n",
            "Step 29500 | Loss: 2.368047\n",
            "Step 29600 | Loss: 0.037284\n",
            "Episode 148 | Reward: -266.22 | Epsilon: 0.438\n",
            "Step 29700 | Loss: 0.006876\n",
            "Step 29800 | Loss: 0.021765\n",
            "Episode 149 | Reward: -210.01 | Epsilon: 0.434\n",
            "Step 29900 | Loss: 0.425556\n",
            "Step 30000 | Loss: 0.007483\n",
            "Episode 150 | Reward: -85.70 | Epsilon: 0.430\n",
            "Step 30100 | Loss: 0.005841\n",
            "Step 30200 | Loss: 0.007991\n",
            "Episode 151 | Reward: -125.09 | Epsilon: 0.426\n",
            "Step 30300 | Loss: 0.529318\n",
            "Step 30400 | Loss: 0.010188\n",
            "Episode 152 | Reward: -386.37 | Epsilon: 0.422\n",
            "Step 30500 | Loss: 0.017969\n",
            "Step 30600 | Loss: 0.006926\n",
            "Episode 153 | Reward: -197.79 | Epsilon: 0.419\n",
            "Step 30700 | Loss: 0.030207\n",
            "Step 30800 | Loss: 0.018619\n",
            "Episode 154 | Reward: -595.19 | Epsilon: 0.415\n",
            "Step 30900 | Loss: 0.137272\n",
            "Step 31000 | Loss: 0.008785\n",
            "Episode 155 | Reward: -721.52 | Epsilon: 0.411\n",
            "Step 31100 | Loss: 0.010410\n",
            "Step 31200 | Loss: 0.221814\n",
            "Episode 156 | Reward: -317.21 | Epsilon: 0.407\n",
            "Step 31300 | Loss: 2.737790\n",
            "Step 31400 | Loss: 0.010443\n",
            "Episode 157 | Reward: -196.03 | Epsilon: 0.403\n",
            "Step 31500 | Loss: 0.157204\n",
            "Step 31600 | Loss: 0.035227\n",
            "Episode 158 | Reward: -149.21 | Epsilon: 0.400\n",
            "Step 31700 | Loss: 0.032650\n",
            "Step 31800 | Loss: 0.118301\n",
            "Episode 159 | Reward: -40.87 | Epsilon: 0.396\n",
            "Step 31900 | Loss: 0.015027\n",
            "Step 32000 | Loss: 0.013130\n",
            "Episode 160 | Target network updated.\n",
            "Episode 160 | Reward: -126.07 | Epsilon: 0.392\n",
            "Step 32100 | Loss: 5.510691\n",
            "Step 32200 | Loss: 0.051667\n",
            "Episode 161 | Reward: -89.60 | Epsilon: 0.388\n",
            "Step 32300 | Loss: 0.084332\n",
            "Step 32400 | Loss: 0.610592\n",
            "Episode 162 | Reward: -125.74 | Epsilon: 0.384\n",
            "Step 32500 | Loss: 0.034664\n",
            "Step 32600 | Loss: 1.009411\n",
            "Episode 163 | Reward: -54.46 | Epsilon: 0.381\n",
            "Step 32700 | Loss: 0.978951\n",
            "Step 32800 | Loss: 0.007973\n",
            "Episode 164 | Reward: -80.80 | Epsilon: 0.377\n",
            "Step 32900 | Loss: 0.018931\n",
            "Step 33000 | Loss: 0.013801\n",
            "Episode 165 | Reward: -230.22 | Epsilon: 0.373\n",
            "Step 33100 | Loss: 0.553221\n",
            "Step 33200 | Loss: 0.008408\n",
            "Episode 166 | Reward: -149.48 | Epsilon: 0.369\n",
            "Step 33300 | Loss: 0.014285\n",
            "Step 33400 | Loss: 0.017014\n",
            "Episode 167 | Reward: -82.07 | Epsilon: 0.365\n",
            "Step 33500 | Loss: 0.020144\n",
            "Step 33600 | Loss: 0.014821\n",
            "Episode 168 | Reward: -137.04 | Epsilon: 0.362\n",
            "Step 33700 | Loss: 0.110963\n",
            "Step 33800 | Loss: 0.950890\n",
            "Episode 169 | Reward: -221.18 | Epsilon: 0.358\n",
            "Step 33900 | Loss: 0.666422\n",
            "Step 34000 | Loss: 0.027153\n",
            "Episode 170 | Reward: -294.32 | Epsilon: 0.354\n",
            "Step 34100 | Loss: 0.891886\n",
            "Step 34200 | Loss: 1.491074\n",
            "Episode 171 | Reward: -267.86 | Epsilon: 0.350\n",
            "Step 34300 | Loss: 0.817284\n",
            "Step 34400 | Loss: 0.010732\n",
            "Episode 172 | Reward: -234.84 | Epsilon: 0.346\n",
            "Step 34500 | Loss: 1.498566\n",
            "Step 34600 | Loss: 0.477060\n",
            "Episode 173 | Reward: -415.07 | Epsilon: 0.343\n",
            "Step 34700 | Loss: 0.013033\n",
            "Step 34800 | Loss: 2.038670\n",
            "Episode 174 | Reward: -130.32 | Epsilon: 0.339\n",
            "Step 34900 | Loss: 1.180398\n",
            "Step 35000 | Loss: 0.022568\n",
            "Episode 175 | Reward: -107.58 | Epsilon: 0.335\n",
            "Step 35100 | Loss: 0.020729\n",
            "Step 35200 | Loss: 0.014043\n",
            "Episode 176 | Reward: -316.41 | Epsilon: 0.331\n",
            "Step 35300 | Loss: 0.139381\n",
            "Step 35400 | Loss: 0.022643\n",
            "Episode 177 | Reward: -67.27 | Epsilon: 0.327\n",
            "Step 35500 | Loss: 0.023432\n",
            "Step 35600 | Loss: 0.033529\n",
            "Episode 178 | Reward: -105.63 | Epsilon: 0.324\n",
            "Step 35700 | Loss: 0.091816\n",
            "Step 35800 | Loss: 0.016934\n",
            "Episode 179 | Reward: -279.22 | Epsilon: 0.320\n",
            "Step 35900 | Loss: 0.014317\n",
            "Step 36000 | Loss: 0.088950\n",
            "Episode 180 | Target network updated.\n",
            "Episode 180 | Reward: -96.75 | Epsilon: 0.316\n",
            "Step 36100 | Loss: 0.099695\n",
            "Step 36200 | Loss: 0.157224\n",
            "Episode 181 | Reward: -66.77 | Epsilon: 0.312\n",
            "Step 36300 | Loss: 0.029172\n",
            "Step 36400 | Loss: 1.232167\n",
            "Episode 182 | Reward: -24.53 | Epsilon: 0.308\n",
            "Step 36500 | Loss: 0.015763\n",
            "Step 36600 | Loss: 0.033764\n",
            "Episode 183 | Reward: -37.71 | Epsilon: 0.305\n",
            "Step 36700 | Loss: 1.404744\n",
            "Step 36800 | Loss: 0.012750\n",
            "Episode 184 | Reward: -11.01 | Epsilon: 0.301\n",
            "Step 36900 | Loss: 1.106406\n",
            "Step 37000 | Loss: 0.012419\n",
            "Episode 185 | Reward: -88.81 | Epsilon: 0.297\n",
            "Step 37100 | Loss: 0.031138\n",
            "Step 37200 | Loss: 0.010819\n",
            "Episode 186 | Reward: -143.39 | Epsilon: 0.293\n",
            "Step 37300 | Loss: 0.929921\n",
            "Step 37400 | Loss: 0.186964\n",
            "Episode 187 | Reward: -252.88 | Epsilon: 0.289\n",
            "Step 37500 | Loss: 1.443448\n",
            "Step 37600 | Loss: 2.203526\n",
            "Episode 188 | Reward: -272.66 | Epsilon: 0.286\n",
            "Step 37700 | Loss: 0.642606\n",
            "Step 37800 | Loss: 0.024866\n",
            "Episode 189 | Reward: -55.67 | Epsilon: 0.282\n",
            "Step 37900 | Loss: 0.008761\n",
            "Step 38000 | Loss: 0.391737\n",
            "Episode 190 | Reward: -69.68 | Epsilon: 0.278\n",
            "Step 38100 | Loss: 0.017634\n",
            "Step 38200 | Loss: 0.017971\n",
            "Episode 191 | Reward: -465.42 | Epsilon: 0.274\n",
            "Step 38300 | Loss: 4.560824\n",
            "Step 38400 | Loss: 0.755693\n",
            "Episode 192 | Reward: -15.89 | Epsilon: 0.270\n",
            "Step 38500 | Loss: 0.019268\n",
            "Step 38600 | Loss: 0.233240\n",
            "Episode 193 | Reward: -104.97 | Epsilon: 0.267\n",
            "Step 38700 | Loss: 0.575630\n",
            "Step 38800 | Loss: 0.275523\n",
            "Episode 194 | Reward: -242.38 | Epsilon: 0.263\n",
            "Step 38900 | Loss: 0.019778\n",
            "Step 39000 | Loss: 0.280014\n",
            "Episode 195 | Reward: -13.23 | Epsilon: 0.259\n",
            "Step 39100 | Loss: 0.011350\n",
            "Step 39200 | Loss: 0.017002\n",
            "Episode 196 | Reward: -311.95 | Epsilon: 0.255\n",
            "Step 39300 | Loss: 4.061617\n",
            "Step 39400 | Loss: 0.023380\n",
            "Episode 197 | Reward: -59.16 | Epsilon: 0.251\n",
            "Step 39500 | Loss: 0.024134\n",
            "Step 39600 | Loss: 0.025002\n",
            "Episode 198 | Reward: -72.88 | Epsilon: 0.248\n",
            "Step 39700 | Loss: 0.014571\n",
            "Step 39800 | Loss: 0.079352\n",
            "Episode 199 | Reward: -238.73 | Epsilon: 0.244\n",
            "Step 39900 | Loss: 0.010930\n",
            "Step 40000 | Loss: 0.197953\n",
            "Episode 200 | Target network updated.\n",
            "model saved to models/2025-04-29_03-49-05/q_network_ep_0200.pth\n",
            "\n",
            "Episode 200 | Reward: -327.15 | Epsilon: 0.240\n",
            "Step 40100 | Loss: 0.034475\n",
            "Step 40200 | Loss: 0.027298\n",
            "Episode 201 | Reward: -64.43 | Epsilon: 0.236\n",
            "Step 40300 | Loss: 0.023809\n",
            "Step 40400 | Loss: 0.129186\n",
            "Episode 202 | Reward: -54.11 | Epsilon: 0.232\n",
            "Step 40500 | Loss: 5.116115\n",
            "Step 40600 | Loss: 0.289658\n",
            "Episode 203 | Reward: -44.12 | Epsilon: 0.229\n",
            "Step 40700 | Loss: 0.010753\n",
            "Step 40800 | Loss: 2.610153\n",
            "Episode 204 | Reward: -106.41 | Epsilon: 0.225\n",
            "Step 40900 | Loss: 0.021215\n",
            "Step 41000 | Loss: 0.011711\n",
            "Episode 205 | Reward: -248.49 | Epsilon: 0.221\n",
            "Step 41100 | Loss: 0.860417\n",
            "Step 41200 | Loss: 0.007935\n",
            "Episode 206 | Reward: -88.16 | Epsilon: 0.217\n",
            "Step 41300 | Loss: 0.028461\n",
            "Step 41400 | Loss: 0.017556\n",
            "Episode 207 | Reward: -209.71 | Epsilon: 0.213\n",
            "Step 41500 | Loss: 0.042418\n",
            "Step 41600 | Loss: 0.014630\n",
            "Episode 208 | Reward: -166.02 | Epsilon: 0.210\n",
            "Step 41700 | Loss: 0.008723\n",
            "Step 41800 | Loss: 1.390325\n",
            "Episode 209 | Reward: -288.93 | Epsilon: 0.206\n",
            "Step 41900 | Loss: 0.017873\n",
            "Step 42000 | Loss: 0.503113\n",
            "Episode 210 | Reward: -90.75 | Epsilon: 0.202\n",
            "Step 42100 | Loss: 0.011115\n",
            "Step 42200 | Loss: 0.006654\n",
            "Episode 211 | Reward: -73.00 | Epsilon: 0.198\n",
            "Step 42300 | Loss: 0.012668\n",
            "Step 42400 | Loss: 0.038165\n",
            "Episode 212 | Reward: -318.19 | Epsilon: 0.194\n",
            "Step 42500 | Loss: 0.024205\n",
            "Step 42600 | Loss: 0.014310\n",
            "Episode 213 | Reward: -47.26 | Epsilon: 0.191\n",
            "Step 42700 | Loss: 0.012818\n",
            "Step 42800 | Loss: 0.012146\n",
            "Episode 214 | Reward: -225.56 | Epsilon: 0.187\n",
            "Step 42900 | Loss: 1.201692\n",
            "Step 43000 | Loss: 0.036177\n",
            "Episode 215 | Reward: -169.07 | Epsilon: 0.183\n",
            "Step 43100 | Loss: 0.013119\n",
            "Step 43200 | Loss: 0.012164\n",
            "Episode 216 | Reward: -15.81 | Epsilon: 0.179\n",
            "Step 43300 | Loss: 1.987496\n",
            "Step 43400 | Loss: 0.012357\n",
            "Episode 217 | Reward: -13.25 | Epsilon: 0.175\n",
            "Step 43500 | Loss: 0.014653\n",
            "Step 43600 | Loss: 0.008909\n",
            "Episode 218 | Reward: -108.36 | Epsilon: 0.172\n",
            "Step 43700 | Loss: 0.041065\n",
            "Step 43800 | Loss: 1.004540\n",
            "Episode 219 | Reward: -25.79 | Epsilon: 0.168\n",
            "Step 43900 | Loss: 0.004896\n",
            "Step 44000 | Loss: 1.405688\n",
            "Episode 220 | Target network updated.\n",
            "Episode 220 | Reward: -162.06 | Epsilon: 0.164\n",
            "Step 44100 | Loss: 0.011463\n",
            "Step 44200 | Loss: 0.021221\n",
            "Episode 221 | Reward: -41.76 | Epsilon: 0.160\n",
            "Step 44300 | Loss: 0.024246\n",
            "Step 44400 | Loss: 0.017095\n",
            "Episode 222 | Reward: -165.43 | Epsilon: 0.156\n",
            "Step 44500 | Loss: 0.023506\n",
            "Step 44600 | Loss: 0.029722\n",
            "Episode 223 | Reward: -127.36 | Epsilon: 0.153\n",
            "Step 44700 | Loss: 0.010376\n",
            "Step 44800 | Loss: 0.877074\n",
            "Episode 224 | Reward: -233.84 | Epsilon: 0.149\n",
            "Step 44900 | Loss: 1.731468\n",
            "Step 45000 | Loss: 0.014824\n",
            "Episode 225 | Reward: -286.27 | Epsilon: 0.145\n",
            "Step 45100 | Loss: 0.380394\n",
            "Step 45200 | Loss: 0.057664\n",
            "Episode 226 | Reward: -243.28 | Epsilon: 0.141\n",
            "Step 45300 | Loss: 0.010607\n",
            "Step 45400 | Loss: 0.016776\n",
            "Episode 227 | Reward: -145.57 | Epsilon: 0.137\n",
            "Step 45500 | Loss: 0.016732\n",
            "Step 45600 | Loss: 0.626695\n",
            "Episode 228 | Reward: -40.99 | Epsilon: 0.134\n",
            "Step 45700 | Loss: 0.014639\n",
            "Step 45800 | Loss: 0.681634\n",
            "Episode 229 | Reward: -128.09 | Epsilon: 0.130\n",
            "Step 45900 | Loss: 0.032713\n",
            "Step 46000 | Loss: 0.010438\n",
            "Episode 230 | Reward: -56.23 | Epsilon: 0.126\n",
            "Step 46100 | Loss: 0.025297\n",
            "Step 46200 | Loss: 0.008016\n",
            "Episode 231 | Reward: -171.01 | Epsilon: 0.122\n",
            "Step 46300 | Loss: 2.188368\n",
            "Step 46400 | Loss: 0.009092\n",
            "Episode 232 | Reward: -33.59 | Epsilon: 0.118\n",
            "Step 46500 | Loss: 0.006287\n",
            "Step 46600 | Loss: 0.025986\n",
            "Episode 233 | Reward: -94.94 | Epsilon: 0.115\n",
            "Step 46700 | Loss: 0.005429\n",
            "Step 46800 | Loss: 0.013310\n",
            "Episode 234 | Reward: -143.59 | Epsilon: 0.111\n",
            "Step 46900 | Loss: 0.011348\n",
            "Step 47000 | Loss: 0.021392\n",
            "Episode 235 | Reward: -260.38 | Epsilon: 0.107\n",
            "Step 47100 | Loss: 0.012611\n",
            "Step 47200 | Loss: 0.043205\n",
            "Episode 236 | Reward: -117.23 | Epsilon: 0.103\n",
            "Step 47300 | Loss: 0.040501\n",
            "Step 47400 | Loss: 0.011822\n",
            "Episode 237 | Reward: -194.42 | Epsilon: 0.099\n",
            "Step 47500 | Loss: 0.550839\n",
            "Step 47600 | Loss: 0.062833\n",
            "Episode 238 | Reward: -156.58 | Epsilon: 0.096\n",
            "Step 47700 | Loss: 0.267256\n",
            "Step 47800 | Loss: 0.018903\n",
            "Episode 239 | Reward: -61.29 | Epsilon: 0.092\n",
            "Step 47900 | Loss: 0.013319\n",
            "Step 48000 | Loss: 0.011547\n",
            "Episode 240 | Target network updated.\n",
            "Episode 240 | Reward: -40.79 | Epsilon: 0.088\n",
            "Step 48100 | Loss: 0.008708\n",
            "Step 48200 | Loss: 0.013622\n",
            "Episode 241 | Reward: -40.51 | Epsilon: 0.084\n",
            "Step 48300 | Loss: 0.021623\n",
            "Step 48400 | Loss: 0.041301\n",
            "Episode 242 | Reward: -53.27 | Epsilon: 0.080\n",
            "Step 48500 | Loss: 0.023769\n",
            "Step 48600 | Loss: 0.028379\n",
            "Episode 243 | Reward: -165.72 | Epsilon: 0.077\n",
            "Step 48700 | Loss: 0.012356\n",
            "Step 48800 | Loss: 0.012767\n",
            "Episode 244 | Reward: -37.44 | Epsilon: 0.073\n",
            "Step 48900 | Loss: 1.654963\n",
            "Step 49000 | Loss: 0.028802\n",
            "Episode 245 | Reward: -104.50 | Epsilon: 0.069\n",
            "Step 49100 | Loss: 2.721114\n",
            "Step 49200 | Loss: 0.008966\n",
            "Episode 246 | Reward: -43.06 | Epsilon: 0.065\n",
            "Step 49300 | Loss: 0.031655\n",
            "Step 49400 | Loss: 0.214038\n",
            "Episode 247 | Reward: -13.11 | Epsilon: 0.061\n",
            "Step 49500 | Loss: 0.042922\n",
            "Step 49600 | Loss: 2.567575\n",
            "Episode 248 | Reward: -36.37 | Epsilon: 0.058\n",
            "Step 49700 | Loss: 4.428621\n",
            "Step 49800 | Loss: 0.010186\n",
            "Episode 249 | Reward: -48.57 | Epsilon: 0.054\n",
            "Step 49900 | Loss: 0.012098\n",
            "Step 50000 | Loss: 0.017357\n",
            "Episode 250 | Reward: -21.37 | Epsilon: 0.050\n",
            "Step 50100 | Loss: 0.038789\n",
            "Step 50200 | Loss: 0.019553\n",
            "Episode 251 | Reward: -45.87 | Epsilon: 0.050\n",
            "Step 50300 | Loss: 0.017731\n",
            "Step 50400 | Loss: 0.388117\n",
            "Episode 252 | Reward: -43.54 | Epsilon: 0.050\n",
            "Step 50500 | Loss: 0.020392\n",
            "Step 50600 | Loss: 0.023636\n",
            "Episode 253 | Reward: -41.52 | Epsilon: 0.050\n",
            "Step 50700 | Loss: 0.014492\n",
            "Step 50800 | Loss: 0.012349\n",
            "Episode 254 | Reward: -84.56 | Epsilon: 0.050\n",
            "Step 50900 | Loss: 0.495740\n",
            "Step 51000 | Loss: 0.018693\n",
            "Episode 255 | Reward: -182.44 | Epsilon: 0.050\n",
            "Step 51100 | Loss: 0.007724\n",
            "Step 51200 | Loss: 0.012164\n",
            "Episode 256 | Reward: -154.07 | Epsilon: 0.050\n",
            "Step 51300 | Loss: 0.008100\n",
            "Step 51400 | Loss: 0.013076\n",
            "Episode 257 | Reward: -55.65 | Epsilon: 0.050\n",
            "Step 51500 | Loss: 0.033917\n",
            "Step 51600 | Loss: 0.026445\n",
            "Episode 258 | Reward: -28.28 | Epsilon: 0.050\n",
            "Step 51700 | Loss: 0.025798\n",
            "Step 51800 | Loss: 0.011694\n",
            "Episode 259 | Reward: -190.11 | Epsilon: 0.050\n",
            "Step 51900 | Loss: 0.301061\n",
            "Step 52000 | Loss: 0.778582\n",
            "Episode 260 | Target network updated.\n",
            "Episode 260 | Reward: -116.69 | Epsilon: 0.050\n",
            "Step 52100 | Loss: 1.222976\n",
            "Step 52200 | Loss: 0.012194\n",
            "Episode 261 | Reward: -51.84 | Epsilon: 0.050\n",
            "Step 52300 | Loss: 0.009654\n",
            "Step 52400 | Loss: 0.012047\n",
            "Episode 262 | Reward: -26.84 | Epsilon: 0.050\n",
            "Step 52500 | Loss: 0.462441\n",
            "Step 52600 | Loss: 0.016145\n",
            "Episode 263 | Reward: -316.18 | Epsilon: 0.050\n",
            "Step 52700 | Loss: 0.028035\n",
            "Step 52800 | Loss: 0.414372\n",
            "Episode 264 | Reward: -171.08 | Epsilon: 0.050\n",
            "Step 52900 | Loss: 0.019791\n",
            "Step 53000 | Loss: 0.520304\n",
            "Episode 265 | Reward: -156.25 | Epsilon: 0.050\n",
            "Step 53100 | Loss: 0.194583\n",
            "Step 53200 | Loss: 0.030773\n",
            "Episode 266 | Reward: -166.32 | Epsilon: 0.050\n",
            "Step 53300 | Loss: 0.010869\n",
            "Step 53400 | Loss: 0.019535\n",
            "Episode 267 | Reward: -56.30 | Epsilon: 0.050\n",
            "Step 53500 | Loss: 0.008022\n",
            "Step 53600 | Loss: 0.032476\n",
            "Episode 268 | Reward: -325.99 | Epsilon: 0.050\n",
            "Step 53700 | Loss: 0.764369\n",
            "Step 53800 | Loss: 0.037513\n",
            "Episode 269 | Reward: -519.21 | Epsilon: 0.050\n",
            "Step 53900 | Loss: 0.028976\n",
            "Step 54000 | Loss: 4.892756\n",
            "Episode 270 | Reward: -68.95 | Epsilon: 0.050\n",
            "Step 54100 | Loss: 0.674387\n",
            "Step 54200 | Loss: 0.041876\n",
            "Episode 271 | Reward: -42.35 | Epsilon: 0.050\n",
            "Step 54300 | Loss: 0.021598\n",
            "Step 54400 | Loss: 0.018671\n",
            "Episode 272 | Reward: -98.13 | Epsilon: 0.050\n",
            "Step 54500 | Loss: 1.234878\n",
            "Step 54600 | Loss: 0.971893\n",
            "Episode 273 | Reward: -260.95 | Epsilon: 0.050\n",
            "Step 54700 | Loss: 0.012503\n",
            "Step 54800 | Loss: 0.018549\n",
            "Episode 274 | Reward: -612.82 | Epsilon: 0.050\n",
            "Step 54900 | Loss: 0.032945\n",
            "Step 55000 | Loss: 0.063112\n",
            "Episode 275 | Reward: -276.19 | Epsilon: 0.050\n",
            "Step 55100 | Loss: 0.049755\n",
            "Step 55200 | Loss: 0.073185\n",
            "Episode 276 | Reward: -470.66 | Epsilon: 0.050\n",
            "Step 55300 | Loss: 0.023501\n",
            "Step 55400 | Loss: 0.068001\n",
            "Episode 277 | Reward: -28.91 | Epsilon: 0.050\n",
            "Step 55500 | Loss: 0.022927\n",
            "Step 55600 | Loss: 0.633441\n",
            "Episode 278 | Reward: -314.63 | Epsilon: 0.050\n",
            "Step 55700 | Loss: 0.062720\n",
            "Step 55800 | Loss: 0.034405\n",
            "Episode 279 | Reward: -29.76 | Epsilon: 0.050\n",
            "Step 55900 | Loss: 5.430039\n",
            "Step 56000 | Loss: 0.199433\n",
            "Episode 280 | Target network updated.\n",
            "Episode 280 | Reward: -42.93 | Epsilon: 0.050\n",
            "Step 56100 | Loss: 5.758002\n",
            "Step 56200 | Loss: 0.018463\n",
            "Episode 281 | Reward: -313.08 | Epsilon: 0.050\n",
            "Step 56300 | Loss: 0.040892\n",
            "Step 56400 | Loss: 1.269687\n",
            "Episode 282 | Reward: -29.74 | Epsilon: 0.050\n",
            "Step 56500 | Loss: 1.553889\n",
            "Step 56600 | Loss: 0.027269\n",
            "Episode 283 | Reward: -42.00 | Epsilon: 0.050\n",
            "Step 56700 | Loss: 0.046728\n",
            "Step 56800 | Loss: 0.017786\n",
            "Episode 284 | Reward: -370.11 | Epsilon: 0.050\n",
            "Step 56900 | Loss: 0.205286\n",
            "Step 57000 | Loss: 0.034446\n",
            "Episode 285 | Reward: -583.14 | Epsilon: 0.050\n",
            "Step 57100 | Loss: 0.032218\n",
            "Step 57200 | Loss: 0.017018\n",
            "Episode 286 | Reward: -178.11 | Epsilon: 0.050\n",
            "Step 57300 | Loss: 0.596459\n",
            "Step 57400 | Loss: 4.720258\n",
            "Episode 287 | Reward: -234.66 | Epsilon: 0.050\n",
            "Step 57500 | Loss: 4.996825\n",
            "Step 57600 | Loss: 5.832392\n",
            "Episode 288 | Reward: -51.52 | Epsilon: 0.050\n",
            "Step 57700 | Loss: 17.046658\n",
            "Step 57800 | Loss: 9.179304\n",
            "Episode 289 | Reward: -963.77 | Epsilon: 0.050\n",
            "Step 57900 | Loss: 0.842271\n",
            "Step 58000 | Loss: 0.022519\n",
            "Episode 290 | Reward: -801.03 | Epsilon: 0.050\n",
            "Step 58100 | Loss: 0.139621\n",
            "Step 58200 | Loss: 0.120500\n",
            "Episode 291 | Reward: -30.31 | Epsilon: 0.050\n",
            "Step 58300 | Loss: 0.024824\n",
            "Step 58400 | Loss: 0.025828\n",
            "Episode 292 | Reward: -122.50 | Epsilon: 0.050\n",
            "Step 58500 | Loss: 0.054037\n",
            "Step 58600 | Loss: 0.035790\n",
            "Episode 293 | Reward: -140.80 | Epsilon: 0.050\n",
            "Step 58700 | Loss: 0.440538\n",
            "Step 58800 | Loss: 0.014573\n",
            "Episode 294 | Reward: -308.97 | Epsilon: 0.050\n",
            "Step 58900 | Loss: 0.029413\n",
            "Step 59000 | Loss: 0.076049\n",
            "Episode 295 | Reward: -878.98 | Epsilon: 0.050\n",
            "Step 59100 | Loss: 0.037004\n",
            "Step 59200 | Loss: 1.318441\n",
            "Episode 296 | Reward: -24.28 | Epsilon: 0.050\n",
            "Step 59300 | Loss: 11.283915\n",
            "Step 59400 | Loss: 15.720196\n",
            "Episode 297 | Reward: -128.44 | Epsilon: 0.050\n",
            "Step 59500 | Loss: 4.759640\n",
            "Step 59600 | Loss: 2.233073\n",
            "Episode 298 | Reward: -708.98 | Epsilon: 0.050\n",
            "Step 59700 | Loss: 0.070944\n",
            "Step 59800 | Loss: 0.064837\n",
            "Episode 299 | Reward: -266.53 | Epsilon: 0.050\n",
            "Step 59900 | Loss: 2.053439\n",
            "Step 60000 | Loss: 0.043883\n",
            "Episode 300 | Target network updated.\n",
            "model saved to models/2025-04-29_03-51-07/q_network_ep_0300.pth\n",
            "\n",
            "Episode 300 | Reward: -53.95 | Epsilon: 0.050\n",
            "Step 60100 | Loss: 0.053201\n",
            "Step 60200 | Loss: 0.021728\n",
            "Episode 301 | Reward: -20.95 | Epsilon: 0.050\n",
            "Step 60300 | Loss: 6.606399\n",
            "Step 60400 | Loss: 5.606805\n",
            "Episode 302 | Reward: -62.68 | Epsilon: 0.050\n",
            "Step 60500 | Loss: 27.459492\n",
            "Step 60600 | Loss: 0.060550\n",
            "Episode 303 | Reward: -7.39 | Epsilon: 0.050\n",
            "Step 60700 | Loss: 0.033823\n",
            "Step 60800 | Loss: 0.010833\n",
            "Episode 304 | Reward: -53.58 | Epsilon: 0.050\n",
            "Step 60900 | Loss: 0.042363\n",
            "Step 61000 | Loss: 0.035782\n",
            "Episode 305 | Reward: -94.66 | Epsilon: 0.050\n",
            "Step 61100 | Loss: 0.026765\n",
            "Step 61200 | Loss: 13.555222\n",
            "Episode 306 | Reward: -104.87 | Epsilon: 0.050\n",
            "Step 61300 | Loss: 0.022901\n",
            "Step 61400 | Loss: 0.063004\n",
            "Episode 307 | Reward: -104.69 | Epsilon: 0.050\n",
            "Step 61500 | Loss: 5.645101\n",
            "Step 61600 | Loss: 0.042406\n",
            "Episode 308 | Reward: -110.08 | Epsilon: 0.050\n",
            "Step 61700 | Loss: 0.052373\n",
            "Step 61800 | Loss: 0.053069\n",
            "Episode 309 | Reward: -15.50 | Epsilon: 0.050\n",
            "Step 61900 | Loss: 26.189373\n",
            "Step 62000 | Loss: 6.357962\n",
            "Episode 310 | Reward: -66.51 | Epsilon: 0.050\n",
            "Step 62100 | Loss: 0.037491\n",
            "Step 62200 | Loss: 1.152735\n",
            "Episode 311 | Reward: -21.62 | Epsilon: 0.050\n",
            "Step 62300 | Loss: 26.087482\n",
            "Step 62400 | Loss: 0.025807\n",
            "Episode 312 | Reward: -28.97 | Epsilon: 0.050\n",
            "Step 62500 | Loss: 0.020636\n",
            "Step 62600 | Loss: 0.199250\n",
            "Episode 313 | Reward: -58.23 | Epsilon: 0.050\n",
            "Step 62700 | Loss: 0.061759\n",
            "Step 62800 | Loss: 0.173327\n",
            "Episode 314 | Reward: -75.10 | Epsilon: 0.050\n",
            "Step 62900 | Loss: 0.064700\n",
            "Step 63000 | Loss: 6.258641\n",
            "Episode 315 | Reward: -112.59 | Epsilon: 0.050\n",
            "Step 63100 | Loss: 0.041388\n",
            "Step 63200 | Loss: 0.148405\n",
            "Episode 316 | Reward: -41.95 | Epsilon: 0.050\n",
            "Step 63300 | Loss: 3.783827\n",
            "Step 63400 | Loss: 6.628838\n",
            "Episode 317 | Reward: -33.14 | Epsilon: 0.050\n",
            "Step 63500 | Loss: 0.010785\n",
            "Step 63600 | Loss: 0.014826\n",
            "Episode 318 | Reward: -47.55 | Epsilon: 0.050\n",
            "Step 63700 | Loss: 0.006497\n",
            "Step 63800 | Loss: 0.013424\n",
            "Episode 319 | Reward: -167.11 | Epsilon: 0.050\n",
            "Step 63900 | Loss: 7.991292\n",
            "Step 64000 | Loss: 0.030865\n",
            "Episode 320 | Target network updated.\n",
            "Episode 320 | Reward: -16.85 | Epsilon: 0.050\n",
            "Step 64100 | Loss: 0.041354\n",
            "Step 64200 | Loss: 0.077609\n",
            "Episode 321 | Reward: -35.36 | Epsilon: 0.050\n",
            "Step 64300 | Loss: 0.045163\n",
            "Step 64400 | Loss: 9.209608\n",
            "Episode 322 | Reward: -15.88 | Epsilon: 0.050\n",
            "Step 64500 | Loss: 0.045663\n",
            "Step 64600 | Loss: 0.011378\n",
            "Episode 323 | Reward: -59.18 | Epsilon: 0.050\n",
            "Step 64700 | Loss: 18.148903\n",
            "Step 64800 | Loss: 0.010934\n",
            "Episode 324 | Reward: -115.63 | Epsilon: 0.050\n",
            "Step 64900 | Loss: 0.092971\n",
            "Step 65000 | Loss: 0.027654\n",
            "Episode 325 | Reward: -69.72 | Epsilon: 0.050\n",
            "Step 65100 | Loss: 0.023863\n",
            "Step 65200 | Loss: 0.021478\n",
            "Episode 326 | Reward: -9.62 | Epsilon: 0.050\n",
            "Step 65300 | Loss: 0.025493\n",
            "Step 65400 | Loss: 0.030095\n",
            "Episode 327 | Reward: -89.74 | Epsilon: 0.050\n",
            "Step 65500 | Loss: 0.023169\n",
            "Step 65600 | Loss: 0.011706\n",
            "Episode 328 | Reward: -64.74 | Epsilon: 0.050\n",
            "Step 65700 | Loss: 0.022756\n",
            "Step 65800 | Loss: 0.012038\n",
            "Episode 329 | Reward: -105.10 | Epsilon: 0.050\n",
            "Step 65900 | Loss: 0.012517\n",
            "Step 66000 | Loss: 0.058328\n",
            "Episode 330 | Reward: -56.41 | Epsilon: 0.050\n",
            "Step 66100 | Loss: 0.008971\n",
            "Step 66200 | Loss: 0.014055\n",
            "Episode 331 | Reward: -31.63 | Epsilon: 0.050\n",
            "Step 66300 | Loss: 0.012206\n",
            "Step 66400 | Loss: 0.111659\n",
            "Episode 332 | Reward: -83.71 | Epsilon: 0.050\n",
            "Step 66500 | Loss: 0.022320\n",
            "Step 66600 | Loss: 0.007538\n",
            "Episode 333 | Reward: -9.18 | Epsilon: 0.050\n",
            "Step 66700 | Loss: 0.015401\n",
            "Step 66800 | Loss: 0.020710\n",
            "Episode 334 | Reward: -15.31 | Epsilon: 0.050\n",
            "Step 66900 | Loss: 0.010504\n",
            "Step 67000 | Loss: 0.012598\n",
            "Episode 335 | Reward: -51.72 | Epsilon: 0.050\n",
            "Step 67100 | Loss: 0.028896\n",
            "Step 67200 | Loss: 35.962193\n",
            "Episode 336 | Reward: -6.96 | Epsilon: 0.050\n",
            "Step 67300 | Loss: 31.898918\n",
            "Step 67400 | Loss: 0.055342\n",
            "Episode 337 | Reward: -149.02 | Epsilon: 0.050\n",
            "Step 67500 | Loss: 0.006079\n",
            "Step 67600 | Loss: 0.037315\n",
            "Episode 338 | Reward: -16.59 | Epsilon: 0.050\n",
            "Step 67700 | Loss: 36.982910\n",
            "Step 67800 | Loss: 31.250874\n",
            "Episode 339 | Reward: -112.15 | Epsilon: 0.050\n",
            "Step 67900 | Loss: 0.024187\n",
            "Step 68000 | Loss: 0.024862\n",
            "Episode 340 | Target network updated.\n",
            "Episode 340 | Reward: -16.99 | Epsilon: 0.050\n",
            "Step 68100 | Loss: 0.025152\n",
            "Step 68200 | Loss: 0.017152\n",
            "Episode 341 | Reward: -13.28 | Epsilon: 0.050\n",
            "Step 68300 | Loss: 0.029495\n",
            "Step 68400 | Loss: 0.010801\n",
            "Episode 342 | Reward: -25.75 | Epsilon: 0.050\n",
            "Step 68500 | Loss: 0.014121\n",
            "Step 68600 | Loss: 0.009399\n",
            "Episode 343 | Reward: -36.46 | Epsilon: 0.050\n",
            "Step 68700 | Loss: 31.767920\n",
            "Step 68800 | Loss: 0.006228\n",
            "Episode 344 | Reward: -41.33 | Epsilon: 0.050\n",
            "Step 68900 | Loss: 0.005957\n",
            "Step 69000 | Loss: 0.012771\n",
            "Episode 345 | Reward: -445.23 | Epsilon: 0.050\n",
            "Step 69100 | Loss: 0.308381\n",
            "Step 69200 | Loss: 0.007530\n",
            "Episode 346 | Reward: -370.18 | Epsilon: 0.050\n",
            "Step 69300 | Loss: 0.015788\n",
            "Step 69400 | Loss: 0.013903\n",
            "Episode 347 | Reward: -9.54 | Epsilon: 0.050\n",
            "Step 69500 | Loss: 0.015225\n",
            "Step 69600 | Loss: 0.013015\n",
            "Episode 348 | Reward: -8.89 | Epsilon: 0.050\n",
            "Step 69700 | Loss: 0.011397\n",
            "Step 69800 | Loss: 0.021860\n",
            "Episode 349 | Reward: -721.18 | Epsilon: 0.050\n",
            "Step 69900 | Loss: 0.007624\n",
            "Step 70000 | Loss: 0.009645\n",
            "Episode 350 | Reward: -31.90 | Epsilon: 0.050\n",
            "Step 70100 | Loss: 0.018879\n",
            "Step 70200 | Loss: 0.063851\n",
            "Episode 351 | Reward: -12.70 | Epsilon: 0.050\n",
            "Step 70300 | Loss: 0.010166\n",
            "Step 70400 | Loss: 10.496574\n",
            "Episode 352 | Reward: -174.08 | Epsilon: 0.050\n",
            "Step 70500 | Loss: 0.009560\n",
            "Step 70600 | Loss: 0.008910\n",
            "Episode 353 | Reward: -121.29 | Epsilon: 0.050\n",
            "Step 70700 | Loss: 0.012403\n",
            "Step 70800 | Loss: 0.006996\n",
            "Episode 354 | Reward: -113.07 | Epsilon: 0.050\n",
            "Step 70900 | Loss: 0.006033\n",
            "Step 71000 | Loss: 0.008921\n",
            "Episode 355 | Reward: -67.63 | Epsilon: 0.050\n",
            "Step 71100 | Loss: 31.866034\n",
            "Step 71200 | Loss: 0.023280\n",
            "Episode 356 | Reward: -14.81 | Epsilon: 0.050\n",
            "Step 71300 | Loss: 0.005891\n",
            "Step 71400 | Loss: 0.006216\n",
            "Episode 357 | Reward: -9.98 | Epsilon: 0.050\n",
            "Step 71500 | Loss: 0.004641\n",
            "Step 71600 | Loss: 0.003322\n",
            "Episode 358 | Reward: -79.09 | Epsilon: 0.050\n",
            "Step 71700 | Loss: 0.268728\n",
            "Step 71800 | Loss: 0.013462\n",
            "Episode 359 | Reward: -23.64 | Epsilon: 0.050\n",
            "Step 71900 | Loss: 0.013770\n",
            "Step 72000 | Loss: 0.012885\n",
            "Episode 360 | Target network updated.\n",
            "Episode 360 | Reward: -729.16 | Epsilon: 0.050\n",
            "Step 72100 | Loss: 39.389900\n",
            "Step 72200 | Loss: 0.007569\n",
            "Episode 361 | Reward: -33.33 | Epsilon: 0.050\n",
            "Step 72300 | Loss: 0.018620\n",
            "Step 72400 | Loss: 0.162107\n",
            "Episode 362 | Reward: -33.98 | Epsilon: 0.050\n",
            "Step 72500 | Loss: 0.280245\n",
            "Step 72600 | Loss: 0.015944\n",
            "Episode 363 | Reward: -27.92 | Epsilon: 0.050\n",
            "Step 72700 | Loss: 0.006963\n",
            "Step 72800 | Loss: 0.005738\n",
            "Episode 364 | Reward: -44.18 | Epsilon: 0.050\n",
            "Step 72900 | Loss: 0.007735\n",
            "Step 73000 | Loss: 0.246168\n",
            "Episode 365 | Reward: -87.76 | Epsilon: 0.050\n",
            "Step 73100 | Loss: 0.010578\n",
            "Step 73200 | Loss: 0.017650\n",
            "Episode 366 | Reward: -14.01 | Epsilon: 0.050\n",
            "Step 73300 | Loss: 0.006244\n",
            "Step 73400 | Loss: 0.038067\n",
            "Episode 367 | Reward: -15.82 | Epsilon: 0.050\n",
            "Step 73500 | Loss: 0.007000\n",
            "Step 73600 | Loss: 0.007934\n",
            "Episode 368 | Reward: -10.95 | Epsilon: 0.050\n",
            "Step 73700 | Loss: 0.006281\n",
            "Step 73800 | Loss: 0.007784\n",
            "Episode 369 | Reward: -42.84 | Epsilon: 0.050\n",
            "Step 73900 | Loss: 0.003088\n",
            "Step 74000 | Loss: 0.015855\n",
            "Episode 370 | Reward: -38.31 | Epsilon: 0.050\n",
            "Step 74100 | Loss: 0.013241\n",
            "Step 74200 | Loss: 0.018763\n",
            "Episode 371 | Reward: -68.43 | Epsilon: 0.050\n",
            "Step 74300 | Loss: 1.840222\n",
            "Step 74400 | Loss: 0.012314\n",
            "Episode 372 | Reward: -32.35 | Epsilon: 0.050\n",
            "Step 74500 | Loss: 0.015897\n",
            "Step 74600 | Loss: 0.006853\n",
            "Episode 373 | Reward: -74.90 | Epsilon: 0.050\n",
            "Step 74700 | Loss: 0.008895\n",
            "Step 74800 | Loss: 0.007760\n",
            "Episode 374 | Reward: -19.38 | Epsilon: 0.050\n",
            "Step 74900 | Loss: 0.020771\n",
            "Step 75000 | Loss: 0.019502\n",
            "Episode 375 | Reward: -263.92 | Epsilon: 0.050\n",
            "Step 75100 | Loss: 0.089758\n",
            "Step 75200 | Loss: 0.011474\n",
            "Episode 376 | Reward: -16.70 | Epsilon: 0.050\n",
            "Step 75300 | Loss: 0.015203\n",
            "Step 75400 | Loss: 0.016770\n",
            "Episode 377 | Reward: -310.46 | Epsilon: 0.050\n",
            "Step 75500 | Loss: 0.041308\n",
            "Step 75600 | Loss: 0.019140\n",
            "Episode 378 | Reward: -51.40 | Epsilon: 0.050\n",
            "Step 75700 | Loss: 1.180950\n",
            "Step 75800 | Loss: 0.006123\n",
            "Episode 379 | Reward: -81.48 | Epsilon: 0.050\n",
            "Step 75900 | Loss: 0.042916\n",
            "Step 76000 | Loss: 0.010452\n",
            "Episode 380 | Target network updated.\n",
            "Episode 380 | Reward: -32.15 | Epsilon: 0.050\n",
            "Step 76100 | Loss: 0.006631\n",
            "Step 76200 | Loss: 0.012140\n",
            "Episode 381 | Reward: -165.93 | Epsilon: 0.050\n",
            "Step 76300 | Loss: 0.009542\n",
            "Step 76400 | Loss: 0.019212\n",
            "Episode 382 | Reward: -28.19 | Epsilon: 0.050\n",
            "Step 76500 | Loss: 0.038704\n",
            "Step 76600 | Loss: 0.018387\n",
            "Episode 383 | Reward: -38.42 | Epsilon: 0.050\n",
            "Step 76700 | Loss: 0.018185\n",
            "Step 76800 | Loss: 0.008923\n",
            "Episode 384 | Reward: -25.94 | Epsilon: 0.050\n",
            "Step 76900 | Loss: 0.004128\n",
            "Step 77000 | Loss: 0.014366\n",
            "Episode 385 | Reward: -8.92 | Epsilon: 0.050\n",
            "Step 77100 | Loss: 0.006034\n",
            "Step 77200 | Loss: 0.017240\n",
            "Episode 386 | Reward: -15.49 | Epsilon: 0.050\n",
            "Step 77300 | Loss: 0.020931\n",
            "Step 77400 | Loss: 0.008602\n",
            "Episode 387 | Reward: -97.21 | Epsilon: 0.050\n",
            "Step 77500 | Loss: 0.043133\n",
            "Step 77600 | Loss: 0.561113\n",
            "Episode 388 | Reward: -25.60 | Epsilon: 0.050\n",
            "Step 77700 | Loss: 0.004709\n",
            "Step 77800 | Loss: 0.045834\n",
            "Episode 389 | Reward: -23.54 | Epsilon: 0.050\n",
            "Step 77900 | Loss: 0.011376\n",
            "Step 78000 | Loss: 12.792023\n",
            "Episode 390 | Reward: -16.93 | Epsilon: 0.050\n",
            "Step 78100 | Loss: 0.019659\n",
            "Step 78200 | Loss: 0.009003\n",
            "Episode 391 | Reward: -46.42 | Epsilon: 0.050\n",
            "Step 78300 | Loss: 0.013132\n",
            "Step 78400 | Loss: 0.013153\n",
            "Episode 392 | Reward: -20.15 | Epsilon: 0.050\n",
            "Step 78500 | Loss: 0.023311\n",
            "Step 78600 | Loss: 0.007965\n",
            "Episode 393 | Reward: -42.20 | Epsilon: 0.050\n",
            "Step 78700 | Loss: 0.032717\n",
            "Step 78800 | Loss: 0.020980\n",
            "Episode 394 | Reward: -69.08 | Epsilon: 0.050\n",
            "Step 78900 | Loss: 0.020435\n",
            "Step 79000 | Loss: 0.029901\n",
            "Episode 395 | Reward: -40.97 | Epsilon: 0.050\n",
            "Step 79100 | Loss: 0.049022\n",
            "Step 79200 | Loss: 0.004068\n",
            "Episode 396 | Reward: -14.87 | Epsilon: 0.050\n",
            "Step 79300 | Loss: 0.008960\n",
            "Step 79400 | Loss: 0.009435\n",
            "Episode 397 | Reward: -44.34 | Epsilon: 0.050\n",
            "Step 79500 | Loss: 0.021919\n",
            "Step 79600 | Loss: 0.011956\n",
            "Episode 398 | Reward: -118.90 | Epsilon: 0.050\n",
            "Step 79700 | Loss: 0.014902\n",
            "Step 79800 | Loss: 0.006576\n",
            "Episode 399 | Reward: -37.53 | Epsilon: 0.050\n",
            "Step 79900 | Loss: 0.038288\n",
            "Step 80000 | Loss: 0.014515\n",
            "Episode 400 | Target network updated.\n",
            "model saved to models/2025-04-29_03-53-21/q_network_ep_0400.pth\n",
            "\n",
            "Episode 400 | Reward: -89.00 | Epsilon: 0.050\n",
            "Step 80100 | Loss: 0.013078\n",
            "Step 80200 | Loss: 0.009856\n",
            "Episode 401 | Reward: -24.11 | Epsilon: 0.050\n",
            "Step 80300 | Loss: 0.108788\n",
            "Step 80400 | Loss: 0.010959\n",
            "Episode 402 | Reward: -76.97 | Epsilon: 0.050\n",
            "Step 80500 | Loss: 0.009558\n",
            "Step 80600 | Loss: 0.010154\n",
            "Episode 403 | Reward: -11.70 | Epsilon: 0.050\n",
            "Step 80700 | Loss: 0.054959\n",
            "Step 80800 | Loss: 13.117207\n",
            "Episode 404 | Reward: -36.71 | Epsilon: 0.050\n",
            "Step 80900 | Loss: 0.077419\n",
            "Step 81000 | Loss: 0.010084\n",
            "Episode 405 | Reward: -6.50 | Epsilon: 0.050\n",
            "Step 81100 | Loss: 0.023028\n",
            "Step 81200 | Loss: 0.035682\n",
            "Episode 406 | Reward: -22.25 | Epsilon: 0.050\n",
            "Step 81300 | Loss: 0.145568\n",
            "Step 81400 | Loss: 0.010743\n",
            "Episode 407 | Reward: -39.56 | Epsilon: 0.050\n",
            "Step 81500 | Loss: 15.353807\n",
            "Step 81600 | Loss: 0.010321\n",
            "Episode 408 | Reward: -36.09 | Epsilon: 0.050\n",
            "Step 81700 | Loss: 0.255663\n",
            "Step 81800 | Loss: 0.007114\n",
            "Episode 409 | Reward: -75.63 | Epsilon: 0.050\n",
            "Step 81900 | Loss: 0.026587\n",
            "Step 82000 | Loss: 0.007576\n",
            "Episode 410 | Reward: -5.79 | Epsilon: 0.050\n",
            "Step 82100 | Loss: 0.005942\n",
            "Step 82200 | Loss: 0.028541\n",
            "Episode 411 | Reward: -40.10 | Epsilon: 0.050\n",
            "Step 82300 | Loss: 0.005652\n",
            "Step 82400 | Loss: 0.026924\n",
            "Episode 412 | Reward: -42.62 | Epsilon: 0.050\n",
            "Step 82500 | Loss: 0.016500\n",
            "Step 82600 | Loss: 0.091475\n",
            "Episode 413 | Reward: -32.44 | Epsilon: 0.050\n",
            "Step 82700 | Loss: 0.007477\n",
            "Step 82800 | Loss: 0.006372\n",
            "Episode 414 | Reward: -50.62 | Epsilon: 0.050\n",
            "Step 82900 | Loss: 0.019642\n",
            "Step 83000 | Loss: 0.008529\n",
            "Episode 415 | Reward: -23.55 | Epsilon: 0.050\n",
            "Step 83100 | Loss: 0.011869\n",
            "Step 83200 | Loss: 0.014367\n",
            "Episode 416 | Reward: -27.61 | Epsilon: 0.050\n",
            "Step 83300 | Loss: 0.007387\n",
            "Step 83400 | Loss: 3.259416\n",
            "Episode 417 | Reward: -8.75 | Epsilon: 0.050\n",
            "Step 83500 | Loss: 0.010381\n",
            "Step 83600 | Loss: 0.088616\n",
            "Episode 418 | Reward: -5.34 | Epsilon: 0.050\n",
            "Step 83700 | Loss: 0.323870\n",
            "Step 83800 | Loss: 0.008621\n",
            "Episode 419 | Reward: -99.84 | Epsilon: 0.050\n",
            "Step 83900 | Loss: 0.010469\n",
            "Step 84000 | Loss: 0.012217\n",
            "Episode 420 | Target network updated.\n",
            "Episode 420 | Reward: -36.14 | Epsilon: 0.050\n",
            "Step 84100 | Loss: 0.008535\n",
            "Step 84200 | Loss: 0.007734\n",
            "Episode 421 | Reward: -21.63 | Epsilon: 0.050\n",
            "Step 84300 | Loss: 0.007592\n",
            "Step 84400 | Loss: 0.010900\n",
            "Episode 422 | Reward: -50.76 | Epsilon: 0.050\n",
            "Step 84500 | Loss: 0.011314\n",
            "Step 84600 | Loss: 0.012238\n",
            "Episode 423 | Reward: -25.37 | Epsilon: 0.050\n",
            "Step 84700 | Loss: 0.007762\n",
            "Step 84800 | Loss: 0.063354\n",
            "Episode 424 | Reward: -69.69 | Epsilon: 0.050\n",
            "Step 84900 | Loss: 0.032365\n",
            "Step 85000 | Loss: 0.039114\n",
            "Episode 425 | Reward: -17.86 | Epsilon: 0.050\n",
            "Step 85100 | Loss: 0.026907\n",
            "Step 85200 | Loss: 2.626477\n",
            "Episode 426 | Reward: -9.34 | Epsilon: 0.050\n",
            "Step 85300 | Loss: 0.012709\n",
            "Step 85400 | Loss: 0.005889\n",
            "Episode 427 | Reward: -21.50 | Epsilon: 0.050\n",
            "Step 85500 | Loss: 0.329370\n",
            "Step 85600 | Loss: 0.010137\n",
            "Episode 428 | Reward: -15.69 | Epsilon: 0.050\n",
            "Step 85700 | Loss: 0.018799\n",
            "Step 85800 | Loss: 0.004277\n",
            "Episode 429 | Reward: -7.92 | Epsilon: 0.050\n",
            "Step 85900 | Loss: 0.021745\n",
            "Step 86000 | Loss: 0.005291\n",
            "Episode 430 | Reward: -6.82 | Epsilon: 0.050\n",
            "Step 86100 | Loss: 0.007346\n",
            "Step 86200 | Loss: 0.005189\n",
            "Episode 431 | Reward: -42.41 | Epsilon: 0.050\n",
            "Step 86300 | Loss: 0.008309\n",
            "Step 86400 | Loss: 0.007391\n",
            "Episode 432 | Reward: -8.49 | Epsilon: 0.050\n",
            "Step 86500 | Loss: 0.009064\n",
            "Step 86600 | Loss: 0.008518\n",
            "Episode 433 | Reward: -46.19 | Epsilon: 0.050\n",
            "Step 86700 | Loss: 0.009392\n",
            "Step 86800 | Loss: 0.004357\n",
            "Episode 434 | Reward: -10.07 | Epsilon: 0.050\n",
            "Step 86900 | Loss: 0.005138\n",
            "Step 87000 | Loss: 0.257717\n",
            "Episode 435 | Reward: -14.17 | Epsilon: 0.050\n",
            "Step 87100 | Loss: 0.291770\n",
            "Step 87200 | Loss: 0.093942\n",
            "Episode 436 | Reward: -47.57 | Epsilon: 0.050\n",
            "Step 87300 | Loss: 0.005013\n",
            "Step 87400 | Loss: 0.005422\n",
            "Episode 437 | Reward: -11.40 | Epsilon: 0.050\n",
            "Step 87500 | Loss: 0.011218\n",
            "Step 87600 | Loss: 0.581437\n",
            "Episode 438 | Reward: -5.53 | Epsilon: 0.050\n",
            "Step 87700 | Loss: 0.256879\n",
            "Step 87800 | Loss: 0.005961\n",
            "Episode 439 | Reward: -10.98 | Epsilon: 0.050\n",
            "Step 87900 | Loss: 0.030242\n",
            "Step 88000 | Loss: 0.006763\n",
            "Episode 440 | Target network updated.\n",
            "Episode 440 | Reward: -12.02 | Epsilon: 0.050\n",
            "Step 88100 | Loss: 0.006269\n",
            "Step 88200 | Loss: 0.761926\n",
            "Episode 441 | Reward: -14.72 | Epsilon: 0.050\n",
            "Step 88300 | Loss: 0.004412\n",
            "Step 88400 | Loss: 0.009852\n",
            "Episode 442 | Reward: -9.84 | Epsilon: 0.050\n",
            "Step 88500 | Loss: 0.017669\n",
            "Step 88600 | Loss: 3.938766\n",
            "Episode 443 | Reward: -12.23 | Epsilon: 0.050\n",
            "Step 88700 | Loss: 0.007038\n",
            "Step 88800 | Loss: 0.007251\n",
            "Episode 444 | Reward: -8.49 | Epsilon: 0.050\n",
            "Step 88900 | Loss: 0.011763\n",
            "Step 89000 | Loss: 0.288379\n",
            "Episode 445 | Reward: -35.02 | Epsilon: 0.050\n",
            "Step 89100 | Loss: 0.034536\n",
            "Step 89200 | Loss: 0.003764\n",
            "Episode 446 | Reward: -20.92 | Epsilon: 0.050\n",
            "Step 89300 | Loss: 0.021685\n",
            "Step 89400 | Loss: 0.779844\n",
            "Episode 447 | Reward: -8.14 | Epsilon: 0.050\n",
            "Step 89500 | Loss: 0.016582\n",
            "Step 89600 | Loss: 0.006426\n",
            "Episode 448 | Reward: -12.44 | Epsilon: 0.050\n",
            "Step 89700 | Loss: 0.007651\n",
            "Step 89800 | Loss: 0.007099\n",
            "Episode 449 | Reward: -7.37 | Epsilon: 0.050\n",
            "Step 89900 | Loss: 0.005605\n",
            "Step 90000 | Loss: 0.005029\n",
            "Episode 450 | Reward: -16.19 | Epsilon: 0.050\n",
            "Step 90100 | Loss: 0.002314\n",
            "Step 90200 | Loss: 0.006756\n",
            "Episode 451 | Reward: -61.12 | Epsilon: 0.050\n",
            "Step 90300 | Loss: 0.104846\n",
            "Step 90400 | Loss: 0.007981\n",
            "Episode 452 | Reward: -28.15 | Epsilon: 0.050\n",
            "Step 90500 | Loss: 0.010026\n",
            "Step 90600 | Loss: 0.004614\n",
            "Episode 453 | Reward: -25.75 | Epsilon: 0.050\n",
            "Step 90700 | Loss: 0.018713\n",
            "Step 90800 | Loss: 0.005471\n",
            "Episode 454 | Reward: -21.34 | Epsilon: 0.050\n",
            "Step 90900 | Loss: 0.006717\n",
            "Step 91000 | Loss: 0.007233\n",
            "Episode 455 | Reward: -27.34 | Epsilon: 0.050\n",
            "Step 91100 | Loss: 0.499653\n",
            "Step 91200 | Loss: 0.007485\n",
            "Episode 456 | Reward: -138.09 | Epsilon: 0.050\n",
            "Step 91300 | Loss: 0.002933\n",
            "Step 91400 | Loss: 0.007897\n",
            "Episode 457 | Reward: -67.71 | Epsilon: 0.050\n",
            "Step 91500 | Loss: 0.011908\n",
            "Step 91600 | Loss: 0.005683\n",
            "Episode 458 | Reward: -2.83 | Epsilon: 0.050\n",
            "Step 91700 | Loss: 0.004613\n",
            "Step 91800 | Loss: 0.005163\n",
            "Episode 459 | Reward: -4.82 | Epsilon: 0.050\n",
            "Step 91900 | Loss: 0.005920\n",
            "Step 92000 | Loss: 2.318881\n",
            "Episode 460 | Target network updated.\n",
            "Episode 460 | Reward: -7.62 | Epsilon: 0.050\n",
            "Step 92100 | Loss: 0.015345\n",
            "Step 92200 | Loss: 0.002687\n",
            "Episode 461 | Reward: -4.28 | Epsilon: 0.050\n",
            "Step 92300 | Loss: 0.003300\n",
            "Step 92400 | Loss: 0.003594\n",
            "Episode 462 | Reward: -74.01 | Epsilon: 0.050\n",
            "Step 92500 | Loss: 1.247890\n",
            "Step 92600 | Loss: 0.006427\n",
            "Episode 463 | Reward: -17.17 | Epsilon: 0.050\n",
            "Step 92700 | Loss: 0.009383\n",
            "Step 92800 | Loss: 0.004308\n",
            "Episode 464 | Reward: -212.38 | Epsilon: 0.050\n",
            "Step 92900 | Loss: 0.005664\n",
            "Step 93000 | Loss: 0.004749\n",
            "Episode 465 | Reward: -14.22 | Epsilon: 0.050\n",
            "Step 93100 | Loss: 0.012844\n",
            "Step 93200 | Loss: 0.003350\n",
            "Episode 466 | Reward: -45.32 | Epsilon: 0.050\n",
            "Step 93300 | Loss: 0.003476\n",
            "Step 93400 | Loss: 0.003528\n",
            "Episode 467 | Reward: -26.62 | Epsilon: 0.050\n",
            "Step 93500 | Loss: 0.003488\n",
            "Step 93600 | Loss: 0.003957\n",
            "Episode 468 | Reward: -19.02 | Epsilon: 0.050\n",
            "Step 93700 | Loss: 0.004839\n",
            "Step 93800 | Loss: 0.017015\n",
            "Episode 469 | Reward: -19.44 | Epsilon: 0.050\n",
            "Step 93900 | Loss: 0.003541\n",
            "Step 94000 | Loss: 0.003187\n",
            "Episode 470 | Reward: -25.77 | Epsilon: 0.050\n",
            "Step 94100 | Loss: 0.008920\n",
            "Step 94200 | Loss: 0.008808\n",
            "Episode 471 | Reward: -13.06 | Epsilon: 0.050\n",
            "Step 94300 | Loss: 1.083056\n",
            "Step 94400 | Loss: 0.003679\n",
            "Episode 472 | Reward: -39.79 | Epsilon: 0.050\n",
            "Step 94500 | Loss: 0.007726\n",
            "Step 94600 | Loss: 1.211994\n",
            "Episode 473 | Reward: -9.32 | Epsilon: 0.050\n",
            "Step 94700 | Loss: 0.005671\n",
            "Step 94800 | Loss: 0.012359\n",
            "Episode 474 | Reward: -283.48 | Epsilon: 0.050\n",
            "Step 94900 | Loss: 0.017134\n",
            "Step 95000 | Loss: 0.002659\n",
            "Episode 475 | Reward: -11.17 | Epsilon: 0.050\n",
            "Step 95100 | Loss: 0.043909\n",
            "Step 95200 | Loss: 0.016739\n",
            "Episode 476 | Reward: -205.05 | Epsilon: 0.050\n",
            "Step 95300 | Loss: 0.008664\n",
            "Step 95400 | Loss: 1.259045\n",
            "Episode 477 | Reward: -10.48 | Epsilon: 0.050\n",
            "Step 95500 | Loss: 0.003104\n",
            "Step 95600 | Loss: 0.005204\n",
            "Episode 478 | Reward: -12.38 | Epsilon: 0.050\n",
            "Step 95700 | Loss: 0.004559\n",
            "Step 95800 | Loss: 0.148944\n",
            "Episode 479 | Reward: -6.40 | Epsilon: 0.050\n",
            "Step 95900 | Loss: 0.014102\n",
            "Step 96000 | Loss: 0.005382\n",
            "Episode 480 | Target network updated.\n",
            "Episode 480 | Reward: -66.50 | Epsilon: 0.050\n",
            "Step 96100 | Loss: 0.006574\n",
            "Step 96200 | Loss: 0.159270\n",
            "Episode 481 | Reward: -29.69 | Epsilon: 0.050\n",
            "Step 96300 | Loss: 0.006041\n",
            "Step 96400 | Loss: 0.023756\n",
            "Episode 482 | Reward: -27.41 | Epsilon: 0.050\n",
            "Step 96500 | Loss: 0.008612\n",
            "Step 96600 | Loss: 0.005789\n",
            "Episode 483 | Reward: -9.73 | Epsilon: 0.050\n",
            "Step 96700 | Loss: 0.006207\n",
            "Step 96800 | Loss: 0.003496\n",
            "Episode 484 | Reward: -18.47 | Epsilon: 0.050\n",
            "Step 96900 | Loss: 0.008097\n",
            "Step 97000 | Loss: 0.034437\n",
            "Episode 485 | Reward: -47.66 | Epsilon: 0.050\n",
            "Step 97100 | Loss: 0.016922\n",
            "Step 97200 | Loss: 0.033568\n",
            "Episode 486 | Reward: -18.52 | Epsilon: 0.050\n",
            "Step 97300 | Loss: 0.007842\n",
            "Step 97400 | Loss: 0.003397\n",
            "Episode 487 | Reward: -19.39 | Epsilon: 0.050\n",
            "Step 97500 | Loss: 0.007244\n",
            "Step 97600 | Loss: 0.003280\n",
            "Episode 488 | Reward: -46.59 | Epsilon: 0.050\n",
            "Step 97700 | Loss: 0.003308\n",
            "Step 97800 | Loss: 0.031160\n",
            "Episode 489 | Reward: -33.68 | Epsilon: 0.050\n",
            "Step 97900 | Loss: 0.923801\n",
            "Step 98000 | Loss: 0.003616\n",
            "Episode 490 | Reward: -7.61 | Epsilon: 0.050\n",
            "Step 98100 | Loss: 23.568705\n",
            "Step 98200 | Loss: 0.262036\n",
            "Episode 491 | Reward: -27.67 | Epsilon: 0.050\n",
            "Step 98300 | Loss: 0.004492\n",
            "Step 98400 | Loss: 0.007825\n",
            "Episode 492 | Reward: -1.74 | Epsilon: 0.050\n",
            "Step 98500 | Loss: 0.013743\n",
            "Step 98600 | Loss: 0.006970\n",
            "Episode 493 | Reward: -2.04 | Epsilon: 0.050\n",
            "Step 98700 | Loss: 0.009037\n",
            "Step 98800 | Loss: 0.663356\n",
            "Episode 494 | Reward: -36.21 | Epsilon: 0.050\n",
            "Step 98900 | Loss: 0.653732\n",
            "Step 99000 | Loss: 1.265117\n",
            "Episode 495 | Reward: -22.65 | Epsilon: 0.050\n",
            "Step 99100 | Loss: 0.015390\n",
            "Step 99200 | Loss: 0.023717\n",
            "Episode 496 | Reward: -25.68 | Epsilon: 0.050\n",
            "Step 99300 | Loss: 0.004799\n",
            "Step 99400 | Loss: 0.009349\n",
            "Episode 497 | Reward: -28.35 | Epsilon: 0.050\n",
            "Step 99500 | Loss: 0.005395\n",
            "Step 99600 | Loss: 0.007074\n",
            "Episode 498 | Reward: -27.57 | Epsilon: 0.050\n",
            "Step 99700 | Loss: 0.643793\n",
            "Step 99800 | Loss: 0.011356\n",
            "Episode 499 | Reward: -52.93 | Epsilon: 0.050\n",
            "Step 99900 | Loss: 0.013726\n",
            "Step 100000 | Loss: 0.004173\n",
            "Episode 500 | Target network updated.\n",
            "model saved to models/2025-04-29_03-55-33/q_network_ep_0500.pth\n",
            "\n",
            "Episode 500 | Reward: -21.96 | Epsilon: 0.050\n",
            "Step 100100 | Loss: 0.030858\n",
            "Step 100200 | Loss: 0.018434\n",
            "Episode 501 | Reward: -41.10 | Epsilon: 0.050\n",
            "Step 100300 | Loss: 0.008774\n",
            "Step 100400 | Loss: 0.010361\n",
            "Episode 502 | Reward: -7.73 | Epsilon: 0.050\n",
            "Step 100500 | Loss: 0.007224\n",
            "Step 100600 | Loss: 0.014688\n",
            "Episode 503 | Reward: -33.86 | Epsilon: 0.050\n",
            "Step 100700 | Loss: 0.006381\n",
            "Step 100800 | Loss: 0.006917\n",
            "Episode 504 | Reward: -44.58 | Epsilon: 0.050\n",
            "Step 100900 | Loss: 0.008681\n",
            "Step 101000 | Loss: 0.007006\n",
            "Episode 505 | Reward: -162.29 | Epsilon: 0.050\n",
            "Step 101100 | Loss: 0.013911\n",
            "Step 101200 | Loss: 0.009679\n",
            "Episode 506 | Reward: -88.08 | Epsilon: 0.050\n",
            "Step 101300 | Loss: 0.006771\n",
            "Step 101400 | Loss: 0.269762\n",
            "Episode 507 | Reward: -13.58 | Epsilon: 0.050\n",
            "Step 101500 | Loss: 0.005053\n",
            "Step 101600 | Loss: 0.010317\n",
            "Episode 508 | Reward: -12.77 | Epsilon: 0.050\n",
            "Step 101700 | Loss: 0.005426\n",
            "Step 101800 | Loss: 0.490237\n",
            "Episode 509 | Reward: -11.11 | Epsilon: 0.050\n",
            "Step 101900 | Loss: 0.005268\n",
            "Step 102000 | Loss: 0.009268\n",
            "Episode 510 | Reward: -8.55 | Epsilon: 0.050\n",
            "Step 102100 | Loss: 0.101239\n",
            "Step 102200 | Loss: 0.010987\n",
            "Episode 511 | Reward: -11.21 | Epsilon: 0.050\n",
            "Step 102300 | Loss: 0.027180\n",
            "Step 102400 | Loss: 0.011135\n",
            "Episode 512 | Reward: -57.18 | Epsilon: 0.050\n",
            "Step 102500 | Loss: 0.625997\n",
            "Step 102600 | Loss: 0.025122\n",
            "Episode 513 | Reward: -50.67 | Epsilon: 0.050\n",
            "Step 102700 | Loss: 0.022713\n",
            "Step 102800 | Loss: 0.284824\n",
            "Episode 514 | Reward: -29.43 | Epsilon: 0.050\n",
            "Step 102900 | Loss: 0.004465\n",
            "Step 103000 | Loss: 0.010263\n",
            "Episode 515 | Reward: -50.96 | Epsilon: 0.050\n",
            "Step 103100 | Loss: 0.165580\n",
            "Step 103200 | Loss: 0.024536\n",
            "Episode 516 | Reward: -103.63 | Epsilon: 0.050\n",
            "Step 103300 | Loss: 0.592024\n",
            "Step 103400 | Loss: 2.028972\n",
            "Episode 517 | Reward: -56.97 | Epsilon: 0.050\n",
            "Step 103500 | Loss: 0.006286\n",
            "Step 103600 | Loss: 0.008221\n",
            "Episode 518 | Reward: -6.17 | Epsilon: 0.050\n",
            "Step 103700 | Loss: 0.018758\n",
            "Step 103800 | Loss: 0.010490\n",
            "Episode 519 | Reward: -22.40 | Epsilon: 0.050\n",
            "Step 103900 | Loss: 0.028339\n",
            "Step 104000 | Loss: 0.208364\n",
            "Episode 520 | Target network updated.\n",
            "Episode 520 | Reward: -14.71 | Epsilon: 0.050\n",
            "Step 104100 | Loss: 0.053930\n",
            "Step 104200 | Loss: 0.184787\n",
            "Episode 521 | Reward: -12.38 | Epsilon: 0.050\n",
            "Step 104300 | Loss: 0.006636\n",
            "Step 104400 | Loss: 0.006846\n",
            "Episode 522 | Reward: -7.67 | Epsilon: 0.050\n",
            "Step 104500 | Loss: 0.019037\n",
            "Step 104600 | Loss: 0.104397\n",
            "Episode 523 | Reward: -15.06 | Epsilon: 0.050\n",
            "Step 104700 | Loss: 0.832071\n",
            "Step 104800 | Loss: 0.003950\n",
            "Episode 524 | Reward: -44.63 | Epsilon: 0.050\n",
            "Step 104900 | Loss: 0.005031\n",
            "Step 105000 | Loss: 0.008780\n",
            "Episode 525 | Reward: -49.14 | Epsilon: 0.050\n",
            "Step 105100 | Loss: 0.114960\n",
            "Step 105200 | Loss: 0.006608\n",
            "Episode 526 | Reward: -65.67 | Epsilon: 0.050\n",
            "Step 105300 | Loss: 0.012539\n",
            "Step 105400 | Loss: 0.029466\n",
            "Episode 527 | Reward: -10.76 | Epsilon: 0.050\n",
            "Step 105500 | Loss: 0.193581\n",
            "Step 105600 | Loss: 3.622311\n",
            "Episode 528 | Reward: -13.94 | Epsilon: 0.050\n",
            "Step 105700 | Loss: 0.007541\n",
            "Step 105800 | Loss: 0.022934\n",
            "Episode 529 | Reward: -10.72 | Epsilon: 0.050\n",
            "Step 105900 | Loss: 0.009850\n",
            "Step 106000 | Loss: 0.007432\n",
            "Episode 530 | Reward: -8.54 | Epsilon: 0.050\n",
            "Step 106100 | Loss: 0.114567\n",
            "Step 106200 | Loss: 0.015028\n",
            "Episode 531 | Reward: -4.97 | Epsilon: 0.050\n",
            "Step 106300 | Loss: 0.006558\n",
            "Step 106400 | Loss: 0.010183\n",
            "Episode 532 | Reward: -79.62 | Epsilon: 0.050\n",
            "Step 106500 | Loss: 2.274751\n",
            "Step 106600 | Loss: 0.010913\n",
            "Episode 533 | Reward: -34.97 | Epsilon: 0.050\n",
            "Step 106700 | Loss: 0.186008\n",
            "Step 106800 | Loss: 0.017932\n",
            "Episode 534 | Reward: -61.02 | Epsilon: 0.050\n",
            "Step 106900 | Loss: 0.029157\n",
            "Step 107000 | Loss: 0.010487\n",
            "Episode 535 | Reward: -33.23 | Epsilon: 0.050\n",
            "Step 107100 | Loss: 0.405202\n",
            "Step 107200 | Loss: 0.062410\n",
            "Episode 536 | Reward: -41.69 | Epsilon: 0.050\n",
            "Step 107300 | Loss: 0.171502\n",
            "Step 107400 | Loss: 0.007306\n",
            "Episode 537 | Reward: -12.81 | Epsilon: 0.050\n",
            "Step 107500 | Loss: 0.037190\n",
            "Step 107600 | Loss: 1.212849\n",
            "Episode 538 | Reward: -39.46 | Epsilon: 0.050\n",
            "Step 107700 | Loss: 1.230232\n",
            "Step 107800 | Loss: 0.005579\n",
            "Episode 539 | Reward: -48.54 | Epsilon: 0.050\n",
            "Step 107900 | Loss: 0.008369\n",
            "Step 108000 | Loss: 0.007276\n",
            "Episode 540 | Target network updated.\n",
            "Episode 540 | Reward: -6.56 | Epsilon: 0.050\n",
            "Step 108100 | Loss: 0.045683\n",
            "Step 108200 | Loss: 0.013519\n",
            "Episode 541 | Reward: -8.36 | Epsilon: 0.050\n",
            "Step 108300 | Loss: 1.186093\n",
            "Step 108400 | Loss: 0.021752\n",
            "Episode 542 | Reward: -58.45 | Epsilon: 0.050\n",
            "Step 108500 | Loss: 0.010381\n",
            "Step 108600 | Loss: 0.011641\n",
            "Episode 543 | Reward: -98.11 | Epsilon: 0.050\n",
            "Step 108700 | Loss: 0.006787\n",
            "Step 108800 | Loss: 0.004801\n",
            "Episode 544 | Reward: -15.68 | Epsilon: 0.050\n",
            "Step 108900 | Loss: 0.550342\n",
            "Step 109000 | Loss: 0.004009\n",
            "Episode 545 | Reward: -12.57 | Epsilon: 0.050\n",
            "Step 109100 | Loss: 0.012623\n",
            "Step 109200 | Loss: 0.003817\n",
            "Episode 546 | Reward: -25.73 | Epsilon: 0.050\n",
            "Step 109300 | Loss: 0.103210\n",
            "Step 109400 | Loss: 2.043040\n",
            "Episode 547 | Reward: -41.29 | Epsilon: 0.050\n",
            "Step 109500 | Loss: 0.015443\n",
            "Step 109600 | Loss: 0.029109\n",
            "Episode 548 | Reward: -14.70 | Epsilon: 0.050\n",
            "Step 109700 | Loss: 0.040564\n",
            "Step 109800 | Loss: 0.010596\n",
            "Episode 549 | Reward: -68.52 | Epsilon: 0.050\n",
            "Step 109900 | Loss: 1.587929\n",
            "Step 110000 | Loss: 0.017960\n",
            "Episode 550 | Reward: -5.87 | Epsilon: 0.050\n",
            "Step 110100 | Loss: 0.005408\n",
            "Step 110200 | Loss: 0.010007\n",
            "Episode 551 | Reward: -44.07 | Epsilon: 0.050\n",
            "Step 110300 | Loss: 0.009572\n",
            "Step 110400 | Loss: 0.005731\n",
            "Episode 552 | Reward: -26.14 | Epsilon: 0.050\n",
            "Step 110500 | Loss: 2.276932\n",
            "Step 110600 | Loss: 0.031380\n",
            "Episode 553 | Reward: -12.23 | Epsilon: 0.050\n",
            "Step 110700 | Loss: 0.033916\n",
            "Step 110800 | Loss: 0.031515\n",
            "Episode 554 | Reward: -33.65 | Epsilon: 0.050\n",
            "Step 110900 | Loss: 0.030118\n",
            "Step 111000 | Loss: 0.008239\n",
            "Episode 555 | Reward: -11.20 | Epsilon: 0.050\n",
            "Step 111100 | Loss: 0.007866\n",
            "Step 111200 | Loss: 0.010326\n",
            "Episode 556 | Reward: -8.55 | Epsilon: 0.050\n",
            "Step 111300 | Loss: 0.022605\n",
            "Step 111400 | Loss: 0.004464\n",
            "Episode 557 | Reward: -22.39 | Epsilon: 0.050\n",
            "Step 111500 | Loss: 0.489908\n",
            "Step 111600 | Loss: 0.004907\n",
            "Episode 558 | Reward: -6.98 | Epsilon: 0.050\n",
            "Step 111700 | Loss: 0.007774\n",
            "Step 111800 | Loss: 0.030236\n",
            "Episode 559 | Reward: -41.94 | Epsilon: 0.050\n",
            "Step 111900 | Loss: 0.012786\n",
            "Step 112000 | Loss: 0.080143\n",
            "Episode 560 | Target network updated.\n",
            "Episode 560 | Reward: -19.38 | Epsilon: 0.050\n",
            "Step 112100 | Loss: 0.008561\n",
            "Step 112200 | Loss: 0.492145\n",
            "Episode 561 | Reward: -19.03 | Epsilon: 0.050\n",
            "Step 112300 | Loss: 0.009183\n",
            "Step 112400 | Loss: 0.023408\n",
            "Episode 562 | Reward: -46.63 | Epsilon: 0.050\n",
            "Step 112500 | Loss: 0.484746\n",
            "Step 112600 | Loss: 0.108234\n",
            "Episode 563 | Reward: -42.26 | Epsilon: 0.050\n",
            "Step 112700 | Loss: 0.004310\n",
            "Step 112800 | Loss: 0.119948\n",
            "Episode 564 | Reward: -22.43 | Epsilon: 0.050\n",
            "Step 112900 | Loss: 0.004132\n",
            "Step 113000 | Loss: 0.007506\n",
            "Episode 565 | Reward: -9.92 | Epsilon: 0.050\n",
            "Step 113100 | Loss: 0.011704\n",
            "Step 113200 | Loss: 0.006689\n",
            "Episode 566 | Reward: -13.93 | Epsilon: 0.050\n",
            "Step 113300 | Loss: 1.881411\n",
            "Step 113400 | Loss: 0.026930\n",
            "Episode 567 | Reward: -5.89 | Epsilon: 0.050\n",
            "Step 113500 | Loss: 0.016594\n",
            "Step 113600 | Loss: 0.004380\n",
            "Episode 568 | Reward: -5.63 | Epsilon: 0.050\n",
            "Step 113700 | Loss: 0.026931\n",
            "Step 113800 | Loss: 0.003945\n",
            "Episode 569 | Reward: -12.45 | Epsilon: 0.050\n",
            "Step 113900 | Loss: 0.006043\n",
            "Step 114000 | Loss: 0.003462\n",
            "Episode 570 | Reward: -10.32 | Epsilon: 0.050\n",
            "Step 114100 | Loss: 0.003624\n",
            "Step 114200 | Loss: 0.305667\n",
            "Episode 571 | Reward: -6.28 | Epsilon: 0.050\n",
            "Step 114300 | Loss: 0.007225\n",
            "Step 114400 | Loss: 0.083328\n",
            "Episode 572 | Reward: -55.54 | Epsilon: 0.050\n",
            "Step 114500 | Loss: 0.575897\n",
            "Step 114600 | Loss: 0.003401\n",
            "Episode 573 | Reward: -15.78 | Epsilon: 0.050\n",
            "Step 114700 | Loss: 0.002802\n",
            "Step 114800 | Loss: 0.009612\n",
            "Episode 574 | Reward: -11.61 | Epsilon: 0.050\n",
            "Step 114900 | Loss: 0.014791\n",
            "Step 115000 | Loss: 0.006150\n",
            "Episode 575 | Reward: -11.59 | Epsilon: 0.050\n",
            "Step 115100 | Loss: 0.006957\n",
            "Step 115200 | Loss: 2.603138\n",
            "Episode 576 | Reward: -41.32 | Epsilon: 0.050\n",
            "Step 115300 | Loss: 0.003894\n",
            "Step 115400 | Loss: 0.005278\n",
            "Episode 577 | Reward: -7.76 | Epsilon: 0.050\n",
            "Step 115500 | Loss: 0.031907\n",
            "Step 115600 | Loss: 0.026209\n",
            "Episode 578 | Reward: -136.92 | Epsilon: 0.050\n",
            "Step 115700 | Loss: 0.135752\n",
            "Step 115800 | Loss: 0.005779\n",
            "Episode 579 | Reward: -26.53 | Epsilon: 0.050\n",
            "Step 115900 | Loss: 0.012153\n",
            "Step 116000 | Loss: 0.008479\n",
            "Episode 580 | Target network updated.\n",
            "Episode 580 | Reward: -13.77 | Epsilon: 0.050\n",
            "Step 116100 | Loss: 0.009160\n",
            "Step 116200 | Loss: 0.005054\n",
            "Episode 581 | Reward: -37.03 | Epsilon: 0.050\n",
            "Step 116300 | Loss: 0.005990\n",
            "Step 116400 | Loss: 0.004645\n",
            "Episode 582 | Reward: -22.60 | Epsilon: 0.050\n",
            "Step 116500 | Loss: 0.006881\n",
            "Step 116600 | Loss: 0.004442\n",
            "Episode 583 | Reward: -322.98 | Epsilon: 0.050\n",
            "Step 116700 | Loss: 0.007531\n",
            "Step 116800 | Loss: 0.008773\n",
            "Episode 584 | Reward: -19.72 | Epsilon: 0.050\n",
            "Step 116900 | Loss: 0.014048\n",
            "Step 117000 | Loss: 0.011769\n",
            "Episode 585 | Reward: -24.76 | Epsilon: 0.050\n",
            "Step 117100 | Loss: 0.005028\n",
            "Step 117200 | Loss: 0.016852\n",
            "Episode 586 | Reward: -4.45 | Epsilon: 0.050\n",
            "Step 117300 | Loss: 0.049251\n",
            "Step 117400 | Loss: 0.008007\n",
            "Episode 587 | Reward: -2.39 | Epsilon: 0.050\n",
            "Step 117500 | Loss: 0.003696\n",
            "Step 117600 | Loss: 0.007583\n",
            "Episode 588 | Reward: -7.44 | Epsilon: 0.050\n",
            "Step 117700 | Loss: 0.014607\n",
            "Step 117800 | Loss: 0.058039\n",
            "Episode 589 | Reward: -7.68 | Epsilon: 0.050\n",
            "Step 117900 | Loss: 0.004344\n",
            "Step 118000 | Loss: 0.006470\n",
            "Episode 590 | Reward: -93.04 | Epsilon: 0.050\n",
            "Step 118100 | Loss: 0.016229\n",
            "Step 118200 | Loss: 0.014303\n",
            "Episode 591 | Reward: -27.09 | Epsilon: 0.050\n",
            "Step 118300 | Loss: 0.005098\n",
            "Step 118400 | Loss: 0.006455\n",
            "Episode 592 | Reward: -7.21 | Epsilon: 0.050\n",
            "Step 118500 | Loss: 0.008114\n",
            "Step 118600 | Loss: 0.007766\n",
            "Episode 593 | Reward: -4.93 | Epsilon: 0.050\n",
            "Step 118700 | Loss: 0.005526\n",
            "Step 118800 | Loss: 0.007919\n",
            "Episode 594 | Reward: -4.54 | Epsilon: 0.050\n",
            "Step 118900 | Loss: 0.006429\n",
            "Step 119000 | Loss: 1.256980\n",
            "Episode 595 | Reward: -23.83 | Epsilon: 0.050\n",
            "Step 119100 | Loss: 2.059187\n",
            "Step 119200 | Loss: 0.034466\n",
            "Episode 596 | Reward: -21.09 | Epsilon: 0.050\n",
            "Step 119300 | Loss: 0.694897\n",
            "Step 119400 | Loss: 0.008833\n",
            "Episode 597 | Reward: -8.24 | Epsilon: 0.050\n",
            "Step 119500 | Loss: 4.851764\n",
            "Step 119600 | Loss: 0.006406\n",
            "Episode 598 | Reward: -9.48 | Epsilon: 0.050\n",
            "Step 119700 | Loss: 0.005673\n",
            "Step 119800 | Loss: 0.008590\n",
            "Episode 599 | Reward: -6.57 | Epsilon: 0.050\n",
            "Step 119900 | Loss: 0.006852\n",
            "Step 120000 | Loss: 0.029382\n",
            "Episode 600 | Target network updated.\n",
            "model saved to models/2025-04-29_03-57-41/q_network_ep_0600.pth\n",
            "\n",
            "Episode 600 | Reward: -556.12 | Epsilon: 0.050\n",
            "Step 120100 | Loss: 0.050152\n",
            "Step 120200 | Loss: 0.008950\n",
            "Episode 601 | Reward: -51.70 | Epsilon: 0.050\n",
            "Step 120300 | Loss: 0.005947\n",
            "Step 120400 | Loss: 0.005957\n",
            "Episode 602 | Reward: -19.30 | Epsilon: 0.050\n",
            "Step 120500 | Loss: 0.011812\n",
            "Step 120600 | Loss: 0.010334\n",
            "Episode 603 | Reward: -11.16 | Epsilon: 0.050\n",
            "Step 120700 | Loss: 0.005872\n",
            "Step 120800 | Loss: 0.024910\n",
            "Episode 604 | Reward: -13.79 | Epsilon: 0.050\n",
            "Step 120900 | Loss: 2.267645\n",
            "Step 121000 | Loss: 0.007514\n",
            "Episode 605 | Reward: -16.63 | Epsilon: 0.050\n",
            "Step 121100 | Loss: 0.003468\n",
            "Step 121200 | Loss: 0.009295\n",
            "Episode 606 | Reward: -16.77 | Epsilon: 0.050\n",
            "Step 121300 | Loss: 2.325297\n",
            "Step 121400 | Loss: 0.016939\n",
            "Episode 607 | Reward: -135.45 | Epsilon: 0.050\n",
            "Step 121500 | Loss: 0.004943\n",
            "Step 121600 | Loss: 0.003753\n",
            "Episode 608 | Reward: -7.15 | Epsilon: 0.050\n",
            "Step 121700 | Loss: 0.006896\n",
            "Step 121800 | Loss: 0.007138\n",
            "Episode 609 | Reward: -177.44 | Epsilon: 0.050\n",
            "Step 121900 | Loss: 0.005156\n",
            "Step 122000 | Loss: 0.013295\n",
            "Episode 610 | Reward: -1.39 | Epsilon: 0.050\n",
            "Step 122100 | Loss: 0.014686\n",
            "Step 122200 | Loss: 0.055561\n",
            "Episode 611 | Reward: -122.00 | Epsilon: 0.050\n",
            "Step 122300 | Loss: 0.009175\n",
            "Step 122400 | Loss: 0.008962\n",
            "Episode 612 | Reward: -50.49 | Epsilon: 0.050\n",
            "Step 122500 | Loss: 0.008250\n",
            "Step 122600 | Loss: 0.011549\n",
            "Episode 613 | Reward: -23.69 | Epsilon: 0.050\n",
            "Step 122700 | Loss: 0.008076\n",
            "Step 122800 | Loss: 0.005591\n",
            "Episode 614 | Reward: -10.73 | Epsilon: 0.050\n",
            "Step 122900 | Loss: 24.119783\n",
            "Step 123000 | Loss: 0.066861\n",
            "Episode 615 | Reward: -8.93 | Epsilon: 0.050\n",
            "Step 123100 | Loss: 0.009803\n",
            "Step 123200 | Loss: 0.008280\n",
            "Episode 616 | Reward: -48.83 | Epsilon: 0.050\n",
            "Step 123300 | Loss: 0.023393\n",
            "Step 123400 | Loss: 0.019944\n",
            "Episode 617 | Reward: -30.61 | Epsilon: 0.050\n",
            "Step 123500 | Loss: 0.013109\n",
            "Step 123600 | Loss: 0.010209\n",
            "Episode 618 | Reward: -213.13 | Epsilon: 0.050\n",
            "Step 123700 | Loss: 0.068594\n",
            "Step 123800 | Loss: 0.004017\n",
            "Episode 619 | Reward: -25.22 | Epsilon: 0.050\n",
            "Step 123900 | Loss: 0.007287\n",
            "Step 124000 | Loss: 0.005531\n",
            "Episode 620 | Target network updated.\n",
            "Episode 620 | Reward: -66.97 | Epsilon: 0.050\n",
            "Step 124100 | Loss: 0.013158\n",
            "Step 124200 | Loss: 0.010777\n",
            "Episode 621 | Reward: -35.71 | Epsilon: 0.050\n",
            "Step 124300 | Loss: 6.959953\n",
            "Step 124400 | Loss: 0.008475\n",
            "Episode 622 | Reward: -45.24 | Epsilon: 0.050\n",
            "Step 124500 | Loss: 0.524595\n",
            "Step 124600 | Loss: 0.006799\n",
            "Episode 623 | Reward: -3.41 | Epsilon: 0.050\n",
            "Step 124700 | Loss: 0.032156\n",
            "Step 124800 | Loss: 0.009695\n",
            "Episode 624 | Reward: -1.91 | Epsilon: 0.050\n",
            "Step 124900 | Loss: 0.096609\n",
            "Step 125000 | Loss: 0.006593\n",
            "Episode 625 | Reward: -7.72 | Epsilon: 0.050\n",
            "Step 125100 | Loss: 0.015277\n",
            "Step 125200 | Loss: 0.027671\n",
            "Episode 626 | Reward: -3.10 | Epsilon: 0.050\n",
            "Step 125300 | Loss: 0.014915\n",
            "Step 125400 | Loss: 0.165544\n",
            "Episode 627 | Reward: -63.88 | Epsilon: 0.050\n",
            "Step 125500 | Loss: 0.011120\n",
            "Step 125600 | Loss: 0.007014\n",
            "Episode 628 | Reward: -304.81 | Epsilon: 0.050\n",
            "Step 125700 | Loss: 0.009514\n",
            "Step 125800 | Loss: 0.028290\n",
            "Episode 629 | Reward: -59.30 | Epsilon: 0.050\n",
            "Step 125900 | Loss: 0.008380\n",
            "Step 126000 | Loss: 0.042661\n",
            "Episode 630 | Reward: -34.01 | Epsilon: 0.050\n",
            "Step 126100 | Loss: 0.007133\n",
            "Step 126200 | Loss: 0.006994\n",
            "Episode 631 | Reward: -26.41 | Epsilon: 0.050\n",
            "Step 126300 | Loss: 0.007811\n",
            "Step 126400 | Loss: 0.017777\n",
            "Episode 632 | Reward: -3.12 | Epsilon: 0.050\n",
            "Step 126500 | Loss: 0.029170\n",
            "Step 126600 | Loss: 2.577085\n",
            "Episode 633 | Reward: -38.94 | Epsilon: 0.050\n",
            "Step 126700 | Loss: 0.004588\n",
            "Step 126800 | Loss: 0.013591\n",
            "Episode 634 | Reward: -8.77 | Epsilon: 0.050\n",
            "Step 126900 | Loss: 0.064417\n",
            "Step 127000 | Loss: 0.013851\n",
            "Episode 635 | Reward: -11.22 | Epsilon: 0.050\n",
            "Step 127100 | Loss: 0.013780\n",
            "Step 127200 | Loss: 0.008030\n",
            "Episode 636 | Reward: -223.66 | Epsilon: 0.050\n",
            "Step 127300 | Loss: 0.004317\n",
            "Step 127400 | Loss: 0.013237\n",
            "Episode 637 | Reward: -13.15 | Epsilon: 0.050\n",
            "Step 127500 | Loss: 0.059357\n",
            "Step 127600 | Loss: 0.064102\n",
            "Episode 638 | Reward: -6.13 | Epsilon: 0.050\n",
            "Step 127700 | Loss: 0.014004\n",
            "Step 127800 | Loss: 0.003492\n",
            "Episode 639 | Reward: -4.34 | Epsilon: 0.050\n",
            "Step 127900 | Loss: 0.006047\n",
            "Step 128000 | Loss: 0.008842\n",
            "Episode 640 | Target network updated.\n",
            "Episode 640 | Reward: -0.73 | Epsilon: 0.050\n",
            "Step 128100 | Loss: 0.007725\n",
            "Step 128200 | Loss: 26.643637\n",
            "Episode 641 | Reward: -7.78 | Epsilon: 0.050\n",
            "Step 128300 | Loss: 0.008934\n",
            "Step 128400 | Loss: 0.006170\n",
            "Episode 642 | Reward: -32.63 | Epsilon: 0.050\n",
            "Step 128500 | Loss: 0.016683\n",
            "Step 128600 | Loss: 0.011083\n",
            "Episode 643 | Reward: -68.77 | Epsilon: 0.050\n",
            "Step 128700 | Loss: 0.007293\n",
            "Step 128800 | Loss: 0.013876\n",
            "Episode 644 | Reward: -29.51 | Epsilon: 0.050\n",
            "Step 128900 | Loss: 0.008538\n",
            "Step 129000 | Loss: 0.007795\n",
            "Episode 645 | Reward: -37.80 | Epsilon: 0.050\n",
            "Step 129100 | Loss: 0.007929\n",
            "Step 129200 | Loss: 0.176590\n",
            "Episode 646 | Reward: -51.50 | Epsilon: 0.050\n",
            "Step 129300 | Loss: 0.005778\n",
            "Step 129400 | Loss: 0.004795\n",
            "Episode 647 | Reward: -11.31 | Epsilon: 0.050\n",
            "Step 129500 | Loss: 0.055021\n",
            "Step 129600 | Loss: 0.297669\n",
            "Episode 648 | Reward: -32.86 | Epsilon: 0.050\n",
            "Step 129700 | Loss: 0.100329\n",
            "Step 129800 | Loss: 0.821065\n",
            "Episode 649 | Reward: -48.72 | Epsilon: 0.050\n",
            "Step 129900 | Loss: 0.007358\n",
            "Step 130000 | Loss: 0.009353\n",
            "Episode 650 | Reward: -63.43 | Epsilon: 0.050\n",
            "Step 130100 | Loss: 2.475064\n",
            "Step 130200 | Loss: 0.018010\n",
            "Episode 651 | Reward: -16.42 | Epsilon: 0.050\n",
            "Step 130300 | Loss: 0.007968\n",
            "Step 130400 | Loss: 0.024221\n",
            "Episode 652 | Reward: -60.22 | Epsilon: 0.050\n",
            "Step 130500 | Loss: 0.006345\n",
            "Step 130600 | Loss: 0.008280\n",
            "Episode 653 | Reward: -49.36 | Epsilon: 0.050\n",
            "Step 130700 | Loss: 0.004280\n",
            "Step 130800 | Loss: 0.006629\n",
            "Episode 654 | Reward: -37.25 | Epsilon: 0.050\n",
            "Step 130900 | Loss: 0.010142\n",
            "Step 131000 | Loss: 0.053502\n",
            "Episode 655 | Reward: -6.81 | Epsilon: 0.050\n",
            "Step 131100 | Loss: 4.488696\n",
            "Step 131200 | Loss: 0.013192\n",
            "Episode 656 | Reward: -51.21 | Epsilon: 0.050\n",
            "Step 131300 | Loss: 0.018737\n",
            "Step 131400 | Loss: 0.007375\n",
            "Episode 657 | Reward: -50.74 | Epsilon: 0.050\n",
            "Step 131500 | Loss: 0.008303\n",
            "Step 131600 | Loss: 0.009426\n",
            "Episode 658 | Reward: -4.97 | Epsilon: 0.050\n",
            "Step 131700 | Loss: 0.067302\n",
            "Step 131800 | Loss: 4.612304\n",
            "Episode 659 | Reward: -1042.82 | Epsilon: 0.050\n",
            "Step 131900 | Loss: 0.420051\n",
            "Step 132000 | Loss: 0.005269\n",
            "Episode 660 | Target network updated.\n",
            "Episode 660 | Reward: -53.79 | Epsilon: 0.050\n",
            "Step 132100 | Loss: 0.411676\n",
            "Step 132200 | Loss: 0.025086\n",
            "Episode 661 | Reward: -53.84 | Epsilon: 0.050\n",
            "Step 132300 | Loss: 0.008329\n",
            "Step 132400 | Loss: 0.016813\n",
            "Episode 662 | Reward: -50.92 | Epsilon: 0.050\n",
            "Step 132500 | Loss: 0.240169\n",
            "Step 132600 | Loss: 0.004856\n",
            "Episode 663 | Reward: -18.83 | Epsilon: 0.050\n",
            "Step 132700 | Loss: 0.014078\n",
            "Step 132800 | Loss: 0.043718\n",
            "Episode 664 | Reward: -108.65 | Epsilon: 0.050\n",
            "Step 132900 | Loss: 0.039841\n",
            "Step 133000 | Loss: 0.020697\n",
            "Episode 665 | Reward: -37.27 | Epsilon: 0.050\n",
            "Step 133100 | Loss: 0.025559\n",
            "Step 133200 | Loss: 0.005390\n",
            "Episode 666 | Reward: -26.66 | Epsilon: 0.050\n",
            "Step 133300 | Loss: 0.058931\n",
            "Step 133400 | Loss: 0.019798\n",
            "Episode 667 | Reward: -6.98 | Epsilon: 0.050\n",
            "Step 133500 | Loss: 0.010799\n",
            "Step 133600 | Loss: 0.011062\n",
            "Episode 668 | Reward: -29.62 | Epsilon: 0.050\n",
            "Step 133700 | Loss: 0.011520\n",
            "Step 133800 | Loss: 0.015710\n",
            "Episode 669 | Reward: -3.92 | Epsilon: 0.050\n",
            "Step 133900 | Loss: 0.364219\n",
            "Step 134000 | Loss: 0.019343\n",
            "Episode 670 | Reward: -18.90 | Epsilon: 0.050\n",
            "Step 134100 | Loss: 0.018316\n",
            "Step 134200 | Loss: 0.008118\n",
            "Episode 671 | Reward: -12.10 | Epsilon: 0.050\n",
            "Step 134300 | Loss: 0.012362\n",
            "Step 134400 | Loss: 0.018874\n",
            "Episode 672 | Reward: -58.78 | Epsilon: 0.050\n",
            "Step 134500 | Loss: 0.007719\n",
            "Step 134600 | Loss: 0.798276\n",
            "Episode 673 | Reward: -176.01 | Epsilon: 0.050\n",
            "Step 134700 | Loss: 0.006974\n",
            "Step 134800 | Loss: 0.008555\n",
            "Episode 674 | Reward: -23.35 | Epsilon: 0.050\n",
            "Step 134900 | Loss: 0.010104\n",
            "Step 135000 | Loss: 0.167825\n",
            "Episode 675 | Reward: -15.86 | Epsilon: 0.050\n",
            "Step 135100 | Loss: 0.139771\n",
            "Step 135200 | Loss: 0.848710\n",
            "Episode 676 | Reward: -8.98 | Epsilon: 0.050\n",
            "Step 135300 | Loss: 0.008184\n",
            "Step 135400 | Loss: 0.012911\n",
            "Episode 677 | Reward: -35.71 | Epsilon: 0.050\n",
            "Step 135500 | Loss: 0.011359\n",
            "Step 135600 | Loss: 0.518359\n",
            "Episode 678 | Reward: -48.00 | Epsilon: 0.050\n",
            "Step 135700 | Loss: 0.019236\n",
            "Step 135800 | Loss: 0.008049\n",
            "Episode 679 | Reward: -30.21 | Epsilon: 0.050\n",
            "Step 135900 | Loss: 95.618019\n",
            "Step 136000 | Loss: 0.009508\n",
            "Episode 680 | Target network updated.\n",
            "Episode 680 | Reward: -16.49 | Epsilon: 0.050\n",
            "Step 136100 | Loss: 0.022166\n",
            "Step 136200 | Loss: 0.012657\n",
            "Episode 681 | Reward: -60.21 | Epsilon: 0.050\n",
            "Step 136300 | Loss: 4.044780\n",
            "Step 136400 | Loss: 0.008984\n",
            "Episode 682 | Reward: -25.54 | Epsilon: 0.050\n",
            "Step 136500 | Loss: 0.621277\n",
            "Step 136600 | Loss: 0.033663\n",
            "Episode 683 | Reward: -28.11 | Epsilon: 0.050\n",
            "Step 136700 | Loss: 0.015856\n",
            "Step 136800 | Loss: 0.632111\n",
            "Episode 684 | Reward: -47.70 | Epsilon: 0.050\n",
            "Step 136900 | Loss: 0.028125\n",
            "Step 137000 | Loss: 0.015887\n",
            "Episode 685 | Reward: -6.36 | Epsilon: 0.050\n",
            "Step 137100 | Loss: 0.026925\n",
            "Step 137200 | Loss: 0.059904\n",
            "Episode 686 | Reward: -55.35 | Epsilon: 0.050\n",
            "Step 137300 | Loss: 0.006435\n",
            "Step 137400 | Loss: 0.103608\n",
            "Episode 687 | Reward: -88.69 | Epsilon: 0.050\n",
            "Step 137500 | Loss: 0.012232\n",
            "Step 137600 | Loss: 0.080826\n",
            "Episode 688 | Reward: -136.98 | Epsilon: 0.050\n",
            "Step 137700 | Loss: 0.015655\n",
            "Step 137800 | Loss: 0.156389\n",
            "Episode 689 | Reward: -19.59 | Epsilon: 0.050\n",
            "Step 137900 | Loss: 0.016191\n",
            "Step 138000 | Loss: 0.020600\n",
            "Episode 690 | Reward: -10.21 | Epsilon: 0.050\n",
            "Step 138100 | Loss: 0.009795\n",
            "Step 138200 | Loss: 1.159877\n",
            "Episode 691 | Reward: -567.13 | Epsilon: 0.050\n",
            "Step 138300 | Loss: 0.013394\n",
            "Step 138400 | Loss: 0.013938\n",
            "Episode 692 | Reward: -2.67 | Epsilon: 0.050\n",
            "Step 138500 | Loss: 0.475520\n",
            "Step 138600 | Loss: 0.022626\n",
            "Episode 693 | Reward: -11.35 | Epsilon: 0.050\n",
            "Step 138700 | Loss: 0.011312\n",
            "Step 138800 | Loss: 0.008717\n",
            "Episode 694 | Reward: -6.85 | Epsilon: 0.050\n",
            "Step 138900 | Loss: 0.022560\n",
            "Step 139000 | Loss: 0.018285\n",
            "Episode 695 | Reward: -16.80 | Epsilon: 0.050\n",
            "Step 139100 | Loss: 1.027018\n",
            "Step 139200 | Loss: 0.031105\n",
            "Episode 696 | Reward: -23.16 | Epsilon: 0.050\n",
            "Step 139300 | Loss: 0.036878\n",
            "Step 139400 | Loss: 0.013539\n",
            "Episode 697 | Reward: -36.38 | Epsilon: 0.050\n",
            "Step 139500 | Loss: 0.055193\n",
            "Step 139600 | Loss: 0.027835\n",
            "Episode 698 | Reward: -2.55 | Epsilon: 0.050\n",
            "Step 139700 | Loss: 0.046302\n",
            "Step 139800 | Loss: 0.024073\n",
            "Episode 699 | Reward: -58.57 | Epsilon: 0.050\n",
            "Step 139900 | Loss: 0.049515\n",
            "Step 140000 | Loss: 0.031544\n",
            "Episode 700 | Target network updated.\n",
            "model saved to models/2025-04-29_03-59-50/q_network_ep_0700.pth\n",
            "\n",
            "Episode 700 | Reward: -860.89 | Epsilon: 0.050\n",
            "Step 140100 | Loss: 1.714824\n",
            "Step 140200 | Loss: 0.014306\n",
            "Episode 701 | Reward: -5.38 | Epsilon: 0.050\n",
            "Step 140300 | Loss: 23.055391\n",
            "Step 140400 | Loss: 0.008508\n",
            "Episode 702 | Reward: -25.82 | Epsilon: 0.050\n",
            "Step 140500 | Loss: 0.039168\n",
            "Step 140600 | Loss: 0.045158\n",
            "Episode 703 | Reward: -34.21 | Epsilon: 0.050\n",
            "Step 140700 | Loss: 0.016235\n",
            "Step 140800 | Loss: 0.017208\n",
            "Episode 704 | Reward: -36.95 | Epsilon: 0.050\n",
            "Step 140900 | Loss: 0.008897\n",
            "Step 141000 | Loss: 0.028942\n",
            "Episode 705 | Reward: -1.24 | Epsilon: 0.050\n",
            "Step 141100 | Loss: 0.018580\n",
            "Step 141200 | Loss: 0.010651\n",
            "Episode 706 | Reward: -12.33 | Epsilon: 0.050\n",
            "Step 141300 | Loss: 0.040698\n",
            "Step 141400 | Loss: 0.018013\n",
            "Episode 707 | Reward: -65.31 | Epsilon: 0.050\n",
            "Step 141500 | Loss: 0.026286\n",
            "Step 141600 | Loss: 0.040504\n",
            "Episode 708 | Reward: -294.43 | Epsilon: 0.050\n",
            "Step 141700 | Loss: 0.024853\n",
            "Step 141800 | Loss: 0.020359\n",
            "Episode 709 | Reward: -319.11 | Epsilon: 0.050\n",
            "Step 141900 | Loss: 0.516561\n",
            "Step 142000 | Loss: 0.007419\n",
            "Episode 710 | Reward: -9.30 | Epsilon: 0.050\n",
            "Step 142100 | Loss: 0.008207\n",
            "Step 142200 | Loss: 0.021902\n",
            "Episode 711 | Reward: -31.31 | Epsilon: 0.050\n",
            "Step 142300 | Loss: 0.021607\n",
            "Step 142400 | Loss: 0.118620\n",
            "Episode 712 | Reward: -7.84 | Epsilon: 0.050\n",
            "Step 142500 | Loss: 0.010479\n",
            "Step 142600 | Loss: 23.412659\n",
            "Episode 713 | Reward: -11.14 | Epsilon: 0.050\n",
            "Step 142700 | Loss: 0.714941\n",
            "Step 142800 | Loss: 0.013622\n",
            "Episode 714 | Reward: -39.88 | Epsilon: 0.050\n",
            "Step 142900 | Loss: 0.011414\n",
            "Step 143000 | Loss: 0.006281\n",
            "Episode 715 | Reward: -28.82 | Epsilon: 0.050\n",
            "Step 143100 | Loss: 0.012496\n",
            "Step 143200 | Loss: 0.008233\n",
            "Episode 716 | Reward: -26.21 | Epsilon: 0.050\n",
            "Step 143300 | Loss: 0.009519\n",
            "Step 143400 | Loss: 0.003936\n",
            "Episode 717 | Reward: -6.36 | Epsilon: 0.050\n",
            "Step 143500 | Loss: 0.017706\n",
            "Step 143600 | Loss: 0.021651\n",
            "Episode 718 | Reward: -5.66 | Epsilon: 0.050\n",
            "Step 143700 | Loss: 0.035632\n",
            "Step 143800 | Loss: 0.008749\n",
            "Episode 719 | Reward: -26.38 | Epsilon: 0.050\n",
            "Step 143900 | Loss: 0.015288\n",
            "Step 144000 | Loss: 2.502596\n",
            "Episode 720 | Target network updated.\n",
            "Episode 720 | Reward: -5.90 | Epsilon: 0.050\n",
            "Step 144100 | Loss: 0.037987\n",
            "Step 144200 | Loss: 0.018749\n",
            "Episode 721 | Reward: -27.44 | Epsilon: 0.050\n",
            "Step 144300 | Loss: 0.005355\n",
            "Step 144400 | Loss: 0.009383\n",
            "Episode 722 | Reward: -5.93 | Epsilon: 0.050\n",
            "Step 144500 | Loss: 0.006726\n",
            "Step 144600 | Loss: 0.010327\n",
            "Episode 723 | Reward: -22.13 | Epsilon: 0.050\n",
            "Step 144700 | Loss: 0.019965\n",
            "Step 144800 | Loss: 43.870018\n",
            "Episode 724 | Reward: -3.42 | Epsilon: 0.050\n",
            "Step 144900 | Loss: 0.020982\n",
            "Step 145000 | Loss: 0.019410\n",
            "Episode 725 | Reward: -11.02 | Epsilon: 0.050\n",
            "Step 145100 | Loss: 2.853819\n",
            "Step 145200 | Loss: 0.023663\n",
            "Episode 726 | Reward: -1.35 | Epsilon: 0.050\n",
            "Step 145300 | Loss: 0.019451\n",
            "Step 145400 | Loss: 0.010878\n",
            "Episode 727 | Reward: -32.99 | Epsilon: 0.050\n",
            "Step 145500 | Loss: 0.014400\n",
            "Step 145600 | Loss: 0.078592\n",
            "Episode 728 | Reward: -4.92 | Epsilon: 0.050\n",
            "Step 145700 | Loss: 0.007432\n",
            "Step 145800 | Loss: 0.050460\n",
            "Episode 729 | Reward: -7.46 | Epsilon: 0.050\n",
            "Step 145900 | Loss: 0.007058\n",
            "Step 146000 | Loss: 0.019028\n",
            "Episode 730 | Reward: -10.15 | Epsilon: 0.050\n",
            "Step 146100 | Loss: 0.009463\n",
            "Step 146200 | Loss: 0.014927\n",
            "Episode 731 | Reward: -21.61 | Epsilon: 0.050\n",
            "Step 146300 | Loss: 0.185329\n",
            "Step 146400 | Loss: 0.007671\n",
            "Episode 732 | Reward: -13.67 | Epsilon: 0.050\n",
            "Step 146500 | Loss: 0.022175\n",
            "Step 146600 | Loss: 0.004390\n",
            "Episode 733 | Reward: -6.34 | Epsilon: 0.050\n",
            "Step 146700 | Loss: 0.008722\n",
            "Step 146800 | Loss: 0.005193\n",
            "Episode 734 | Reward: -15.50 | Epsilon: 0.050\n",
            "Step 146900 | Loss: 0.009781\n",
            "Step 147000 | Loss: 0.016185\n",
            "Episode 735 | Reward: -17.12 | Epsilon: 0.050\n",
            "Step 147100 | Loss: 0.008488\n",
            "Step 147200 | Loss: 0.014450\n",
            "Episode 736 | Reward: -9.95 | Epsilon: 0.050\n",
            "Step 147300 | Loss: 0.004407\n",
            "Step 147400 | Loss: 0.013541\n",
            "Episode 737 | Reward: -23.93 | Epsilon: 0.050\n",
            "Step 147500 | Loss: 0.007123\n",
            "Step 147600 | Loss: 0.005434\n",
            "Episode 738 | Reward: -18.33 | Epsilon: 0.050\n",
            "Step 147700 | Loss: 0.005514\n",
            "Step 147800 | Loss: 0.023223\n",
            "Episode 739 | Reward: -24.09 | Epsilon: 0.050\n",
            "Step 147900 | Loss: 0.006684\n",
            "Step 148000 | Loss: 0.005202\n",
            "Episode 740 | Target network updated.\n",
            "Episode 740 | Reward: -20.26 | Epsilon: 0.050\n",
            "Step 148100 | Loss: 0.007855\n",
            "Step 148200 | Loss: 0.009022\n",
            "Episode 741 | Reward: -9.24 | Epsilon: 0.050\n",
            "Step 148300 | Loss: 0.018062\n",
            "Step 148400 | Loss: 0.012026\n",
            "Episode 742 | Reward: -47.04 | Epsilon: 0.050\n",
            "Step 148500 | Loss: 0.030816\n",
            "Step 148600 | Loss: 0.010452\n",
            "Episode 743 | Reward: -38.26 | Epsilon: 0.050\n",
            "Step 148700 | Loss: 0.026112\n",
            "Step 148800 | Loss: 0.012549\n",
            "Episode 744 | Reward: -16.61 | Epsilon: 0.050\n",
            "Step 148900 | Loss: 0.004473\n",
            "Step 149000 | Loss: 0.055215\n",
            "Episode 745 | Reward: -6.42 | Epsilon: 0.050\n",
            "Step 149100 | Loss: 0.043337\n",
            "Step 149200 | Loss: 0.016051\n",
            "Episode 746 | Reward: -7.35 | Epsilon: 0.050\n",
            "Step 149300 | Loss: 0.004112\n",
            "Step 149400 | Loss: 0.017453\n",
            "Episode 747 | Reward: -26.25 | Epsilon: 0.050\n",
            "Step 149500 | Loss: 0.004350\n",
            "Step 149600 | Loss: 0.012981\n",
            "Episode 748 | Reward: -62.63 | Epsilon: 0.050\n",
            "Step 149700 | Loss: 0.022230\n",
            "Step 149800 | Loss: 0.035659\n",
            "Episode 749 | Reward: -20.35 | Epsilon: 0.050\n",
            "Step 149900 | Loss: 0.005776\n",
            "Step 150000 | Loss: 0.011037\n",
            "Episode 750 | Reward: -12.22 | Epsilon: 0.050\n",
            "Step 150100 | Loss: 0.004050\n",
            "Step 150200 | Loss: 0.009315\n",
            "Episode 751 | Reward: -18.87 | Epsilon: 0.050\n",
            "Step 150300 | Loss: 0.005504\n",
            "Step 150400 | Loss: 0.026989\n",
            "Episode 752 | Reward: -4.81 | Epsilon: 0.050\n",
            "Step 150500 | Loss: 0.006774\n",
            "Step 150600 | Loss: 0.003110\n",
            "Episode 753 | Reward: -7.76 | Epsilon: 0.050\n",
            "Step 150700 | Loss: 3.544990\n",
            "Step 150800 | Loss: 0.004848\n",
            "Episode 754 | Reward: -1.60 | Epsilon: 0.050\n",
            "Step 150900 | Loss: 0.003395\n",
            "Step 151000 | Loss: 0.005601\n",
            "Episode 755 | Reward: -1.34 | Epsilon: 0.050\n",
            "Step 151100 | Loss: 0.003321\n",
            "Step 151200 | Loss: 0.006611\n",
            "Episode 756 | Reward: -14.55 | Epsilon: 0.050\n",
            "Step 151300 | Loss: 0.022948\n",
            "Step 151400 | Loss: 0.002124\n",
            "Episode 757 | Reward: -6.25 | Epsilon: 0.050\n",
            "Step 151500 | Loss: 0.010532\n",
            "Step 151600 | Loss: 0.066453\n",
            "Episode 758 | Reward: -6.21 | Epsilon: 0.050\n",
            "Step 151700 | Loss: 0.003765\n",
            "Step 151800 | Loss: 0.001755\n",
            "Episode 759 | Reward: -14.85 | Epsilon: 0.050\n",
            "Step 151900 | Loss: 0.003046\n",
            "Step 152000 | Loss: 0.003342\n",
            "Episode 760 | Target network updated.\n",
            "Episode 760 | Reward: -34.46 | Epsilon: 0.050\n",
            "Step 152100 | Loss: 0.002631\n",
            "Step 152200 | Loss: 0.002854\n",
            "Episode 761 | Reward: -19.24 | Epsilon: 0.050\n",
            "Step 152300 | Loss: 0.002214\n",
            "Step 152400 | Loss: 0.001743\n",
            "Episode 762 | Reward: -12.73 | Epsilon: 0.050\n",
            "Step 152500 | Loss: 0.003228\n",
            "Step 152600 | Loss: 0.001651\n",
            "Episode 763 | Reward: -18.23 | Epsilon: 0.050\n",
            "Step 152700 | Loss: 0.003742\n",
            "Step 152800 | Loss: 0.001562\n",
            "Episode 764 | Reward: -10.33 | Epsilon: 0.050\n",
            "Step 152900 | Loss: 0.002113\n",
            "Step 153000 | Loss: 0.009170\n",
            "Episode 765 | Reward: -29.10 | Epsilon: 0.050\n",
            "Step 153100 | Loss: 0.004489\n",
            "Step 153200 | Loss: 0.007653\n",
            "Episode 766 | Reward: -34.45 | Epsilon: 0.050\n",
            "Step 153300 | Loss: 0.041070\n",
            "Step 153400 | Loss: 0.004625\n",
            "Episode 767 | Reward: -238.54 | Epsilon: 0.050\n",
            "Step 153500 | Loss: 0.002320\n",
            "Step 153600 | Loss: 0.002768\n",
            "Episode 768 | Reward: -13.23 | Epsilon: 0.050\n",
            "Step 153700 | Loss: 0.012597\n",
            "Step 153800 | Loss: 0.001952\n",
            "Episode 769 | Reward: -17.68 | Epsilon: 0.050\n",
            "Step 153900 | Loss: 0.015245\n",
            "Step 154000 | Loss: 0.003028\n",
            "Episode 770 | Reward: -3.56 | Epsilon: 0.050\n",
            "Step 154100 | Loss: 0.007310\n",
            "Step 154200 | Loss: 0.002498\n",
            "Episode 771 | Reward: -10.58 | Epsilon: 0.050\n",
            "Step 154300 | Loss: 0.004242\n",
            "Step 154400 | Loss: 0.005376\n",
            "Episode 772 | Reward: -19.51 | Epsilon: 0.050\n",
            "Step 154500 | Loss: 0.003984\n",
            "Step 154600 | Loss: 0.006761\n",
            "Episode 773 | Reward: -5.42 | Epsilon: 0.050\n",
            "Step 154700 | Loss: 0.003274\n",
            "Step 154800 | Loss: 0.001690\n",
            "Episode 774 | Reward: -0.87 | Epsilon: 0.050\n",
            "Step 154900 | Loss: 0.008334\n",
            "Step 155000 | Loss: 0.003715\n",
            "Episode 775 | Reward: -20.32 | Epsilon: 0.050\n",
            "Step 155100 | Loss: 6.740464\n",
            "Step 155200 | Loss: 0.015185\n",
            "Episode 776 | Reward: -336.94 | Epsilon: 0.050\n",
            "Step 155300 | Loss: 0.008056\n",
            "Step 155400 | Loss: 0.002705\n",
            "Episode 777 | Reward: -3.83 | Epsilon: 0.050\n",
            "Step 155500 | Loss: 0.001914\n",
            "Step 155600 | Loss: 0.003725\n",
            "Episode 778 | Reward: -80.61 | Epsilon: 0.050\n",
            "Step 155700 | Loss: 0.004791\n",
            "Step 155800 | Loss: 0.002539\n",
            "Episode 779 | Reward: -8.03 | Epsilon: 0.050\n",
            "Step 155900 | Loss: 0.004221\n",
            "Step 156000 | Loss: 0.006607\n",
            "Episode 780 | Target network updated.\n",
            "Episode 780 | Reward: -75.32 | Epsilon: 0.050\n",
            "Step 156100 | Loss: 0.029565\n",
            "Step 156200 | Loss: 0.002213\n",
            "Episode 781 | Reward: -10.90 | Epsilon: 0.050\n",
            "Step 156300 | Loss: 0.002687\n",
            "Step 156400 | Loss: 0.002442\n",
            "Episode 782 | Reward: -11.85 | Epsilon: 0.050\n",
            "Step 156500 | Loss: 0.008277\n",
            "Step 156600 | Loss: 0.001828\n",
            "Episode 783 | Reward: -4.84 | Epsilon: 0.050\n",
            "Step 156700 | Loss: 0.002638\n",
            "Step 156800 | Loss: 0.003754\n",
            "Episode 784 | Reward: -17.55 | Epsilon: 0.050\n",
            "Step 156900 | Loss: 0.003325\n",
            "Step 157000 | Loss: 0.006311\n",
            "Episode 785 | Reward: -3.61 | Epsilon: 0.050\n",
            "Step 157100 | Loss: 0.003462\n",
            "Step 157200 | Loss: 0.011596\n",
            "Episode 786 | Reward: -15.54 | Epsilon: 0.050\n",
            "Step 157300 | Loss: 0.029592\n",
            "Step 157400 | Loss: 0.001796\n",
            "Episode 787 | Reward: -5.46 | Epsilon: 0.050\n",
            "Step 157500 | Loss: 0.004680\n",
            "Step 157600 | Loss: 0.004817\n",
            "Episode 788 | Reward: -31.44 | Epsilon: 0.050\n",
            "Step 157700 | Loss: 0.023903\n",
            "Step 157800 | Loss: 0.003413\n",
            "Episode 789 | Reward: -16.32 | Epsilon: 0.050\n",
            "Step 157900 | Loss: 0.003089\n",
            "Step 158000 | Loss: 0.002414\n",
            "Episode 790 | Reward: -12.10 | Epsilon: 0.050\n",
            "Step 158100 | Loss: 0.012398\n",
            "Step 158200 | Loss: 0.002857\n",
            "Episode 791 | Reward: -20.76 | Epsilon: 0.050\n",
            "Step 158300 | Loss: 0.002717\n",
            "Step 158400 | Loss: 0.007473\n",
            "Episode 792 | Reward: -28.61 | Epsilon: 0.050\n",
            "Step 158500 | Loss: 0.005979\n",
            "Step 158600 | Loss: 0.003486\n",
            "Episode 793 | Reward: -1.08 | Epsilon: 0.050\n",
            "Step 158700 | Loss: 0.003714\n",
            "Step 158800 | Loss: 0.006133\n",
            "Episode 794 | Reward: -62.60 | Epsilon: 0.050\n",
            "Step 158900 | Loss: 0.001933\n",
            "Step 159000 | Loss: 0.004014\n",
            "Episode 795 | Reward: -37.94 | Epsilon: 0.050\n",
            "Step 159100 | Loss: 0.003676\n",
            "Step 159200 | Loss: 0.002402\n",
            "Episode 796 | Reward: -10.29 | Epsilon: 0.050\n",
            "Step 159300 | Loss: 0.009089\n",
            "Step 159400 | Loss: 0.003737\n",
            "Episode 797 | Reward: -9.88 | Epsilon: 0.050\n",
            "Step 159500 | Loss: 0.002622\n",
            "Step 159600 | Loss: 0.006633\n",
            "Episode 798 | Reward: -18.93 | Epsilon: 0.050\n",
            "Step 159700 | Loss: 0.003612\n",
            "Step 159800 | Loss: 0.024229\n",
            "Episode 799 | Reward: -16.60 | Epsilon: 0.050\n",
            "Step 159900 | Loss: 0.003746\n",
            "Step 160000 | Loss: 0.005176\n",
            "Episode 800 | Target network updated.\n",
            "model saved to models/2025-04-29_04-02-02/q_network_ep_0800.pth\n",
            "\n",
            "Episode 800 | Reward: -34.94 | Epsilon: 0.050\n",
            "Step 160100 | Loss: 0.005627\n",
            "Step 160200 | Loss: 0.005394\n",
            "Episode 801 | Reward: -2.49 | Epsilon: 0.050\n",
            "Step 160300 | Loss: 4.048710\n",
            "Step 160400 | Loss: 0.003150\n",
            "Episode 802 | Reward: -30.79 | Epsilon: 0.050\n",
            "Step 160500 | Loss: 0.007053\n",
            "Step 160600 | Loss: 0.006198\n",
            "Episode 803 | Reward: -18.88 | Epsilon: 0.050\n",
            "Step 160700 | Loss: 0.003670\n",
            "Step 160800 | Loss: 0.003125\n",
            "Episode 804 | Reward: -21.49 | Epsilon: 0.050\n",
            "Step 160900 | Loss: 0.003749\n",
            "Step 161000 | Loss: 0.002000\n",
            "Episode 805 | Reward: -11.80 | Epsilon: 0.050\n",
            "Step 161100 | Loss: 0.004077\n",
            "Step 161200 | Loss: 0.002416\n",
            "Episode 806 | Reward: -2.14 | Epsilon: 0.050\n",
            "Step 161300 | Loss: 0.009104\n",
            "Step 161400 | Loss: 0.009902\n",
            "Episode 807 | Reward: -7.20 | Epsilon: 0.050\n",
            "Step 161500 | Loss: 0.020438\n",
            "Step 161600 | Loss: 0.012957\n",
            "Episode 808 | Reward: -28.51 | Epsilon: 0.050\n",
            "Step 161700 | Loss: 0.003031\n",
            "Step 161800 | Loss: 0.011526\n",
            "Episode 809 | Reward: -95.92 | Epsilon: 0.050\n",
            "Step 161900 | Loss: 0.004187\n",
            "Step 162000 | Loss: 0.006148\n",
            "Episode 810 | Reward: -218.29 | Epsilon: 0.050\n",
            "Step 162100 | Loss: 0.013814\n",
            "Step 162200 | Loss: 3.099059\n",
            "Episode 811 | Reward: -17.92 | Epsilon: 0.050\n",
            "Step 162300 | Loss: 0.096165\n",
            "Step 162400 | Loss: 0.003577\n",
            "Episode 812 | Reward: -14.31 | Epsilon: 0.050\n",
            "Step 162500 | Loss: 0.003677\n",
            "Step 162600 | Loss: 0.003346\n",
            "Episode 813 | Reward: -18.67 | Epsilon: 0.050\n",
            "Step 162700 | Loss: 0.004142\n",
            "Step 162800 | Loss: 2.898543\n",
            "Episode 814 | Reward: -7.33 | Epsilon: 0.050\n",
            "Step 162900 | Loss: 0.122342\n",
            "Step 163000 | Loss: 4.004258\n",
            "Episode 815 | Reward: -11.30 | Epsilon: 0.050\n",
            "Step 163100 | Loss: 0.007758\n",
            "Step 163200 | Loss: 0.022638\n",
            "Episode 816 | Reward: -24.66 | Epsilon: 0.050\n",
            "Step 163300 | Loss: 0.015040\n",
            "Step 163400 | Loss: 0.006974\n",
            "Episode 817 | Reward: -50.77 | Epsilon: 0.050\n",
            "Step 163500 | Loss: 0.006247\n",
            "Step 163600 | Loss: 0.018332\n",
            "Episode 818 | Reward: -42.19 | Epsilon: 0.050\n",
            "Step 163700 | Loss: 0.003649\n",
            "Step 163800 | Loss: 0.005314\n",
            "Episode 819 | Reward: -30.08 | Epsilon: 0.050\n",
            "Step 163900 | Loss: 0.006730\n",
            "Step 164000 | Loss: 0.007083\n",
            "Episode 820 | Target network updated.\n",
            "Episode 820 | Reward: -20.58 | Epsilon: 0.050\n",
            "Step 164100 | Loss: 0.004054\n",
            "Step 164200 | Loss: 0.005445\n",
            "Episode 821 | Reward: -13.83 | Epsilon: 0.050\n",
            "Step 164300 | Loss: 0.003944\n",
            "Step 164400 | Loss: 0.010219\n",
            "Episode 822 | Reward: -7.88 | Epsilon: 0.050\n",
            "Step 164500 | Loss: 0.018892\n",
            "Step 164600 | Loss: 0.002183\n",
            "Episode 823 | Reward: -36.72 | Epsilon: 0.050\n",
            "Step 164700 | Loss: 0.006585\n",
            "Step 164800 | Loss: 0.003922\n",
            "Episode 824 | Reward: -30.19 | Epsilon: 0.050\n",
            "Step 164900 | Loss: 3.082211\n",
            "Step 165000 | Loss: 0.005827\n",
            "Episode 825 | Reward: -29.67 | Epsilon: 0.050\n",
            "Step 165100 | Loss: 0.003822\n",
            "Step 165200 | Loss: 0.004003\n",
            "Episode 826 | Reward: -15.15 | Epsilon: 0.050\n",
            "Step 165300 | Loss: 0.003654\n",
            "Step 165400 | Loss: 0.002696\n",
            "Episode 827 | Reward: -29.18 | Epsilon: 0.050\n",
            "Step 165500 | Loss: 0.016121\n",
            "Step 165600 | Loss: 0.014103\n",
            "Episode 828 | Reward: -6.29 | Epsilon: 0.050\n",
            "Step 165700 | Loss: 0.002477\n",
            "Step 165800 | Loss: 0.004468\n",
            "Episode 829 | Reward: -7.20 | Epsilon: 0.050\n",
            "Step 165900 | Loss: 0.014827\n",
            "Step 166000 | Loss: 0.005066\n",
            "Episode 830 | Reward: -23.58 | Epsilon: 0.050\n",
            "Step 166100 | Loss: 0.005871\n",
            "Step 166200 | Loss: 3.049591\n",
            "Episode 831 | Reward: -6.99 | Epsilon: 0.050\n",
            "Step 166300 | Loss: 0.093512\n",
            "Step 166400 | Loss: 0.003745\n",
            "Episode 832 | Reward: -5.18 | Epsilon: 0.050\n",
            "Step 166500 | Loss: 0.003788\n",
            "Step 166600 | Loss: 0.006539\n",
            "Episode 833 | Reward: -7.22 | Epsilon: 0.050\n",
            "Step 166700 | Loss: 0.005992\n",
            "Step 166800 | Loss: 0.004672\n",
            "Episode 834 | Reward: -14.59 | Epsilon: 0.050\n",
            "Step 166900 | Loss: 0.003454\n",
            "Step 167000 | Loss: 0.003467\n",
            "Episode 835 | Reward: -16.99 | Epsilon: 0.050\n",
            "Step 167100 | Loss: 0.003988\n",
            "Step 167200 | Loss: 0.002177\n",
            "Episode 836 | Reward: -12.24 | Epsilon: 0.050\n",
            "Step 167300 | Loss: 0.002472\n",
            "Step 167400 | Loss: 0.005464\n",
            "Episode 837 | Reward: -7.18 | Epsilon: 0.050\n",
            "Step 167500 | Loss: 0.004062\n",
            "Step 167600 | Loss: 0.006995\n",
            "Episode 838 | Reward: -18.56 | Epsilon: 0.050\n",
            "Step 167700 | Loss: 0.003622\n",
            "Step 167800 | Loss: 0.004164\n",
            "Episode 839 | Reward: -7.67 | Epsilon: 0.050\n",
            "Step 167900 | Loss: 0.003879\n",
            "Step 168000 | Loss: 0.006264\n",
            "Episode 840 | Target network updated.\n",
            "Episode 840 | Reward: -11.25 | Epsilon: 0.050\n",
            "Step 168100 | Loss: 3.250402\n",
            "Step 168200 | Loss: 0.003593\n",
            "Episode 841 | Reward: -36.62 | Epsilon: 0.050\n",
            "Step 168300 | Loss: 0.002373\n",
            "Step 168400 | Loss: 0.007729\n",
            "Episode 842 | Reward: -40.19 | Epsilon: 0.050\n",
            "Step 168500 | Loss: 0.004011\n",
            "Step 168600 | Loss: 0.003894\n",
            "Episode 843 | Reward: -6.72 | Epsilon: 0.050\n",
            "Step 168700 | Loss: 3.276079\n",
            "Step 168800 | Loss: 0.004011\n",
            "Episode 844 | Reward: -21.77 | Epsilon: 0.050\n",
            "Step 168900 | Loss: 0.002384\n",
            "Step 169000 | Loss: 0.002782\n",
            "Episode 845 | Reward: -3.03 | Epsilon: 0.050\n",
            "Step 169100 | Loss: 0.004913\n",
            "Step 169200 | Loss: 0.006208\n",
            "Episode 846 | Reward: -22.06 | Epsilon: 0.050\n",
            "Step 169300 | Loss: 0.003586\n",
            "Step 169400 | Loss: 0.001939\n",
            "Episode 847 | Reward: -22.07 | Epsilon: 0.050\n",
            "Step 169500 | Loss: 0.005575\n",
            "Step 169600 | Loss: 0.003199\n",
            "Episode 848 | Reward: -13.77 | Epsilon: 0.050\n",
            "Step 169700 | Loss: 0.002268\n",
            "Step 169800 | Loss: 0.032641\n",
            "Episode 849 | Reward: -16.42 | Epsilon: 0.050\n",
            "Step 169900 | Loss: 0.002695\n",
            "Step 170000 | Loss: 0.001530\n",
            "Episode 850 | Reward: -18.08 | Epsilon: 0.050\n",
            "Step 170100 | Loss: 0.003142\n",
            "Step 170200 | Loss: 0.016616\n",
            "Episode 851 | Reward: -9.45 | Epsilon: 0.050\n",
            "Step 170300 | Loss: 0.012082\n",
            "Step 170400 | Loss: 0.003455\n",
            "Episode 852 | Reward: -32.39 | Epsilon: 0.050\n",
            "Step 170500 | Loss: 0.003875\n",
            "Step 170600 | Loss: 0.004371\n",
            "Episode 853 | Reward: -18.47 | Epsilon: 0.050\n",
            "Step 170700 | Loss: 0.003806\n",
            "Step 170800 | Loss: 0.003600\n",
            "Episode 854 | Reward: -9.17 | Epsilon: 0.050\n",
            "Step 170900 | Loss: 0.001518\n",
            "Step 171000 | Loss: 0.001651\n",
            "Episode 855 | Reward: -37.72 | Epsilon: 0.050\n",
            "Step 171100 | Loss: 0.002751\n",
            "Step 171200 | Loss: 0.003404\n",
            "Episode 856 | Reward: -12.34 | Epsilon: 0.050\n",
            "Step 171300 | Loss: 0.004178\n",
            "Step 171400 | Loss: 0.002456\n",
            "Episode 857 | Reward: -27.23 | Epsilon: 0.050\n",
            "Step 171500 | Loss: 0.002192\n",
            "Step 171600 | Loss: 0.002063\n",
            "Episode 858 | Reward: -11.74 | Epsilon: 0.050\n",
            "Step 171700 | Loss: 0.009092\n",
            "Step 171800 | Loss: 0.004081\n",
            "Episode 859 | Reward: -25.69 | Epsilon: 0.050\n",
            "Step 171900 | Loss: 0.003737\n",
            "Step 172000 | Loss: 0.005706\n",
            "Episode 860 | Target network updated.\n",
            "Episode 860 | Reward: -28.32 | Epsilon: 0.050\n",
            "Step 172100 | Loss: 0.002802\n",
            "Step 172200 | Loss: 0.008612\n",
            "Episode 861 | Reward: -23.93 | Epsilon: 0.050\n",
            "Step 172300 | Loss: 0.002942\n",
            "Step 172400 | Loss: 0.003429\n",
            "Episode 862 | Reward: -11.25 | Epsilon: 0.050\n",
            "Step 172500 | Loss: 0.002485\n",
            "Step 172600 | Loss: 0.002854\n",
            "Episode 863 | Reward: -16.46 | Epsilon: 0.050\n",
            "Step 172700 | Loss: 0.003540\n",
            "Step 172800 | Loss: 0.248798\n",
            "Episode 864 | Reward: -12.70 | Epsilon: 0.050\n",
            "Step 172900 | Loss: 0.002869\n",
            "Step 173000 | Loss: 0.004871\n",
            "Episode 865 | Reward: -22.60 | Epsilon: 0.050\n",
            "Step 173100 | Loss: 0.002589\n",
            "Step 173200 | Loss: 0.045712\n",
            "Episode 866 | Reward: -7.43 | Epsilon: 0.050\n",
            "Step 173300 | Loss: 0.002807\n",
            "Step 173400 | Loss: 0.002426\n",
            "Episode 867 | Reward: -28.59 | Epsilon: 0.050\n",
            "Step 173500 | Loss: 0.002208\n",
            "Step 173600 | Loss: 0.011073\n",
            "Episode 868 | Reward: -22.21 | Epsilon: 0.050\n",
            "Step 173700 | Loss: 0.009805\n",
            "Step 173800 | Loss: 0.004568\n",
            "Episode 869 | Reward: -24.69 | Epsilon: 0.050\n",
            "Step 173900 | Loss: 0.004444\n",
            "Step 174000 | Loss: 0.250440\n",
            "Episode 870 | Reward: -26.56 | Epsilon: 0.050\n",
            "Step 174100 | Loss: 1.122424\n",
            "Step 174200 | Loss: 0.003108\n",
            "Episode 871 | Reward: -26.50 | Epsilon: 0.050\n",
            "Step 174300 | Loss: 0.004515\n",
            "Step 174400 | Loss: 0.001653\n",
            "Episode 872 | Reward: -19.30 | Epsilon: 0.050\n",
            "Step 174500 | Loss: 0.003566\n",
            "Step 174600 | Loss: 0.002807\n",
            "Episode 873 | Reward: -17.09 | Epsilon: 0.050\n",
            "Step 174700 | Loss: 0.002172\n",
            "Step 174800 | Loss: 0.726286\n",
            "Episode 874 | Reward: -41.66 | Epsilon: 0.050\n",
            "Step 174900 | Loss: 0.003644\n",
            "Step 175000 | Loss: 0.001757\n",
            "Episode 875 | Reward: -7.91 | Epsilon: 0.050\n",
            "Step 175100 | Loss: 0.009163\n",
            "Step 175200 | Loss: 0.002708\n",
            "Episode 876 | Reward: -4.53 | Epsilon: 0.050\n",
            "Step 175300 | Loss: 0.002362\n",
            "Step 175400 | Loss: 0.002494\n",
            "Episode 877 | Reward: -4.25 | Epsilon: 0.050\n",
            "Step 175500 | Loss: 0.006041\n",
            "Step 175600 | Loss: 0.003545\n",
            "Episode 878 | Reward: -6.96 | Epsilon: 0.050\n",
            "Step 175700 | Loss: 0.004837\n",
            "Step 175800 | Loss: 0.010223\n",
            "Episode 879 | Reward: -476.13 | Epsilon: 0.050\n",
            "Step 175900 | Loss: 0.009917\n",
            "Step 176000 | Loss: 0.027290\n",
            "Episode 880 | Target network updated.\n",
            "Episode 880 | Reward: -20.58 | Epsilon: 0.050\n",
            "Step 176100 | Loss: 0.027839\n",
            "Step 176200 | Loss: 0.003167\n",
            "Episode 881 | Reward: -28.50 | Epsilon: 0.050\n",
            "Step 176300 | Loss: 0.003670\n",
            "Step 176400 | Loss: 0.011688\n",
            "Episode 882 | Reward: -10.62 | Epsilon: 0.050\n",
            "Step 176500 | Loss: 0.014260\n",
            "Step 176600 | Loss: 0.003851\n",
            "Episode 883 | Reward: -33.84 | Epsilon: 0.050\n",
            "Step 176700 | Loss: 0.004895\n",
            "Step 176800 | Loss: 0.002215\n",
            "Episode 884 | Reward: -26.35 | Epsilon: 0.050\n",
            "Step 176900 | Loss: 0.023903\n",
            "Step 177000 | Loss: 0.003649\n",
            "Episode 885 | Reward: -6.29 | Epsilon: 0.050\n",
            "Step 177100 | Loss: 0.037622\n",
            "Step 177200 | Loss: 0.010086\n",
            "Episode 886 | Reward: -8.15 | Epsilon: 0.050\n",
            "Step 177300 | Loss: 0.005999\n",
            "Step 177400 | Loss: 0.008940\n",
            "Episode 887 | Reward: -22.47 | Epsilon: 0.050\n",
            "Step 177500 | Loss: 0.002802\n",
            "Step 177600 | Loss: 0.009874\n",
            "Episode 888 | Reward: -16.49 | Epsilon: 0.050\n",
            "Step 177700 | Loss: 0.016581\n",
            "Step 177800 | Loss: 0.415761\n",
            "Episode 889 | Reward: -4.26 | Epsilon: 0.050\n",
            "Step 177900 | Loss: 0.007878\n",
            "Step 178000 | Loss: 0.003353\n",
            "Episode 890 | Reward: -25.82 | Epsilon: 0.050\n",
            "Step 178100 | Loss: 0.011534\n",
            "Step 178200 | Loss: 0.003932\n",
            "Episode 891 | Reward: -4.18 | Epsilon: 0.050\n",
            "Step 178300 | Loss: 0.002480\n",
            "Step 178400 | Loss: 0.015857\n",
            "Episode 892 | Reward: -32.30 | Epsilon: 0.050\n",
            "Step 178500 | Loss: 0.003719\n",
            "Step 178600 | Loss: 0.002505\n",
            "Episode 893 | Reward: -4.76 | Epsilon: 0.050\n",
            "Step 178700 | Loss: 0.002682\n",
            "Step 178800 | Loss: 0.002781\n",
            "Episode 894 | Reward: -8.71 | Epsilon: 0.050\n",
            "Step 178900 | Loss: 0.172126\n",
            "Step 179000 | Loss: 0.004959\n",
            "Episode 895 | Reward: -16.71 | Epsilon: 0.050\n",
            "Step 179100 | Loss: 0.007945\n",
            "Step 179200 | Loss: 0.003408\n",
            "Episode 896 | Reward: -2.87 | Epsilon: 0.050\n",
            "Step 179300 | Loss: 0.014708\n",
            "Step 179400 | Loss: 0.003944\n",
            "Episode 897 | Reward: -23.45 | Epsilon: 0.050\n",
            "Step 179500 | Loss: 0.009821\n",
            "Step 179600 | Loss: 0.002996\n",
            "Episode 898 | Reward: -36.11 | Epsilon: 0.050\n",
            "Step 179700 | Loss: 0.004450\n",
            "Step 179800 | Loss: 0.002959\n",
            "Episode 899 | Reward: -4.36 | Epsilon: 0.050\n",
            "Step 179900 | Loss: 0.002173\n",
            "Step 180000 | Loss: 0.006997\n",
            "Episode 900 | Target network updated.\n",
            "model saved to models/2025-04-29_04-04-17/q_network_ep_0900.pth\n",
            "\n",
            "Episode 900 | Reward: -5.29 | Epsilon: 0.050\n",
            "Step 180100 | Loss: 0.006323\n",
            "Step 180200 | Loss: 0.002621\n",
            "Episode 901 | Reward: -4.72 | Epsilon: 0.050\n",
            "Step 180300 | Loss: 0.004710\n",
            "Step 180400 | Loss: 0.003945\n",
            "Episode 902 | Reward: -7.78 | Epsilon: 0.050\n",
            "Step 180500 | Loss: 0.010673\n",
            "Step 180600 | Loss: 0.002482\n",
            "Episode 903 | Reward: -12.81 | Epsilon: 0.050\n",
            "Step 180700 | Loss: 0.001720\n",
            "Step 180800 | Loss: 0.002139\n",
            "Episode 904 | Reward: -27.95 | Epsilon: 0.050\n",
            "Step 180900 | Loss: 0.005806\n",
            "Step 181000 | Loss: 0.005215\n",
            "Episode 905 | Reward: -31.31 | Epsilon: 0.050\n",
            "Step 181100 | Loss: 0.005579\n",
            "Step 181200 | Loss: 0.757795\n",
            "Episode 906 | Reward: -11.10 | Epsilon: 0.050\n",
            "Step 181300 | Loss: 0.002793\n",
            "Step 181400 | Loss: 0.008602\n",
            "Episode 907 | Reward: -112.50 | Epsilon: 0.050\n",
            "Step 181500 | Loss: 0.006446\n",
            "Step 181600 | Loss: 0.006031\n",
            "Episode 908 | Reward: -10.59 | Epsilon: 0.050\n",
            "Step 181700 | Loss: 0.005546\n",
            "Step 181800 | Loss: 0.001767\n",
            "Episode 909 | Reward: -37.67 | Epsilon: 0.050\n",
            "Step 181900 | Loss: 0.011731\n",
            "Step 182000 | Loss: 0.003202\n",
            "Episode 910 | Reward: -65.39 | Epsilon: 0.050\n",
            "Step 182100 | Loss: 0.003043\n",
            "Step 182200 | Loss: 0.004179\n",
            "Episode 911 | Reward: -4.31 | Epsilon: 0.050\n",
            "Step 182300 | Loss: 0.002842\n",
            "Step 182400 | Loss: 0.003550\n",
            "Episode 912 | Reward: -20.45 | Epsilon: 0.050\n",
            "Step 182500 | Loss: 0.004686\n",
            "Step 182600 | Loss: 0.004590\n",
            "Episode 913 | Reward: -46.06 | Epsilon: 0.050\n",
            "Step 182700 | Loss: 0.005726\n",
            "Step 182800 | Loss: 0.006417\n",
            "Episode 914 | Reward: -2.46 | Epsilon: 0.050\n",
            "Step 182900 | Loss: 0.151998\n",
            "Step 183000 | Loss: 0.005336\n",
            "Episode 915 | Reward: -19.43 | Epsilon: 0.050\n",
            "Step 183100 | Loss: 0.004844\n",
            "Step 183200 | Loss: 0.151382\n",
            "Episode 916 | Reward: -25.04 | Epsilon: 0.050\n",
            "Step 183300 | Loss: 0.003661\n",
            "Step 183400 | Loss: 0.046380\n",
            "Episode 917 | Reward: -29.10 | Epsilon: 0.050\n",
            "Step 183500 | Loss: 0.003125\n",
            "Step 183600 | Loss: 0.002211\n",
            "Episode 918 | Reward: -3.11 | Epsilon: 0.050\n",
            "Step 183700 | Loss: 0.005147\n",
            "Step 183800 | Loss: 0.002858\n",
            "Episode 919 | Reward: -11.89 | Epsilon: 0.050\n",
            "Step 183900 | Loss: 0.004957\n",
            "Step 184000 | Loss: 0.011083\n",
            "Episode 920 | Target network updated.\n",
            "Episode 920 | Reward: -15.66 | Epsilon: 0.050\n",
            "Step 184100 | Loss: 0.002817\n",
            "Step 184200 | Loss: 0.382261\n",
            "Episode 921 | Reward: -46.14 | Epsilon: 0.050\n",
            "Step 184300 | Loss: 0.004306\n",
            "Step 184400 | Loss: 0.003582\n",
            "Episode 922 | Reward: -13.65 | Epsilon: 0.050\n",
            "Step 184500 | Loss: 0.003833\n",
            "Step 184600 | Loss: 0.009983\n",
            "Episode 923 | Reward: -11.64 | Epsilon: 0.050\n",
            "Step 184700 | Loss: 0.003204\n",
            "Step 184800 | Loss: 0.001731\n",
            "Episode 924 | Reward: -14.56 | Epsilon: 0.050\n",
            "Step 184900 | Loss: 0.002650\n",
            "Step 185000 | Loss: 0.005518\n",
            "Episode 925 | Reward: -11.32 | Epsilon: 0.050\n",
            "Step 185100 | Loss: 0.002801\n",
            "Step 185200 | Loss: 0.009168\n",
            "Episode 926 | Reward: -7.24 | Epsilon: 0.050\n",
            "Step 185300 | Loss: 0.004692\n",
            "Step 185400 | Loss: 0.003117\n",
            "Episode 927 | Reward: -6.19 | Epsilon: 0.050\n",
            "Step 185500 | Loss: 0.003093\n",
            "Step 185600 | Loss: 0.004625\n",
            "Episode 928 | Reward: -11.29 | Epsilon: 0.050\n",
            "Step 185700 | Loss: 0.002005\n",
            "Step 185800 | Loss: 0.321796\n",
            "Episode 929 | Reward: -11.21 | Epsilon: 0.050\n",
            "Step 185900 | Loss: 0.001965\n",
            "Step 186000 | Loss: 0.003275\n",
            "Episode 930 | Reward: -15.41 | Epsilon: 0.050\n",
            "Step 186100 | Loss: 0.002824\n",
            "Step 186200 | Loss: 0.002564\n",
            "Episode 931 | Reward: -30.65 | Epsilon: 0.050\n",
            "Step 186300 | Loss: 0.010251\n",
            "Step 186400 | Loss: 0.347439\n",
            "Episode 932 | Reward: -5.82 | Epsilon: 0.050\n",
            "Step 186500 | Loss: 0.008165\n",
            "Step 186600 | Loss: 0.005088\n",
            "Episode 933 | Reward: -5.75 | Epsilon: 0.050\n",
            "Step 186700 | Loss: 0.009490\n",
            "Step 186800 | Loss: 0.003446\n",
            "Episode 934 | Reward: -25.79 | Epsilon: 0.050\n",
            "Step 186900 | Loss: 0.001856\n",
            "Step 187000 | Loss: 0.003275\n",
            "Episode 935 | Reward: -5.89 | Epsilon: 0.050\n",
            "Step 187100 | Loss: 0.330469\n",
            "Step 187200 | Loss: 0.006920\n",
            "Episode 936 | Reward: -0.97 | Epsilon: 0.050\n",
            "Step 187300 | Loss: 0.003987\n",
            "Step 187400 | Loss: 0.001531\n",
            "Episode 937 | Reward: -10.58 | Epsilon: 0.050\n",
            "Step 187500 | Loss: 0.001904\n",
            "Step 187600 | Loss: 1.700729\n",
            "Episode 938 | Reward: -4.10 | Epsilon: 0.050\n",
            "Step 187700 | Loss: 0.002936\n",
            "Step 187800 | Loss: 0.004320\n",
            "Episode 939 | Reward: -28.13 | Epsilon: 0.050\n",
            "Step 187900 | Loss: 0.007234\n",
            "Step 188000 | Loss: 0.002742\n",
            "Episode 940 | Target network updated.\n",
            "Episode 940 | Reward: -8.38 | Epsilon: 0.050\n",
            "Step 188100 | Loss: 0.006755\n",
            "Step 188200 | Loss: 0.002338\n",
            "Episode 941 | Reward: -3.52 | Epsilon: 0.050\n",
            "Step 188300 | Loss: 0.002955\n",
            "Step 188400 | Loss: 0.031840\n",
            "Episode 942 | Reward: -37.71 | Epsilon: 0.050\n",
            "Step 188500 | Loss: 0.009889\n",
            "Step 188600 | Loss: 0.002661\n",
            "Episode 943 | Reward: -26.18 | Epsilon: 0.050\n",
            "Step 188700 | Loss: 0.001390\n",
            "Step 188800 | Loss: 0.007181\n",
            "Episode 944 | Reward: -27.49 | Epsilon: 0.050\n",
            "Step 188900 | Loss: 0.002406\n",
            "Step 189000 | Loss: 0.004000\n",
            "Episode 945 | Reward: -16.76 | Epsilon: 0.050\n",
            "Step 189100 | Loss: 0.006486\n",
            "Step 189200 | Loss: 0.002339\n",
            "Episode 946 | Reward: -6.40 | Epsilon: 0.050\n",
            "Step 189300 | Loss: 0.002409\n",
            "Step 189400 | Loss: 0.001979\n",
            "Episode 947 | Reward: -61.39 | Epsilon: 0.050\n",
            "Step 189500 | Loss: 0.003158\n",
            "Step 189600 | Loss: 0.003279\n",
            "Episode 948 | Reward: -20.42 | Epsilon: 0.050\n",
            "Step 189700 | Loss: 0.003189\n",
            "Step 189800 | Loss: 0.002235\n",
            "Episode 949 | Reward: -11.32 | Epsilon: 0.050\n",
            "Step 189900 | Loss: 0.002388\n",
            "Step 190000 | Loss: 0.003397\n",
            "Episode 950 | Reward: -14.19 | Epsilon: 0.050\n",
            "Step 190100 | Loss: 0.002940\n",
            "Step 190200 | Loss: 0.003378\n",
            "Episode 951 | Reward: -7.96 | Epsilon: 0.050\n",
            "Step 190300 | Loss: 0.003263\n",
            "Step 190400 | Loss: 0.003280\n",
            "Episode 952 | Reward: -4.99 | Epsilon: 0.050\n",
            "Step 190500 | Loss: 0.004304\n",
            "Step 190600 | Loss: 0.002573\n",
            "Episode 953 | Reward: -14.17 | Epsilon: 0.050\n",
            "Step 190700 | Loss: 0.002643\n",
            "Step 190800 | Loss: 0.004378\n",
            "Episode 954 | Reward: -1.95 | Epsilon: 0.050\n",
            "Step 190900 | Loss: 0.001904\n",
            "Step 191000 | Loss: 0.001779\n",
            "Episode 955 | Reward: -4.43 | Epsilon: 0.050\n",
            "Step 191100 | Loss: 0.002575\n",
            "Step 191200 | Loss: 0.003252\n",
            "Episode 956 | Reward: -15.31 | Epsilon: 0.050\n",
            "Step 191300 | Loss: 0.003848\n",
            "Step 191400 | Loss: 0.006592\n",
            "Episode 957 | Reward: -16.64 | Epsilon: 0.050\n",
            "Step 191500 | Loss: 0.277656\n",
            "Step 191600 | Loss: 0.008103\n",
            "Episode 958 | Reward: -12.74 | Epsilon: 0.050\n",
            "Step 191700 | Loss: 0.002157\n",
            "Step 191800 | Loss: 0.001575\n",
            "Episode 959 | Reward: -15.16 | Epsilon: 0.050\n",
            "Step 191900 | Loss: 0.003763\n",
            "Step 192000 | Loss: 0.003160\n",
            "Episode 960 | Target network updated.\n",
            "Episode 960 | Reward: -20.12 | Epsilon: 0.050\n",
            "Step 192100 | Loss: 0.004866\n",
            "Step 192200 | Loss: 0.001712\n",
            "Episode 961 | Reward: -16.92 | Epsilon: 0.050\n",
            "Step 192300 | Loss: 0.003079\n",
            "Step 192400 | Loss: 0.006638\n",
            "Episode 962 | Reward: -45.70 | Epsilon: 0.050\n",
            "Step 192500 | Loss: 0.002607\n",
            "Step 192600 | Loss: 0.002522\n",
            "Episode 963 | Reward: -1.51 | Epsilon: 0.050\n",
            "Step 192700 | Loss: 0.003083\n",
            "Step 192800 | Loss: 0.002212\n",
            "Episode 964 | Reward: -23.53 | Epsilon: 0.050\n",
            "Step 192900 | Loss: 0.008020\n",
            "Step 193000 | Loss: 0.002245\n",
            "Episode 965 | Reward: -4.06 | Epsilon: 0.050\n",
            "Step 193100 | Loss: 0.002495\n",
            "Step 193200 | Loss: 0.002201\n",
            "Episode 966 | Reward: -22.55 | Epsilon: 0.050\n",
            "Step 193300 | Loss: 0.003685\n",
            "Step 193400 | Loss: 0.003455\n",
            "Episode 967 | Reward: -30.38 | Epsilon: 0.050\n",
            "Step 193500 | Loss: 0.005084\n",
            "Step 193600 | Loss: 0.002424\n",
            "Episode 968 | Reward: -21.68 | Epsilon: 0.050\n",
            "Step 193700 | Loss: 0.001802\n",
            "Step 193800 | Loss: 0.002636\n",
            "Episode 969 | Reward: -14.49 | Epsilon: 0.050\n",
            "Step 193900 | Loss: 0.001575\n",
            "Step 194000 | Loss: 0.002178\n",
            "Episode 970 | Reward: -45.25 | Epsilon: 0.050\n",
            "Step 194100 | Loss: 0.005423\n",
            "Step 194200 | Loss: 0.006808\n",
            "Episode 971 | Reward: -33.58 | Epsilon: 0.050\n",
            "Step 194300 | Loss: 0.003020\n",
            "Step 194400 | Loss: 0.039842\n",
            "Episode 972 | Reward: -16.86 | Epsilon: 0.050\n",
            "Step 194500 | Loss: 0.007686\n",
            "Step 194600 | Loss: 0.002581\n",
            "Episode 973 | Reward: -13.33 | Epsilon: 0.050\n",
            "Step 194700 | Loss: 0.005015\n",
            "Step 194800 | Loss: 0.016636\n",
            "Episode 974 | Reward: -106.40 | Epsilon: 0.050\n",
            "Step 194900 | Loss: 0.004177\n",
            "Step 195000 | Loss: 0.006474\n",
            "Episode 975 | Reward: -317.60 | Epsilon: 0.050\n",
            "Step 195100 | Loss: 0.005295\n",
            "Step 195200 | Loss: 0.004536\n",
            "Episode 976 | Reward: -5.81 | Epsilon: 0.050\n",
            "Step 195300 | Loss: 0.007172\n",
            "Step 195400 | Loss: 0.007824\n",
            "Episode 977 | Reward: -10.00 | Epsilon: 0.050\n",
            "Step 195500 | Loss: 0.005625\n",
            "Step 195600 | Loss: 0.004584\n",
            "Episode 978 | Reward: -28.21 | Epsilon: 0.050\n",
            "Step 195700 | Loss: 0.008312\n",
            "Step 195800 | Loss: 0.011194\n",
            "Episode 979 | Reward: -34.99 | Epsilon: 0.050\n",
            "Step 195900 | Loss: 0.003082\n",
            "Step 196000 | Loss: 0.004202\n",
            "Episode 980 | Target network updated.\n",
            "Episode 980 | Reward: -7.61 | Epsilon: 0.050\n",
            "Step 196100 | Loss: 0.030505\n",
            "Step 196200 | Loss: 31.660925\n",
            "Episode 981 | Reward: -72.58 | Epsilon: 0.050\n",
            "Step 196300 | Loss: 0.002833\n",
            "Step 196400 | Loss: 0.004877\n",
            "Episode 982 | Reward: -44.10 | Epsilon: 0.050\n",
            "Step 196500 | Loss: 0.022131\n",
            "Step 196600 | Loss: 0.004467\n",
            "Episode 983 | Reward: -2.61 | Epsilon: 0.050\n",
            "Step 196700 | Loss: 0.008339\n",
            "Step 196800 | Loss: 0.009985\n",
            "Episode 984 | Reward: -12.36 | Epsilon: 0.050\n",
            "Step 196900 | Loss: 0.051595\n",
            "Step 197000 | Loss: 0.004289\n",
            "Episode 985 | Reward: -11.18 | Epsilon: 0.050\n",
            "Step 197100 | Loss: 0.003381\n",
            "Step 197200 | Loss: 0.005144\n",
            "Episode 986 | Reward: -47.21 | Epsilon: 0.050\n",
            "Step 197300 | Loss: 0.005298\n",
            "Step 197400 | Loss: 0.341727\n",
            "Episode 987 | Reward: -25.66 | Epsilon: 0.050\n",
            "Step 197500 | Loss: 0.006724\n",
            "Step 197600 | Loss: 0.005294\n",
            "Episode 988 | Reward: -22.63 | Epsilon: 0.050\n",
            "Step 197700 | Loss: 0.010457\n",
            "Step 197800 | Loss: 0.013943\n",
            "Episode 989 | Reward: -14.49 | Epsilon: 0.050\n",
            "Step 197900 | Loss: 0.016260\n",
            "Step 198000 | Loss: 0.017962\n",
            "Episode 990 | Reward: -20.99 | Epsilon: 0.050\n",
            "Step 198100 | Loss: 0.076325\n",
            "Step 198200 | Loss: 0.003891\n",
            "Episode 991 | Reward: -7.64 | Epsilon: 0.050\n",
            "Step 198300 | Loss: 0.005280\n",
            "Step 198400 | Loss: 0.012071\n",
            "Episode 992 | Reward: -12.22 | Epsilon: 0.050\n",
            "Step 198500 | Loss: 0.009355\n",
            "Step 198600 | Loss: 0.006140\n",
            "Episode 993 | Reward: -5.88 | Epsilon: 0.050\n",
            "Step 198700 | Loss: 0.015293\n",
            "Step 198800 | Loss: 0.005399\n",
            "Episode 994 | Reward: -15.99 | Epsilon: 0.050\n",
            "Step 198900 | Loss: 0.011309\n",
            "Step 199000 | Loss: 0.023436\n",
            "Episode 995 | Reward: -44.71 | Epsilon: 0.050\n",
            "Step 199100 | Loss: 1.266844\n",
            "Step 199200 | Loss: 0.002568\n",
            "Episode 996 | Reward: -29.28 | Epsilon: 0.050\n",
            "Step 199300 | Loss: 0.002583\n",
            "Step 199400 | Loss: 0.007160\n",
            "Episode 997 | Reward: -14.31 | Epsilon: 0.050\n",
            "Step 199500 | Loss: 0.003446\n",
            "Step 199600 | Loss: 0.006858\n",
            "Episode 998 | Reward: -22.07 | Epsilon: 0.050\n",
            "Step 199700 | Loss: 0.004713\n",
            "Step 199800 | Loss: 0.005528\n",
            "Episode 999 | Reward: -2.53 | Epsilon: 0.050\n",
            "Step 199900 | Loss: 0.014473\n",
            "Step 200000 | Loss: 0.003923\n",
            "Episode 1000 | Target network updated.\n",
            "model saved to models/2025-04-29_04-06-27/q_network_ep_1000.pth\n",
            "\n",
            "Episode 1000 | Reward: -4.10 | Epsilon: 0.050\n",
            "Step 200100 | Loss: 37.423759\n",
            "Step 200200 | Loss: 0.011436\n",
            "Episode 1001 | Reward: -2.05 | Epsilon: 0.050\n",
            "Step 200300 | Loss: 0.004090\n",
            "Step 200400 | Loss: 0.010811\n",
            "Episode 1002 | Reward: -7.60 | Epsilon: 0.050\n",
            "Step 200500 | Loss: 0.004744\n",
            "Step 200600 | Loss: 0.005207\n",
            "Episode 1003 | Reward: -3.49 | Epsilon: 0.050\n",
            "Step 200700 | Loss: 0.014191\n",
            "Step 200800 | Loss: 0.004182\n",
            "Episode 1004 | Reward: -16.53 | Epsilon: 0.050\n",
            "Step 200900 | Loss: 0.003888\n",
            "Step 201000 | Loss: 0.003498\n",
            "Episode 1005 | Reward: -25.29 | Epsilon: 0.050\n",
            "Step 201100 | Loss: 0.004022\n",
            "Step 201200 | Loss: 0.015313\n",
            "Episode 1006 | Reward: -6.00 | Epsilon: 0.050\n",
            "Step 201300 | Loss: 0.007568\n",
            "Step 201400 | Loss: 0.296783\n",
            "Episode 1007 | Reward: -38.27 | Epsilon: 0.050\n",
            "Step 201500 | Loss: 0.007202\n",
            "Step 201600 | Loss: 0.004223\n",
            "Episode 1008 | Reward: -9.41 | Epsilon: 0.050\n",
            "Step 201700 | Loss: 0.151650\n",
            "Step 201800 | Loss: 0.005990\n",
            "Episode 1009 | Reward: -17.91 | Epsilon: 0.050\n",
            "Step 201900 | Loss: 0.005510\n",
            "Step 202000 | Loss: 0.044463\n",
            "Episode 1010 | Reward: -31.02 | Epsilon: 0.050\n",
            "Step 202100 | Loss: 0.004752\n",
            "Step 202200 | Loss: 0.006693\n",
            "Episode 1011 | Reward: -15.16 | Epsilon: 0.050\n",
            "Step 202300 | Loss: 0.003207\n",
            "Step 202400 | Loss: 0.003817\n",
            "Episode 1012 | Reward: -36.73 | Epsilon: 0.050\n",
            "Step 202500 | Loss: 0.003638\n",
            "Step 202600 | Loss: 0.003586\n",
            "Episode 1013 | Reward: -2.37 | Epsilon: 0.050\n",
            "Step 202700 | Loss: 0.004839\n",
            "Step 202800 | Loss: 0.002679\n",
            "Episode 1014 | Reward: -24.80 | Epsilon: 0.050\n",
            "Step 202900 | Loss: 0.002414\n",
            "Step 203000 | Loss: 0.003034\n",
            "Episode 1015 | Reward: -1.96 | Epsilon: 0.050\n",
            "Step 203100 | Loss: 38.165928\n",
            "Step 203200 | Loss: 0.005698\n",
            "Episode 1016 | Reward: -59.56 | Epsilon: 0.050\n",
            "Step 203300 | Loss: 0.006160\n",
            "Step 203400 | Loss: 0.003534\n",
            "Episode 1017 | Reward: -18.64 | Epsilon: 0.050\n",
            "Step 203500 | Loss: 0.004690\n",
            "Step 203600 | Loss: 0.006737\n",
            "Episode 1018 | Reward: -14.29 | Epsilon: 0.050\n",
            "Step 203700 | Loss: 0.006661\n",
            "Step 203800 | Loss: 0.009362\n",
            "Episode 1019 | Reward: -3.50 | Epsilon: 0.050\n",
            "Step 203900 | Loss: 0.007182\n",
            "Step 204000 | Loss: 0.004601\n",
            "Episode 1020 | Target network updated.\n",
            "Episode 1020 | Reward: -2.89 | Epsilon: 0.050\n",
            "Step 204100 | Loss: 0.005701\n",
            "Step 204200 | Loss: 0.011605\n",
            "Episode 1021 | Reward: -30.72 | Epsilon: 0.050\n",
            "Step 204300 | Loss: 0.005152\n",
            "Step 204400 | Loss: 0.005328\n",
            "Episode 1022 | Reward: -16.98 | Epsilon: 0.050\n",
            "Step 204500 | Loss: 0.002755\n",
            "Step 204600 | Loss: 0.003300\n",
            "Episode 1023 | Reward: -40.75 | Epsilon: 0.050\n",
            "Step 204700 | Loss: 0.005435\n",
            "Step 204800 | Loss: 0.005091\n",
            "Episode 1024 | Reward: -4.69 | Epsilon: 0.050\n",
            "Step 204900 | Loss: 0.005976\n",
            "Step 205000 | Loss: 0.007687\n",
            "Episode 1025 | Reward: -28.72 | Epsilon: 0.050\n",
            "Step 205100 | Loss: 0.151895\n",
            "Step 205200 | Loss: 0.004413\n",
            "Episode 1026 | Reward: -53.83 | Epsilon: 0.050\n",
            "Step 205300 | Loss: 0.005510\n",
            "Step 205400 | Loss: 0.003363\n",
            "Episode 1027 | Reward: -6.32 | Epsilon: 0.050\n",
            "Step 205500 | Loss: 0.004011\n",
            "Step 205600 | Loss: 0.020375\n",
            "Episode 1028 | Reward: -554.56 | Epsilon: 0.050\n",
            "Step 205700 | Loss: 0.004645\n",
            "Step 205800 | Loss: 0.003605\n",
            "Episode 1029 | Reward: -19.22 | Epsilon: 0.050\n",
            "Step 205900 | Loss: 0.011171\n",
            "Step 206000 | Loss: 0.017216\n",
            "Episode 1030 | Reward: -19.98 | Epsilon: 0.050\n",
            "Step 206100 | Loss: 0.009879\n",
            "Step 206200 | Loss: 0.004699\n",
            "Episode 1031 | Reward: -4.56 | Epsilon: 0.050\n",
            "Step 206300 | Loss: 0.192917\n",
            "Step 206400 | Loss: 0.003182\n",
            "Episode 1032 | Reward: -13.60 | Epsilon: 0.050\n",
            "Step 206500 | Loss: 21.003935\n",
            "Step 206600 | Loss: 0.006548\n",
            "Episode 1033 | Reward: -4.61 | Epsilon: 0.050\n",
            "Step 206700 | Loss: 0.581550\n",
            "Step 206800 | Loss: 0.003664\n",
            "Episode 1034 | Reward: -18.04 | Epsilon: 0.050\n",
            "Step 206900 | Loss: 0.183147\n",
            "Step 207000 | Loss: 0.014322\n",
            "Episode 1035 | Reward: -30.87 | Epsilon: 0.050\n",
            "Step 207100 | Loss: 0.002015\n",
            "Step 207200 | Loss: 0.004015\n",
            "Episode 1036 | Reward: -6.15 | Epsilon: 0.050\n",
            "Step 207300 | Loss: 0.001940\n",
            "Step 207400 | Loss: 0.004282\n",
            "Episode 1037 | Reward: -16.36 | Epsilon: 0.050\n",
            "Step 207500 | Loss: 0.014938\n",
            "Step 207600 | Loss: 0.014493\n",
            "Episode 1038 | Reward: -9.86 | Epsilon: 0.050\n",
            "Step 207700 | Loss: 21.488100\n",
            "Step 207800 | Loss: 0.101028\n",
            "Episode 1039 | Reward: -3.44 | Epsilon: 0.050\n",
            "Step 207900 | Loss: 0.007482\n",
            "Step 208000 | Loss: 0.002254\n",
            "Episode 1040 | Target network updated.\n",
            "Episode 1040 | Reward: -11.93 | Epsilon: 0.050\n",
            "Step 208100 | Loss: 0.438008\n",
            "Step 208200 | Loss: 0.011960\n",
            "Episode 1041 | Reward: -16.44 | Epsilon: 0.050\n",
            "Step 208300 | Loss: 0.007999\n",
            "Step 208400 | Loss: 25.389074\n",
            "Episode 1042 | Reward: -2.90 | Epsilon: 0.050\n",
            "Step 208500 | Loss: 0.003586\n",
            "Step 208600 | Loss: 0.002529\n",
            "Episode 1043 | Reward: -6.31 | Epsilon: 0.050\n",
            "Step 208700 | Loss: 0.007186\n",
            "Step 208800 | Loss: 0.005548\n",
            "Episode 1044 | Reward: -15.33 | Epsilon: 0.050\n",
            "Step 208900 | Loss: 0.003715\n",
            "Step 209000 | Loss: 0.002871\n",
            "Episode 1045 | Reward: -2.59 | Epsilon: 0.050\n",
            "Step 209100 | Loss: 0.004094\n",
            "Step 209200 | Loss: 0.002487\n",
            "Episode 1046 | Reward: -19.62 | Epsilon: 0.050\n",
            "Step 209300 | Loss: 0.003461\n",
            "Step 209400 | Loss: 0.005515\n",
            "Episode 1047 | Reward: -51.40 | Epsilon: 0.050\n",
            "Step 209500 | Loss: 0.007467\n",
            "Step 209600 | Loss: 0.004174\n",
            "Episode 1048 | Reward: -8.00 | Epsilon: 0.050\n",
            "Step 209700 | Loss: 0.007183\n",
            "Step 209800 | Loss: 0.004998\n",
            "Episode 1049 | Reward: -5.61 | Epsilon: 0.050\n",
            "Step 209900 | Loss: 0.009288\n",
            "Step 210000 | Loss: 0.003927\n",
            "Episode 1050 | Reward: -29.81 | Epsilon: 0.050\n",
            "Step 210100 | Loss: 0.005133\n",
            "Step 210200 | Loss: 0.004050\n",
            "Episode 1051 | Reward: -5.60 | Epsilon: 0.050\n",
            "Step 210300 | Loss: 0.002314\n",
            "Step 210400 | Loss: 0.190252\n",
            "Episode 1052 | Reward: -11.69 | Epsilon: 0.050\n",
            "Step 210500 | Loss: 0.426839\n",
            "Step 210600 | Loss: 0.007230\n",
            "Episode 1053 | Reward: -22.83 | Epsilon: 0.050\n",
            "Step 210700 | Loss: 0.004468\n",
            "Step 210800 | Loss: 0.004932\n",
            "Episode 1054 | Reward: -84.34 | Epsilon: 0.050\n",
            "Step 210900 | Loss: 0.002763\n",
            "Step 211000 | Loss: 0.036029\n",
            "Episode 1055 | Reward: -3.37 | Epsilon: 0.050\n",
            "Step 211100 | Loss: 0.007832\n",
            "Step 211200 | Loss: 0.014429\n",
            "Episode 1056 | Reward: -15.87 | Epsilon: 0.050\n",
            "Step 211300 | Loss: 0.004058\n",
            "Step 211400 | Loss: 0.405797\n",
            "Episode 1057 | Reward: -12.71 | Epsilon: 0.050\n",
            "Step 211500 | Loss: 0.003393\n",
            "Step 211600 | Loss: 0.004296\n",
            "Episode 1058 | Reward: -1.83 | Epsilon: 0.050\n",
            "Step 211700 | Loss: 0.004799\n",
            "Step 211800 | Loss: 0.004013\n",
            "Episode 1059 | Reward: -14.63 | Epsilon: 0.050\n",
            "Step 211900 | Loss: 0.007137\n",
            "Step 212000 | Loss: 0.003783\n",
            "Episode 1060 | Target network updated.\n",
            "Episode 1060 | Reward: -4.99 | Epsilon: 0.050\n",
            "Step 212100 | Loss: 0.002800\n",
            "Step 212200 | Loss: 0.002068\n",
            "Episode 1061 | Reward: -1.91 | Epsilon: 0.050\n",
            "Step 212300 | Loss: 0.004413\n",
            "Step 212400 | Loss: 0.003806\n",
            "Episode 1062 | Reward: -187.86 | Epsilon: 0.050\n",
            "Step 212500 | Loss: 0.004765\n",
            "Step 212600 | Loss: 0.005870\n",
            "Episode 1063 | Reward: -1.19 | Epsilon: 0.050\n",
            "Step 212700 | Loss: 0.057996\n",
            "Step 212800 | Loss: 0.005728\n",
            "Episode 1064 | Reward: -52.23 | Epsilon: 0.050\n",
            "Step 212900 | Loss: 0.012352\n",
            "Step 213000 | Loss: 0.002896\n",
            "Episode 1065 | Reward: -2.64 | Epsilon: 0.050\n",
            "Step 213100 | Loss: 0.010756\n",
            "Step 213200 | Loss: 0.006676\n",
            "Episode 1066 | Reward: -22.03 | Epsilon: 0.050\n",
            "Step 213300 | Loss: 0.005710\n",
            "Step 213400 | Loss: 0.025156\n",
            "Episode 1067 | Reward: -38.66 | Epsilon: 0.050\n",
            "Step 213500 | Loss: 0.025390\n",
            "Step 213600 | Loss: 0.003084\n",
            "Episode 1068 | Reward: -24.03 | Epsilon: 0.050\n",
            "Step 213700 | Loss: 0.720599\n",
            "Step 213800 | Loss: 0.004651\n",
            "Episode 1069 | Reward: -26.76 | Epsilon: 0.050\n",
            "Step 213900 | Loss: 0.005519\n",
            "Step 214000 | Loss: 0.013542\n",
            "Episode 1070 | Reward: -6.30 | Epsilon: 0.050\n",
            "Step 214100 | Loss: 0.003062\n",
            "Step 214200 | Loss: 0.004549\n",
            "Episode 1071 | Reward: -1.96 | Epsilon: 0.050\n",
            "Step 214300 | Loss: 0.002477\n",
            "Step 214400 | Loss: 0.003558\n",
            "Episode 1072 | Reward: -16.10 | Epsilon: 0.050\n",
            "Step 214500 | Loss: 0.003960\n",
            "Step 214600 | Loss: 0.521257\n",
            "Episode 1073 | Reward: -4.13 | Epsilon: 0.050\n",
            "Step 214700 | Loss: 0.002807\n",
            "Step 214800 | Loss: 0.003671\n",
            "Episode 1074 | Reward: -14.68 | Epsilon: 0.050\n",
            "Step 214900 | Loss: 0.005048\n",
            "Step 215000 | Loss: 0.004496\n",
            "Episode 1075 | Reward: -2.16 | Epsilon: 0.050\n",
            "Step 215100 | Loss: 0.308154\n",
            "Step 215200 | Loss: 0.003430\n",
            "Episode 1076 | Reward: -37.30 | Epsilon: 0.050\n",
            "Step 215300 | Loss: 0.003543\n",
            "Step 215400 | Loss: 0.002851\n",
            "Episode 1077 | Reward: -29.78 | Epsilon: 0.050\n",
            "Step 215500 | Loss: 0.006358\n",
            "Step 215600 | Loss: 0.003051\n",
            "Episode 1078 | Reward: -41.89 | Epsilon: 0.050\n",
            "Step 215700 | Loss: 0.004432\n",
            "Step 215800 | Loss: 0.005358\n",
            "Episode 1079 | Reward: -4.51 | Epsilon: 0.050\n",
            "Step 215900 | Loss: 0.005431\n",
            "Step 216000 | Loss: 14.709164\n",
            "Episode 1080 | Target network updated.\n",
            "Episode 1080 | Reward: -12.01 | Epsilon: 0.050\n",
            "Step 216100 | Loss: 0.003672\n",
            "Step 216200 | Loss: 0.018446\n",
            "Episode 1081 | Reward: -11.46 | Epsilon: 0.050\n",
            "Step 216300 | Loss: 0.004851\n",
            "Step 216400 | Loss: 0.003832\n",
            "Episode 1082 | Reward: -10.22 | Epsilon: 0.050\n",
            "Step 216500 | Loss: 0.002576\n",
            "Step 216600 | Loss: 0.001687\n",
            "Episode 1083 | Reward: -14.22 | Epsilon: 0.050\n",
            "Step 216700 | Loss: 0.022337\n",
            "Step 216800 | Loss: 0.010540\n",
            "Episode 1084 | Reward: -4.38 | Epsilon: 0.050\n",
            "Step 216900 | Loss: 0.013454\n",
            "Step 217000 | Loss: 0.003664\n",
            "Episode 1085 | Reward: -2.73 | Epsilon: 0.050\n",
            "Step 217100 | Loss: 0.002583\n",
            "Step 217200 | Loss: 0.002289\n",
            "Episode 1086 | Reward: -16.91 | Epsilon: 0.050\n",
            "Step 217300 | Loss: 0.002779\n",
            "Step 217400 | Loss: 0.003402\n",
            "Episode 1087 | Reward: -51.31 | Epsilon: 0.050\n",
            "Step 217500 | Loss: 0.003132\n",
            "Step 217600 | Loss: 0.003971\n",
            "Episode 1088 | Reward: -30.78 | Epsilon: 0.050\n",
            "Step 217700 | Loss: 0.080340\n",
            "Step 217800 | Loss: 0.003803\n",
            "Episode 1089 | Reward: -4.63 | Epsilon: 0.050\n",
            "Step 217900 | Loss: 0.002329\n",
            "Step 218000 | Loss: 0.003097\n",
            "Episode 1090 | Reward: -15.22 | Epsilon: 0.050\n",
            "Step 218100 | Loss: 0.002769\n",
            "Step 218200 | Loss: 0.002385\n",
            "Episode 1091 | Reward: -161.59 | Epsilon: 0.050\n",
            "Step 218300 | Loss: 0.003506\n",
            "Step 218400 | Loss: 0.003041\n",
            "Episode 1092 | Reward: -4.44 | Epsilon: 0.050\n",
            "Step 218500 | Loss: 0.011151\n",
            "Step 218600 | Loss: 0.004063\n",
            "Episode 1093 | Reward: -3.90 | Epsilon: 0.050\n",
            "Step 218700 | Loss: 0.002894\n",
            "Step 218800 | Loss: 0.002681\n",
            "Episode 1094 | Reward: -53.45 | Epsilon: 0.050\n",
            "Step 218900 | Loss: 0.019562\n",
            "Step 219000 | Loss: 0.005002\n",
            "Episode 1095 | Reward: -8.88 | Epsilon: 0.050\n",
            "Step 219100 | Loss: 0.005216\n",
            "Step 219200 | Loss: 0.001713\n",
            "Episode 1096 | Reward: -1.19 | Epsilon: 0.050\n",
            "Step 219300 | Loss: 0.004121\n",
            "Step 219400 | Loss: 0.004288\n",
            "Episode 1097 | Reward: -40.61 | Epsilon: 0.050\n",
            "Step 219500 | Loss: 0.002784\n",
            "Step 219600 | Loss: 0.007823\n",
            "Episode 1098 | Reward: -2.37 | Epsilon: 0.050\n",
            "Step 219700 | Loss: 0.041352\n",
            "Step 219800 | Loss: 0.004186\n",
            "Episode 1099 | Reward: -22.13 | Epsilon: 0.050\n",
            "Step 219900 | Loss: 0.303544\n",
            "Step 220000 | Loss: 0.005645\n",
            "Episode 1100 | Target network updated.\n",
            "model saved to models/2025-04-29_04-08-37/q_network_ep_1100.pth\n",
            "\n",
            "Episode 1100 | Reward: -23.68 | Epsilon: 0.050\n",
            "Step 220100 | Loss: 0.006757\n",
            "Step 220200 | Loss: 0.003617\n",
            "Episode 1101 | Reward: -29.05 | Epsilon: 0.050\n",
            "Step 220300 | Loss: 0.006850\n",
            "Step 220400 | Loss: 0.003606\n",
            "Episode 1102 | Reward: -15.87 | Epsilon: 0.050\n",
            "Step 220500 | Loss: 0.003650\n",
            "Step 220600 | Loss: 0.004258\n",
            "Episode 1103 | Reward: -3.18 | Epsilon: 0.050\n",
            "Step 220700 | Loss: 0.003026\n",
            "Step 220800 | Loss: 0.003860\n",
            "Episode 1104 | Reward: -7.42 | Epsilon: 0.050\n",
            "Step 220900 | Loss: 0.004420\n",
            "Step 221000 | Loss: 0.192107\n",
            "Episode 1105 | Reward: -1.18 | Epsilon: 0.050\n",
            "Step 221100 | Loss: 0.017497\n",
            "Step 221200 | Loss: 0.004014\n",
            "Episode 1106 | Reward: -172.57 | Epsilon: 0.050\n",
            "Step 221300 | Loss: 0.007268\n",
            "Step 221400 | Loss: 0.004049\n",
            "Episode 1107 | Reward: -0.42 | Epsilon: 0.050\n",
            "Step 221500 | Loss: 0.004131\n",
            "Step 221600 | Loss: 0.004943\n",
            "Episode 1108 | Reward: -131.01 | Epsilon: 0.050\n",
            "Step 221700 | Loss: 0.003897\n",
            "Step 221800 | Loss: 0.004746\n",
            "Episode 1109 | Reward: -7.04 | Epsilon: 0.050\n",
            "Step 221900 | Loss: 0.005214\n",
            "Step 222000 | Loss: 0.051558\n",
            "Episode 1110 | Reward: -31.05 | Epsilon: 0.050\n",
            "Step 222100 | Loss: 0.003509\n",
            "Step 222200 | Loss: 0.004655\n",
            "Episode 1111 | Reward: -2.93 | Epsilon: 0.050\n",
            "Step 222300 | Loss: 0.004293\n",
            "Step 222400 | Loss: 0.009894\n",
            "Episode 1112 | Reward: -43.44 | Epsilon: 0.050\n",
            "Step 222500 | Loss: 0.001955\n",
            "Step 222600 | Loss: 0.020686\n",
            "Episode 1113 | Reward: -65.95 | Epsilon: 0.050\n",
            "Step 222700 | Loss: 0.014905\n",
            "Step 222800 | Loss: 0.007632\n",
            "Episode 1114 | Reward: -47.17 | Epsilon: 0.050\n",
            "Step 222900 | Loss: 0.007344\n",
            "Step 223000 | Loss: 0.006588\n",
            "Episode 1115 | Reward: -19.57 | Epsilon: 0.050\n",
            "Step 223100 | Loss: 0.011872\n",
            "Step 223200 | Loss: 0.006280\n",
            "Episode 1116 | Reward: -4.55 | Epsilon: 0.050\n",
            "Step 223300 | Loss: 12.076342\n",
            "Step 223400 | Loss: 0.004202\n",
            "Episode 1117 | Reward: -36.98 | Epsilon: 0.050\n",
            "Step 223500 | Loss: 0.023060\n",
            "Step 223600 | Loss: 0.281402\n",
            "Episode 1118 | Reward: -122.90 | Epsilon: 0.050\n",
            "Step 223700 | Loss: 0.007847\n",
            "Step 223800 | Loss: 0.005027\n",
            "Episode 1119 | Reward: -6.78 | Epsilon: 0.050\n",
            "Step 223900 | Loss: 0.004005\n",
            "Step 224000 | Loss: 0.006362\n",
            "Episode 1120 | Target network updated.\n",
            "Episode 1120 | Reward: -46.90 | Epsilon: 0.050\n",
            "Step 224100 | Loss: 0.009563\n",
            "Step 224200 | Loss: 0.005005\n",
            "Episode 1121 | Reward: -6.38 | Epsilon: 0.050\n",
            "Step 224300 | Loss: 0.007208\n",
            "Step 224400 | Loss: 0.007786\n",
            "Episode 1122 | Reward: -165.35 | Epsilon: 0.050\n",
            "Step 224500 | Loss: 0.003410\n",
            "Step 224600 | Loss: 0.008171\n",
            "Episode 1123 | Reward: -9.78 | Epsilon: 0.050\n",
            "Step 224700 | Loss: 0.005225\n",
            "Step 224800 | Loss: 0.007238\n",
            "Episode 1124 | Reward: -50.70 | Epsilon: 0.050\n",
            "Step 224900 | Loss: 0.003333\n",
            "Step 225000 | Loss: 0.002921\n",
            "Episode 1125 | Reward: -10.47 | Epsilon: 0.050\n",
            "Step 225100 | Loss: 0.010885\n",
            "Step 225200 | Loss: 0.360339\n",
            "Episode 1126 | Reward: -7.03 | Epsilon: 0.050\n",
            "Step 225300 | Loss: 0.004148\n",
            "Step 225400 | Loss: 0.006135\n",
            "Episode 1127 | Reward: -90.40 | Epsilon: 0.050\n",
            "Step 225500 | Loss: 0.007372\n",
            "Step 225600 | Loss: 0.005888\n",
            "Episode 1128 | Reward: -30.83 | Epsilon: 0.050\n",
            "Step 225700 | Loss: 0.005046\n",
            "Step 225800 | Loss: 0.006070\n",
            "Episode 1129 | Reward: -25.75 | Epsilon: 0.050\n",
            "Step 225900 | Loss: 0.008946\n",
            "Step 226000 | Loss: 0.003904\n",
            "Episode 1130 | Reward: -2.98 | Epsilon: 0.050\n",
            "Step 226100 | Loss: 0.005210\n",
            "Step 226200 | Loss: 0.006168\n",
            "Episode 1131 | Reward: -7.55 | Epsilon: 0.050\n",
            "Step 226300 | Loss: 0.005381\n",
            "Step 226400 | Loss: 0.002545\n",
            "Episode 1132 | Reward: -3.28 | Epsilon: 0.050\n",
            "Step 226500 | Loss: 0.003274\n",
            "Step 226600 | Loss: 0.010458\n",
            "Episode 1133 | Reward: -80.99 | Epsilon: 0.050\n",
            "Step 226700 | Loss: 0.002383\n",
            "Step 226800 | Loss: 0.006377\n",
            "Episode 1134 | Reward: -92.84 | Epsilon: 0.050\n",
            "Step 226900 | Loss: 0.008732\n",
            "Step 227000 | Loss: 0.003883\n",
            "Episode 1135 | Reward: -28.87 | Epsilon: 0.050\n",
            "Step 227100 | Loss: 0.022895\n",
            "Step 227200 | Loss: 0.008576\n",
            "Episode 1136 | Reward: -8.17 | Epsilon: 0.050\n",
            "Step 227300 | Loss: 0.447957\n",
            "Step 227400 | Loss: 0.005741\n",
            "Episode 1137 | Reward: -44.98 | Epsilon: 0.050\n",
            "Step 227500 | Loss: 0.004258\n",
            "Step 227600 | Loss: 13.084375\n",
            "Episode 1138 | Reward: -9.04 | Epsilon: 0.050\n",
            "Step 227700 | Loss: 0.010007\n",
            "Step 227800 | Loss: 0.006519\n",
            "Episode 1139 | Reward: -57.18 | Epsilon: 0.050\n",
            "Step 227900 | Loss: 0.005287\n",
            "Step 228000 | Loss: 0.021363\n",
            "Episode 1140 | Target network updated.\n",
            "Episode 1140 | Reward: -48.53 | Epsilon: 0.050\n",
            "Step 228100 | Loss: 7.160036\n",
            "Step 228200 | Loss: 0.008856\n",
            "Episode 1141 | Reward: -8.40 | Epsilon: 0.050\n",
            "Step 228300 | Loss: 0.007794\n",
            "Step 228400 | Loss: 0.006990\n",
            "Episode 1142 | Reward: -3.41 | Epsilon: 0.050\n",
            "Step 228500 | Loss: 0.179023\n",
            "Step 228600 | Loss: 0.009235\n",
            "Episode 1143 | Reward: -11.60 | Epsilon: 0.050\n",
            "Step 228700 | Loss: 0.004422\n",
            "Step 228800 | Loss: 0.005117\n",
            "Episode 1144 | Reward: -21.51 | Epsilon: 0.050\n",
            "Step 228900 | Loss: 0.006767\n",
            "Step 229000 | Loss: 0.109568\n",
            "Episode 1145 | Reward: -38.81 | Epsilon: 0.050\n",
            "Step 229100 | Loss: 0.008054\n",
            "Step 229200 | Loss: 0.006346\n",
            "Episode 1146 | Reward: -115.21 | Epsilon: 0.050\n",
            "Step 229300 | Loss: 0.011073\n",
            "Step 229400 | Loss: 0.006758\n",
            "Episode 1147 | Reward: -17.87 | Epsilon: 0.050\n",
            "Step 229500 | Loss: 0.006889\n",
            "Step 229600 | Loss: 0.034505\n",
            "Episode 1148 | Reward: -31.51 | Epsilon: 0.050\n",
            "Step 229700 | Loss: 0.006564\n",
            "Step 229800 | Loss: 0.017403\n",
            "Episode 1149 | Reward: -4.57 | Epsilon: 0.050\n",
            "Step 229900 | Loss: 0.006061\n",
            "Step 230000 | Loss: 0.007530\n",
            "Episode 1150 | Reward: -1.94 | Epsilon: 0.050\n",
            "Step 230100 | Loss: 0.008354\n",
            "Step 230200 | Loss: 0.004943\n",
            "Episode 1151 | Reward: -17.62 | Epsilon: 0.050\n",
            "Step 230300 | Loss: 0.003254\n",
            "Step 230400 | Loss: 0.003927\n",
            "Episode 1152 | Reward: -2.93 | Epsilon: 0.050\n",
            "Step 230500 | Loss: 0.008368\n",
            "Step 230600 | Loss: 0.003887\n",
            "Episode 1153 | Reward: -11.43 | Epsilon: 0.050\n",
            "Step 230700 | Loss: 0.018668\n",
            "Step 230800 | Loss: 0.004419\n",
            "Episode 1154 | Reward: -1.94 | Epsilon: 0.050\n",
            "Step 230900 | Loss: 0.007355\n",
            "Step 231000 | Loss: 0.005486\n",
            "Episode 1155 | Reward: -0.52 | Epsilon: 0.050\n",
            "Step 231100 | Loss: 0.023462\n",
            "Step 231200 | Loss: 0.006378\n",
            "Episode 1156 | Reward: -18.70 | Epsilon: 0.050\n",
            "Step 231300 | Loss: 0.005036\n",
            "Step 231400 | Loss: 0.005379\n",
            "Episode 1157 | Reward: -70.26 | Epsilon: 0.050\n",
            "Step 231500 | Loss: 0.006452\n",
            "Step 231600 | Loss: 0.015841\n",
            "Episode 1158 | Reward: -30.75 | Epsilon: 0.050\n",
            "Step 231700 | Loss: 0.003650\n",
            "Step 231800 | Loss: 7.094170\n",
            "Episode 1159 | Reward: -2.58 | Epsilon: 0.050\n",
            "Step 231900 | Loss: 0.005510\n",
            "Step 232000 | Loss: 0.003045\n",
            "Episode 1160 | Target network updated.\n",
            "Episode 1160 | Reward: -6.88 | Epsilon: 0.050\n",
            "Step 232100 | Loss: 0.011053\n",
            "Step 232200 | Loss: 0.005902\n",
            "Episode 1161 | Reward: -90.51 | Epsilon: 0.050\n",
            "Step 232300 | Loss: 0.182865\n",
            "Step 232400 | Loss: 0.008377\n",
            "Episode 1162 | Reward: -16.74 | Epsilon: 0.050\n",
            "Step 232500 | Loss: 0.024722\n",
            "Step 232600 | Loss: 18.661018\n",
            "Episode 1163 | Reward: -3.58 | Epsilon: 0.050\n",
            "Step 232700 | Loss: 0.009825\n",
            "Step 232800 | Loss: 0.007154\n",
            "Episode 1164 | Reward: -17.86 | Epsilon: 0.050\n",
            "Step 232900 | Loss: 0.006178\n",
            "Step 233000 | Loss: 0.003581\n",
            "Episode 1165 | Reward: -10.20 | Epsilon: 0.050\n",
            "Step 233100 | Loss: 0.004404\n",
            "Step 233200 | Loss: 0.007101\n",
            "Episode 1166 | Reward: -7.83 | Epsilon: 0.050\n",
            "Step 233300 | Loss: 0.004884\n",
            "Step 233400 | Loss: 0.004047\n",
            "Episode 1167 | Reward: -10.04 | Epsilon: 0.050\n",
            "Step 233500 | Loss: 3.758891\n",
            "Step 233600 | Loss: 0.005876\n",
            "Episode 1168 | Reward: -3.96 | Epsilon: 0.050\n",
            "Step 233700 | Loss: 0.008095\n",
            "Step 233800 | Loss: 0.004032\n",
            "Episode 1169 | Reward: -57.11 | Epsilon: 0.050\n",
            "Step 233900 | Loss: 0.003946\n",
            "Step 234000 | Loss: 0.002296\n",
            "Episode 1170 | Reward: -31.24 | Epsilon: 0.050\n",
            "Step 234100 | Loss: 0.002890\n",
            "Step 234200 | Loss: 0.004588\n",
            "Episode 1171 | Reward: -13.59 | Epsilon: 0.050\n",
            "Step 234300 | Loss: 0.007840\n",
            "Step 234400 | Loss: 0.002065\n",
            "Episode 1172 | Reward: -5.81 | Epsilon: 0.050\n",
            "Step 234500 | Loss: 0.002784\n",
            "Step 234600 | Loss: 0.016673\n",
            "Episode 1173 | Reward: -27.85 | Epsilon: 0.050\n",
            "Step 234700 | Loss: 0.004140\n",
            "Step 234800 | Loss: 0.003873\n",
            "Episode 1174 | Reward: -9.17 | Epsilon: 0.050\n",
            "Step 234900 | Loss: 0.104340\n",
            "Step 235000 | Loss: 0.009649\n",
            "Episode 1175 | Reward: -318.56 | Epsilon: 0.050\n",
            "Step 235100 | Loss: 0.005280\n",
            "Step 235200 | Loss: 0.013434\n",
            "Episode 1176 | Reward: -11.01 | Epsilon: 0.050\n",
            "Step 235300 | Loss: 0.009713\n",
            "Step 235400 | Loss: 0.003214\n",
            "Episode 1177 | Reward: -8.01 | Epsilon: 0.050\n",
            "Step 235500 | Loss: 0.002727\n",
            "Step 235600 | Loss: 18.679346\n",
            "Episode 1178 | Reward: -3.74 | Epsilon: 0.050\n",
            "Step 235700 | Loss: 0.003068\n",
            "Step 235800 | Loss: 0.002388\n",
            "Episode 1179 | Reward: -34.78 | Epsilon: 0.050\n",
            "Step 235900 | Loss: 0.005849\n",
            "Step 236000 | Loss: 0.004127\n",
            "Episode 1180 | Target network updated.\n",
            "Episode 1180 | Reward: -7.51 | Epsilon: 0.050\n",
            "Step 236100 | Loss: 0.006154\n",
            "Step 236200 | Loss: 0.003545\n",
            "Episode 1181 | Reward: -19.03 | Epsilon: 0.050\n",
            "Step 236300 | Loss: 0.004233\n",
            "Step 236400 | Loss: 0.008566\n",
            "Episode 1182 | Reward: -5.49 | Epsilon: 0.050\n",
            "Step 236500 | Loss: 0.002328\n",
            "Step 236600 | Loss: 0.004606\n",
            "Episode 1183 | Reward: -6.52 | Epsilon: 0.050\n",
            "Step 236700 | Loss: 0.007195\n",
            "Step 236800 | Loss: 0.009057\n",
            "Episode 1184 | Reward: -33.59 | Epsilon: 0.050\n",
            "Step 236900 | Loss: 0.002862\n",
            "Step 237000 | Loss: 0.004629\n",
            "Episode 1185 | Reward: -6.76 | Epsilon: 0.050\n",
            "Step 237100 | Loss: 0.003197\n",
            "Step 237200 | Loss: 0.005335\n",
            "Episode 1186 | Reward: -40.25 | Epsilon: 0.050\n",
            "Step 237300 | Loss: 0.004707\n",
            "Step 237400 | Loss: 0.015382\n",
            "Episode 1187 | Reward: -22.69 | Epsilon: 0.050\n",
            "Step 237500 | Loss: 0.004459\n",
            "Step 237600 | Loss: 0.002529\n",
            "Episode 1188 | Reward: -5.11 | Epsilon: 0.050\n",
            "Step 237700 | Loss: 17.194017\n",
            "Step 237800 | Loss: 0.003547\n",
            "Episode 1189 | Reward: -12.92 | Epsilon: 0.050\n",
            "Step 237900 | Loss: 0.001653\n",
            "Step 238000 | Loss: 19.236626\n",
            "Episode 1190 | Reward: -15.00 | Epsilon: 0.050\n",
            "Step 238100 | Loss: 0.003859\n",
            "Step 238200 | Loss: 0.003321\n",
            "Episode 1191 | Reward: -21.78 | Epsilon: 0.050\n",
            "Step 238300 | Loss: 0.003863\n",
            "Step 238400 | Loss: 0.003590\n",
            "Episode 1192 | Reward: -9.30 | Epsilon: 0.050\n",
            "Step 238500 | Loss: 0.001966\n",
            "Step 238600 | Loss: 0.003951\n",
            "Episode 1193 | Reward: -7.42 | Epsilon: 0.050\n",
            "Step 238700 | Loss: 0.003381\n",
            "Step 238800 | Loss: 0.004709\n",
            "Episode 1194 | Reward: -27.90 | Epsilon: 0.050\n",
            "Step 238900 | Loss: 0.010301\n",
            "Step 239000 | Loss: 0.004717\n",
            "Episode 1195 | Reward: -12.95 | Epsilon: 0.050\n",
            "Step 239100 | Loss: 0.003777\n",
            "Step 239200 | Loss: 0.002909\n",
            "Episode 1196 | Reward: -19.82 | Epsilon: 0.050\n",
            "Step 239300 | Loss: 0.002939\n",
            "Step 239400 | Loss: 0.001939\n",
            "Episode 1197 | Reward: -4.48 | Epsilon: 0.050\n",
            "Step 239500 | Loss: 0.004089\n",
            "Step 239600 | Loss: 0.340831\n",
            "Episode 1198 | Reward: -10.58 | Epsilon: 0.050\n",
            "Step 239700 | Loss: 0.001656\n",
            "Step 239800 | Loss: 0.003090\n",
            "Episode 1199 | Reward: -20.41 | Epsilon: 0.050\n",
            "Step 239900 | Loss: 0.003750\n",
            "Step 240000 | Loss: 0.005073\n",
            "Episode 1200 | Target network updated.\n",
            "model saved to models/2025-04-29_04-10-48/q_network_ep_1200.pth\n",
            "\n",
            "Episode 1200 | Reward: -3.86 | Epsilon: 0.050\n",
            "Step 240100 | Loss: 0.006270\n",
            "Step 240200 | Loss: 0.007260\n",
            "Episode 1201 | Reward: -11.60 | Epsilon: 0.050\n",
            "Step 240300 | Loss: 0.003032\n",
            "Step 240400 | Loss: 0.003004\n",
            "Episode 1202 | Reward: -18.35 | Epsilon: 0.050\n",
            "Step 240500 | Loss: 0.005281\n",
            "Step 240600 | Loss: 0.006628\n",
            "Episode 1203 | Reward: -25.24 | Epsilon: 0.050\n",
            "Step 240700 | Loss: 0.003371\n",
            "Step 240800 | Loss: 0.003481\n",
            "Episode 1204 | Reward: -37.87 | Epsilon: 0.050\n",
            "Step 240900 | Loss: 0.005216\n",
            "Step 241000 | Loss: 0.008194\n",
            "Episode 1205 | Reward: -27.30 | Epsilon: 0.050\n",
            "Step 241100 | Loss: 0.003074\n",
            "Step 241200 | Loss: 0.002865\n",
            "Episode 1206 | Reward: -12.99 | Epsilon: 0.050\n",
            "Step 241300 | Loss: 0.006280\n",
            "Step 241400 | Loss: 0.287848\n",
            "Episode 1207 | Reward: -48.87 | Epsilon: 0.050\n",
            "Step 241500 | Loss: 0.037239\n",
            "Step 241600 | Loss: 0.005430\n",
            "Episode 1208 | Reward: -27.04 | Epsilon: 0.050\n",
            "Step 241700 | Loss: 0.002655\n",
            "Step 241800 | Loss: 0.003524\n",
            "Episode 1209 | Reward: -11.71 | Epsilon: 0.050\n",
            "Step 241900 | Loss: 0.005034\n",
            "Step 242000 | Loss: 0.023956\n",
            "Episode 1210 | Reward: -5.48 | Epsilon: 0.050\n",
            "Step 242100 | Loss: 0.003066\n",
            "Step 242200 | Loss: 0.759825\n",
            "Episode 1211 | Reward: -29.86 | Epsilon: 0.050\n",
            "Step 242300 | Loss: 0.002805\n",
            "Step 242400 | Loss: 0.014552\n",
            "Episode 1212 | Reward: -23.31 | Epsilon: 0.050\n",
            "Step 242500 | Loss: 0.004443\n",
            "Step 242600 | Loss: 0.002235\n",
            "Episode 1213 | Reward: -20.17 | Epsilon: 0.050\n",
            "Step 242700 | Loss: 0.015281\n",
            "Step 242800 | Loss: 0.005855\n",
            "Episode 1214 | Reward: -8.81 | Epsilon: 0.050\n",
            "Step 242900 | Loss: 0.004414\n",
            "Step 243000 | Loss: 0.042657\n",
            "Episode 1215 | Reward: -27.20 | Epsilon: 0.050\n",
            "Step 243100 | Loss: 0.021779\n",
            "Step 243200 | Loss: 0.009997\n",
            "Episode 1216 | Reward: -105.25 | Epsilon: 0.050\n",
            "Step 243300 | Loss: 0.751208\n",
            "Step 243400 | Loss: 0.306985\n",
            "Episode 1217 | Reward: -10.58 | Epsilon: 0.050\n",
            "Step 243500 | Loss: 0.009257\n",
            "Step 243600 | Loss: 0.003318\n",
            "Episode 1218 | Reward: -28.51 | Epsilon: 0.050\n",
            "Step 243700 | Loss: 0.003175\n",
            "Step 243800 | Loss: 0.002798\n",
            "Episode 1219 | Reward: -5.09 | Epsilon: 0.050\n",
            "Step 243900 | Loss: 0.005842\n",
            "Step 244000 | Loss: 0.004793\n",
            "Episode 1220 | Target network updated.\n",
            "Episode 1220 | Reward: -19.19 | Epsilon: 0.050\n",
            "Step 244100 | Loss: 0.009411\n",
            "Step 244200 | Loss: 0.004701\n",
            "Episode 1221 | Reward: -16.76 | Epsilon: 0.050\n",
            "Step 244300 | Loss: 0.133769\n",
            "Step 244400 | Loss: 0.004164\n",
            "Episode 1222 | Reward: -13.89 | Epsilon: 0.050\n",
            "Step 244500 | Loss: 0.005258\n",
            "Step 244600 | Loss: 2.580073\n",
            "Episode 1223 | Reward: -22.47 | Epsilon: 0.050\n",
            "Step 244700 | Loss: 0.003986\n",
            "Step 244800 | Loss: 0.006660\n",
            "Episode 1224 | Reward: -5.92 | Epsilon: 0.050\n",
            "Step 244900 | Loss: 0.003311\n",
            "Step 245000 | Loss: 0.003990\n",
            "Episode 1225 | Reward: -53.22 | Epsilon: 0.050\n",
            "Step 245100 | Loss: 0.002926\n",
            "Step 245200 | Loss: 0.002936\n",
            "Episode 1226 | Reward: -3.58 | Epsilon: 0.050\n",
            "Step 245300 | Loss: 0.002362\n",
            "Step 245400 | Loss: 0.004616\n",
            "Episode 1227 | Reward: -3.68 | Epsilon: 0.050\n",
            "Step 245500 | Loss: 0.007750\n",
            "Step 245600 | Loss: 0.005190\n",
            "Episode 1228 | Reward: -4.87 | Epsilon: 0.050\n",
            "Step 245700 | Loss: 0.004305\n",
            "Step 245800 | Loss: 0.003440\n",
            "Episode 1229 | Reward: -32.06 | Epsilon: 0.050\n",
            "Step 245900 | Loss: 0.002354\n",
            "Step 246000 | Loss: 0.004475\n",
            "Episode 1230 | Reward: -23.41 | Epsilon: 0.050\n",
            "Step 246100 | Loss: 0.003155\n",
            "Step 246200 | Loss: 0.002556\n",
            "Episode 1231 | Reward: -19.63 | Epsilon: 0.050\n",
            "Step 246300 | Loss: 0.003920\n",
            "Step 246400 | Loss: 0.002872\n",
            "Episode 1232 | Reward: -33.53 | Epsilon: 0.050\n",
            "Step 246500 | Loss: 0.002203\n",
            "Step 246600 | Loss: 2.681342\n",
            "Episode 1233 | Reward: -3.22 | Epsilon: 0.050\n",
            "Step 246700 | Loss: 0.035656\n",
            "Step 246800 | Loss: 0.004334\n",
            "Episode 1234 | Reward: -8.49 | Epsilon: 0.050\n",
            "Step 246900 | Loss: 0.003510\n",
            "Step 247000 | Loss: 0.004534\n",
            "Episode 1235 | Reward: -30.84 | Epsilon: 0.050\n",
            "Step 247100 | Loss: 0.004178\n",
            "Step 247200 | Loss: 0.032785\n",
            "Episode 1236 | Reward: -8.20 | Epsilon: 0.050\n",
            "Step 247300 | Loss: 0.003614\n",
            "Step 247400 | Loss: 0.007508\n",
            "Episode 1237 | Reward: -3.44 | Epsilon: 0.050\n",
            "Step 247500 | Loss: 0.002888\n",
            "Step 247600 | Loss: 0.002276\n",
            "Episode 1238 | Reward: -38.79 | Epsilon: 0.050\n",
            "Step 247700 | Loss: 0.003022\n",
            "Step 247800 | Loss: 0.002184\n",
            "Episode 1239 | Reward: -18.94 | Epsilon: 0.050\n",
            "Step 247900 | Loss: 0.003078\n",
            "Step 248000 | Loss: 0.007878\n",
            "Episode 1240 | Target network updated.\n",
            "Episode 1240 | Reward: -5.55 | Epsilon: 0.050\n",
            "Step 248100 | Loss: 0.004086\n",
            "Step 248200 | Loss: 0.008413\n",
            "Episode 1241 | Reward: -25.34 | Epsilon: 0.050\n",
            "Step 248300 | Loss: 0.010408\n",
            "Step 248400 | Loss: 0.008378\n",
            "Episode 1242 | Reward: -82.96 | Epsilon: 0.050\n",
            "Step 248500 | Loss: 0.009764\n",
            "Step 248600 | Loss: 0.004306\n",
            "Episode 1243 | Reward: -7.61 | Epsilon: 0.050\n",
            "Step 248700 | Loss: 0.002995\n",
            "Step 248800 | Loss: 0.044825\n",
            "Episode 1244 | Reward: -23.27 | Epsilon: 0.050\n",
            "Step 248900 | Loss: 0.152569\n",
            "Step 249000 | Loss: 0.005985\n",
            "Episode 1245 | Reward: -14.76 | Epsilon: 0.050\n",
            "Step 249100 | Loss: 0.001873\n",
            "Step 249200 | Loss: 0.022877\n",
            "Episode 1246 | Reward: -42.20 | Epsilon: 0.050\n",
            "Step 249300 | Loss: 0.003935\n",
            "Step 249400 | Loss: 0.038447\n",
            "Episode 1247 | Reward: -1.60 | Epsilon: 0.050\n",
            "Step 249500 | Loss: 0.002704\n",
            "Step 249600 | Loss: 0.005377\n",
            "Episode 1248 | Reward: -3.74 | Epsilon: 0.050\n",
            "Step 249700 | Loss: 0.002235\n",
            "Step 249800 | Loss: 0.004155\n",
            "Episode 1249 | Reward: -7.47 | Epsilon: 0.050\n",
            "Step 249900 | Loss: 0.002809\n",
            "Step 250000 | Loss: 0.002676\n",
            "Episode 1250 | Reward: -20.19 | Epsilon: 0.050\n",
            "Step 250100 | Loss: 0.003333\n",
            "Step 250200 | Loss: 0.004285\n",
            "Episode 1251 | Reward: -59.85 | Epsilon: 0.050\n",
            "Step 250300 | Loss: 0.004845\n",
            "Step 250400 | Loss: 0.016248\n",
            "Episode 1252 | Reward: -3.99 | Epsilon: 0.050\n",
            "Step 250500 | Loss: 0.141197\n",
            "Step 250600 | Loss: 0.021324\n",
            "Episode 1253 | Reward: -30.73 | Epsilon: 0.050\n",
            "Step 250700 | Loss: 0.005160\n",
            "Step 250800 | Loss: 0.017270\n",
            "Episode 1254 | Reward: -6.51 | Epsilon: 0.050\n",
            "Step 250900 | Loss: 0.006479\n",
            "Step 251000 | Loss: 0.002770\n",
            "Episode 1255 | Reward: -20.51 | Epsilon: 0.050\n",
            "Step 251100 | Loss: 0.004916\n",
            "Step 251200 | Loss: 0.005483\n",
            "Episode 1256 | Reward: -5.18 | Epsilon: 0.050\n",
            "Step 251300 | Loss: 0.005611\n",
            "Step 251400 | Loss: 0.002999\n",
            "Episode 1257 | Reward: -24.87 | Epsilon: 0.050\n",
            "Step 251500 | Loss: 0.003944\n",
            "Step 251600 | Loss: 0.003278\n",
            "Episode 1258 | Reward: -5.31 | Epsilon: 0.050\n",
            "Step 251700 | Loss: 0.003021\n",
            "Step 251800 | Loss: 0.005086\n",
            "Episode 1259 | Reward: -5.31 | Epsilon: 0.050\n",
            "Step 251900 | Loss: 0.003856\n",
            "Step 252000 | Loss: 0.003473\n",
            "Episode 1260 | Target network updated.\n",
            "Episode 1260 | Reward: -6.45 | Epsilon: 0.050\n",
            "Step 252100 | Loss: 0.004136\n",
            "Step 252200 | Loss: 0.006669\n",
            "Episode 1261 | Reward: -11.01 | Epsilon: 0.050\n",
            "Step 252300 | Loss: 0.006695\n",
            "Step 252400 | Loss: 0.006244\n",
            "Episode 1262 | Reward: -20.97 | Epsilon: 0.050\n",
            "Step 252500 | Loss: 0.014863\n",
            "Step 252600 | Loss: 0.003234\n",
            "Episode 1263 | Reward: -3.56 | Epsilon: 0.050\n",
            "Step 252700 | Loss: 0.007906\n",
            "Step 252800 | Loss: 0.002270\n",
            "Episode 1264 | Reward: -63.01 | Epsilon: 0.050\n",
            "Step 252900 | Loss: 0.003487\n",
            "Step 253000 | Loss: 0.003476\n",
            "Episode 1265 | Reward: -60.46 | Epsilon: 0.050\n",
            "Step 253100 | Loss: 0.003450\n",
            "Step 253200 | Loss: 0.004235\n",
            "Episode 1266 | Reward: -15.22 | Epsilon: 0.050\n",
            "Step 253300 | Loss: 0.005104\n",
            "Step 253400 | Loss: 0.003633\n",
            "Episode 1267 | Reward: -8.11 | Epsilon: 0.050\n",
            "Step 253500 | Loss: 0.006086\n",
            "Step 253600 | Loss: 0.004298\n",
            "Episode 1268 | Reward: -46.11 | Epsilon: 0.050\n",
            "Step 253700 | Loss: 0.004201\n",
            "Step 253800 | Loss: 0.016377\n",
            "Episode 1269 | Reward: -2.90 | Epsilon: 0.050\n",
            "Step 253900 | Loss: 0.005921\n",
            "Step 254000 | Loss: 0.021330\n",
            "Episode 1270 | Reward: -73.22 | Epsilon: 0.050\n",
            "Step 254100 | Loss: 0.003428\n",
            "Step 254200 | Loss: 0.002676\n",
            "Episode 1271 | Reward: -4.79 | Epsilon: 0.050\n",
            "Step 254300 | Loss: 0.005762\n",
            "Step 254400 | Loss: 0.004727\n",
            "Episode 1272 | Reward: -35.00 | Epsilon: 0.050\n",
            "Step 254500 | Loss: 0.161344\n",
            "Step 254600 | Loss: 0.002926\n",
            "Episode 1273 | Reward: -15.32 | Epsilon: 0.050\n",
            "Step 254700 | Loss: 0.028624\n",
            "Step 254800 | Loss: 0.008952\n",
            "Episode 1274 | Reward: -2.54 | Epsilon: 0.050\n",
            "Step 254900 | Loss: 0.004361\n",
            "Step 255000 | Loss: 0.008308\n",
            "Episode 1275 | Reward: -24.37 | Epsilon: 0.050\n",
            "Step 255100 | Loss: 0.007004\n",
            "Step 255200 | Loss: 0.002800\n",
            "Episode 1276 | Reward: -30.77 | Epsilon: 0.050\n",
            "Step 255300 | Loss: 0.002940\n",
            "Step 255400 | Loss: 0.004920\n",
            "Episode 1277 | Reward: -41.46 | Epsilon: 0.050\n",
            "Step 255500 | Loss: 0.009021\n",
            "Step 255600 | Loss: 0.014530\n",
            "Episode 1278 | Reward: -24.27 | Epsilon: 0.050\n",
            "Step 255700 | Loss: 0.005178\n",
            "Step 255800 | Loss: 0.007256\n",
            "Episode 1279 | Reward: -2.97 | Epsilon: 0.050\n",
            "Step 255900 | Loss: 0.052214\n",
            "Step 256000 | Loss: 0.003119\n",
            "Episode 1280 | Target network updated.\n",
            "Episode 1280 | Reward: -162.61 | Epsilon: 0.050\n",
            "Step 256100 | Loss: 0.006481\n",
            "Step 256200 | Loss: 0.003746\n",
            "Episode 1281 | Reward: -12.69 | Epsilon: 0.050\n",
            "Step 256300 | Loss: 0.008928\n",
            "Step 256400 | Loss: 0.038369\n",
            "Episode 1282 | Reward: -6.48 | Epsilon: 0.050\n",
            "Step 256500 | Loss: 0.004118\n",
            "Step 256600 | Loss: 0.003720\n",
            "Episode 1283 | Reward: -35.70 | Epsilon: 0.050\n",
            "Step 256700 | Loss: 0.008252\n",
            "Step 256800 | Loss: 0.004522\n",
            "Episode 1284 | Reward: -9.07 | Epsilon: 0.050\n",
            "Step 256900 | Loss: 0.022586\n",
            "Step 257000 | Loss: 0.004444\n",
            "Episode 1285 | Reward: -5.86 | Epsilon: 0.050\n",
            "Step 257100 | Loss: 0.002384\n",
            "Step 257200 | Loss: 0.020412\n",
            "Episode 1286 | Reward: -33.77 | Epsilon: 0.050\n",
            "Step 257300 | Loss: 0.003761\n",
            "Step 257400 | Loss: 0.005133\n",
            "Episode 1287 | Reward: -116.11 | Epsilon: 0.050\n",
            "Step 257500 | Loss: 0.008781\n",
            "Step 257600 | Loss: 0.429274\n",
            "Episode 1288 | Reward: -88.65 | Epsilon: 0.050\n",
            "Step 257700 | Loss: 0.006437\n",
            "Step 257800 | Loss: 0.010215\n",
            "Episode 1289 | Reward: -126.90 | Epsilon: 0.050\n",
            "Step 257900 | Loss: 0.012817\n",
            "Step 258000 | Loss: 0.010466\n",
            "Episode 1290 | Reward: -15.10 | Epsilon: 0.050\n",
            "Step 258100 | Loss: 0.008518\n",
            "Step 258200 | Loss: 0.071323\n",
            "Episode 1291 | Reward: -18.68 | Epsilon: 0.050\n",
            "Step 258300 | Loss: 0.014834\n",
            "Step 258400 | Loss: 0.002247\n",
            "Episode 1292 | Reward: -25.99 | Epsilon: 0.050\n",
            "Step 258500 | Loss: 0.007717\n",
            "Step 258600 | Loss: 0.006792\n",
            "Episode 1293 | Reward: -11.67 | Epsilon: 0.050\n",
            "Step 258700 | Loss: 0.007417\n",
            "Step 258800 | Loss: 0.004183\n",
            "Episode 1294 | Reward: -5.56 | Epsilon: 0.050\n",
            "Step 258900 | Loss: 0.002882\n",
            "Step 259000 | Loss: 0.005618\n",
            "Episode 1295 | Reward: -3.97 | Epsilon: 0.050\n",
            "Step 259100 | Loss: 0.007231\n",
            "Step 259200 | Loss: 0.004988\n",
            "Episode 1296 | Reward: -45.28 | Epsilon: 0.050\n",
            "Step 259300 | Loss: 0.010274\n",
            "Step 259400 | Loss: 0.008493\n",
            "Episode 1297 | Reward: -24.35 | Epsilon: 0.050\n",
            "Step 259500 | Loss: 0.006620\n",
            "Step 259600 | Loss: 0.004878\n",
            "Episode 1298 | Reward: -57.42 | Epsilon: 0.050\n",
            "Step 259700 | Loss: 0.007107\n",
            "Step 259800 | Loss: 0.017060\n",
            "Episode 1299 | Reward: -12.30 | Epsilon: 0.050\n",
            "Step 259900 | Loss: 0.005073\n",
            "Step 260000 | Loss: 0.005293\n",
            "Episode 1300 | Target network updated.\n",
            "model saved to models/2025-04-29_04-13-01/q_network_ep_1300.pth\n",
            "\n",
            "Episode 1300 | Reward: -6.21 | Epsilon: 0.050\n",
            "Step 260100 | Loss: 0.007647\n",
            "Step 260200 | Loss: 0.008378\n",
            "Episode 1301 | Reward: -20.10 | Epsilon: 0.050\n",
            "Step 260300 | Loss: 0.004448\n",
            "Step 260400 | Loss: 0.003304\n",
            "Episode 1302 | Reward: -11.17 | Epsilon: 0.050\n",
            "Step 260500 | Loss: 0.004687\n",
            "Step 260600 | Loss: 1.054765\n",
            "Episode 1303 | Reward: -27.21 | Epsilon: 0.050\n",
            "Step 260700 | Loss: 1.008794\n",
            "Step 260800 | Loss: 0.007855\n",
            "Episode 1304 | Reward: -13.29 | Epsilon: 0.050\n",
            "Step 260900 | Loss: 0.003842\n",
            "Step 261000 | Loss: 0.003437\n",
            "Episode 1305 | Reward: -3.41 | Epsilon: 0.050\n",
            "Step 261100 | Loss: 0.017365\n",
            "Step 261200 | Loss: 0.003044\n",
            "Episode 1306 | Reward: -35.34 | Epsilon: 0.050\n",
            "Step 261300 | Loss: 0.006122\n",
            "Step 261400 | Loss: 0.005373\n",
            "Episode 1307 | Reward: -4.14 | Epsilon: 0.050\n",
            "Step 261500 | Loss: 0.010017\n",
            "Step 261600 | Loss: 0.003264\n",
            "Episode 1308 | Reward: -1.50 | Epsilon: 0.050\n",
            "Step 261700 | Loss: 0.005611\n",
            "Step 261800 | Loss: 0.013321\n",
            "Episode 1309 | Reward: -28.85 | Epsilon: 0.050\n",
            "Step 261900 | Loss: 0.012442\n",
            "Step 262000 | Loss: 0.002919\n",
            "Episode 1310 | Reward: -1.74 | Epsilon: 0.050\n",
            "Step 262100 | Loss: 0.007648\n",
            "Step 262200 | Loss: 0.003337\n",
            "Episode 1311 | Reward: -24.38 | Epsilon: 0.050\n",
            "Step 262300 | Loss: 0.012079\n",
            "Step 262400 | Loss: 0.003213\n",
            "Episode 1312 | Reward: -11.43 | Epsilon: 0.050\n",
            "Step 262500 | Loss: 0.005599\n",
            "Step 262600 | Loss: 0.014492\n",
            "Episode 1313 | Reward: -23.66 | Epsilon: 0.050\n",
            "Step 262700 | Loss: 0.015598\n",
            "Step 262800 | Loss: 0.006436\n",
            "Episode 1314 | Reward: -16.84 | Epsilon: 0.050\n",
            "Step 262900 | Loss: 0.003911\n",
            "Step 263000 | Loss: 0.007119\n",
            "Episode 1315 | Reward: -7.59 | Epsilon: 0.050\n",
            "Step 263100 | Loss: 1.661048\n",
            "Step 263200 | Loss: 0.004777\n",
            "Episode 1316 | Reward: -140.40 | Epsilon: 0.050\n",
            "Step 263300 | Loss: 0.004612\n",
            "Step 263400 | Loss: 0.012757\n",
            "Episode 1317 | Reward: -2.22 | Epsilon: 0.050\n",
            "Step 263500 | Loss: 0.009959\n",
            "Step 263600 | Loss: 0.003196\n",
            "Episode 1318 | Reward: -7.87 | Epsilon: 0.050\n",
            "Step 263700 | Loss: 0.009025\n",
            "Step 263800 | Loss: 0.007758\n",
            "Episode 1319 | Reward: -63.23 | Epsilon: 0.050\n",
            "Step 263900 | Loss: 0.003397\n",
            "Step 264000 | Loss: 0.006404\n",
            "Episode 1320 | Target network updated.\n",
            "Episode 1320 | Reward: -60.35 | Epsilon: 0.050\n",
            "Step 264100 | Loss: 0.032130\n",
            "Step 264200 | Loss: 0.006418\n",
            "Episode 1321 | Reward: -25.40 | Epsilon: 0.050\n",
            "Step 264300 | Loss: 0.015560\n",
            "Step 264400 | Loss: 0.006680\n",
            "Episode 1322 | Reward: -2.92 | Epsilon: 0.050\n",
            "Step 264500 | Loss: 0.006282\n",
            "Step 264600 | Loss: 0.007960\n",
            "Episode 1323 | Reward: -4.08 | Epsilon: 0.050\n",
            "Step 264700 | Loss: 0.012275\n",
            "Step 264800 | Loss: 0.008809\n",
            "Episode 1324 | Reward: -4.92 | Epsilon: 0.050\n",
            "Step 264900 | Loss: 0.005446\n",
            "Step 265000 | Loss: 0.009325\n",
            "Episode 1325 | Reward: -5.15 | Epsilon: 0.050\n",
            "Step 265100 | Loss: 0.025022\n",
            "Step 265200 | Loss: 0.014283\n",
            "Episode 1326 | Reward: -6.58 | Epsilon: 0.050\n",
            "Step 265300 | Loss: 0.005877\n",
            "Step 265400 | Loss: 0.003736\n",
            "Episode 1327 | Reward: -51.00 | Epsilon: 0.050\n",
            "Step 265500 | Loss: 0.004626\n",
            "Step 265600 | Loss: 0.007431\n",
            "Episode 1328 | Reward: -112.82 | Epsilon: 0.050\n",
            "Step 265700 | Loss: 0.009995\n",
            "Step 265800 | Loss: 0.005193\n",
            "Episode 1329 | Reward: -8.83 | Epsilon: 0.050\n",
            "Step 265900 | Loss: 0.005779\n",
            "Step 266000 | Loss: 0.014713\n",
            "Episode 1330 | Reward: -8.27 | Epsilon: 0.050\n",
            "Step 266100 | Loss: 0.002660\n",
            "Step 266200 | Loss: 0.012717\n",
            "Episode 1331 | Reward: -2.86 | Epsilon: 0.050\n",
            "Step 266300 | Loss: 0.006546\n",
            "Step 266400 | Loss: 0.032243\n",
            "Episode 1332 | Reward: -5.00 | Epsilon: 0.050\n",
            "Step 266500 | Loss: 0.005075\n",
            "Step 266600 | Loss: 0.005911\n",
            "Episode 1333 | Reward: -42.33 | Epsilon: 0.050\n",
            "Step 266700 | Loss: 0.004107\n",
            "Step 266800 | Loss: 0.013648\n",
            "Episode 1334 | Reward: -3.04 | Epsilon: 0.050\n",
            "Step 266900 | Loss: 0.010927\n",
            "Step 267000 | Loss: 0.013378\n",
            "Episode 1335 | Reward: -2.97 | Epsilon: 0.050\n",
            "Step 267100 | Loss: 0.006804\n",
            "Step 267200 | Loss: 0.007333\n",
            "Episode 1336 | Reward: -8.68 | Epsilon: 0.050\n",
            "Step 267300 | Loss: 0.005494\n",
            "Step 267400 | Loss: 0.004000\n",
            "Episode 1337 | Reward: -26.64 | Epsilon: 0.050\n",
            "Step 267500 | Loss: 0.004524\n",
            "Step 267600 | Loss: 0.010677\n",
            "Episode 1338 | Reward: -10.28 | Epsilon: 0.050\n",
            "Step 267700 | Loss: 0.004918\n",
            "Step 267800 | Loss: 0.004371\n",
            "Episode 1339 | Reward: -3.84 | Epsilon: 0.050\n",
            "Step 267900 | Loss: 0.174900\n",
            "Step 268000 | Loss: 0.175100\n",
            "Episode 1340 | Target network updated.\n",
            "Episode 1340 | Reward: -5.40 | Epsilon: 0.050\n",
            "Step 268100 | Loss: 0.003103\n",
            "Step 268200 | Loss: 0.003543\n",
            "Episode 1341 | Reward: -24.11 | Epsilon: 0.050\n",
            "Step 268300 | Loss: 0.007583\n",
            "Step 268400 | Loss: 0.003963\n",
            "Episode 1342 | Reward: -7.14 | Epsilon: 0.050\n",
            "Step 268500 | Loss: 0.005345\n",
            "Step 268600 | Loss: 0.005919\n",
            "Episode 1343 | Reward: -9.67 | Epsilon: 0.050\n",
            "Step 268700 | Loss: 0.036921\n",
            "Step 268800 | Loss: 1.723830\n",
            "Episode 1344 | Reward: -10.45 | Epsilon: 0.050\n",
            "Step 268900 | Loss: 1.816047\n",
            "Step 269000 | Loss: 0.005184\n",
            "Episode 1345 | Reward: -3.38 | Epsilon: 0.050\n",
            "Step 269100 | Loss: 0.005280\n",
            "Step 269200 | Loss: 0.007364\n",
            "Episode 1346 | Reward: -4.86 | Epsilon: 0.050\n",
            "Step 269300 | Loss: 0.002323\n",
            "Step 269400 | Loss: 0.003435\n",
            "Episode 1347 | Reward: -12.54 | Epsilon: 0.050\n",
            "Step 269500 | Loss: 0.005684\n",
            "Step 269600 | Loss: 0.003870\n",
            "Episode 1348 | Reward: -37.55 | Epsilon: 0.050\n",
            "Step 269700 | Loss: 0.004332\n",
            "Step 269800 | Loss: 0.003364\n",
            "Episode 1349 | Reward: -7.00 | Epsilon: 0.050\n",
            "Step 269900 | Loss: 0.004921\n",
            "Step 270000 | Loss: 0.005198\n",
            "Episode 1350 | Reward: -102.45 | Epsilon: 0.050\n",
            "Step 270100 | Loss: 0.054603\n",
            "Step 270200 | Loss: 0.003916\n",
            "Episode 1351 | Reward: -13.79 | Epsilon: 0.050\n",
            "Step 270300 | Loss: 0.003658\n",
            "Step 270400 | Loss: 0.002049\n",
            "Episode 1352 | Reward: -7.24 | Epsilon: 0.050\n",
            "Step 270500 | Loss: 0.003305\n",
            "Step 270600 | Loss: 0.058718\n",
            "Episode 1353 | Reward: -3.74 | Epsilon: 0.050\n",
            "Step 270700 | Loss: 0.004536\n",
            "Step 270800 | Loss: 0.004848\n",
            "Episode 1354 | Reward: -8.90 | Epsilon: 0.050\n",
            "Step 270900 | Loss: 0.004081\n",
            "Step 271000 | Loss: 0.004895\n",
            "Episode 1355 | Reward: -20.95 | Epsilon: 0.050\n",
            "Step 271100 | Loss: 0.006404\n",
            "Step 271200 | Loss: 0.003924\n",
            "Episode 1356 | Reward: -29.49 | Epsilon: 0.050\n",
            "Step 271300 | Loss: 0.006612\n",
            "Step 271400 | Loss: 0.005301\n",
            "Episode 1357 | Reward: -40.94 | Epsilon: 0.050\n",
            "Step 271500 | Loss: 0.003927\n",
            "Step 271600 | Loss: 0.005224\n",
            "Episode 1358 | Reward: -9.77 | Epsilon: 0.050\n",
            "Step 271700 | Loss: 0.007705\n",
            "Step 271800 | Loss: 0.007546\n",
            "Episode 1359 | Reward: -5.53 | Epsilon: 0.050\n",
            "Step 271900 | Loss: 0.002592\n",
            "Step 272000 | Loss: 0.003368\n",
            "Episode 1360 | Target network updated.\n",
            "Episode 1360 | Reward: -55.40 | Epsilon: 0.050\n",
            "Step 272100 | Loss: 0.003339\n",
            "Step 272200 | Loss: 0.005126\n",
            "Episode 1361 | Reward: -3.90 | Epsilon: 0.050\n",
            "Step 272300 | Loss: 0.002966\n",
            "Step 272400 | Loss: 0.010568\n",
            "Episode 1362 | Reward: -1.78 | Epsilon: 0.050\n",
            "Step 272500 | Loss: 0.005492\n",
            "Step 272600 | Loss: 0.004505\n",
            "Episode 1363 | Reward: -46.30 | Epsilon: 0.050\n",
            "Step 272700 | Loss: 0.003932\n",
            "Step 272800 | Loss: 0.003349\n",
            "Episode 1364 | Reward: -11.75 | Epsilon: 0.050\n",
            "Step 272900 | Loss: 0.009341\n",
            "Step 273000 | Loss: 0.005935\n",
            "Episode 1365 | Reward: -16.90 | Epsilon: 0.050\n",
            "Step 273100 | Loss: 0.005384\n",
            "Step 273200 | Loss: 0.008231\n",
            "Episode 1366 | Reward: -58.64 | Epsilon: 0.050\n",
            "Step 273300 | Loss: 0.010215\n",
            "Step 273400 | Loss: 0.010635\n",
            "Episode 1367 | Reward: -136.85 | Epsilon: 0.050\n",
            "Step 273500 | Loss: 0.004614\n",
            "Step 273600 | Loss: 0.005278\n",
            "Episode 1368 | Reward: -1.18 | Epsilon: 0.050\n",
            "Step 273700 | Loss: 0.005773\n",
            "Step 273800 | Loss: 0.005475\n",
            "Episode 1369 | Reward: -56.14 | Epsilon: 0.050\n",
            "Step 273900 | Loss: 0.008462\n",
            "Step 274000 | Loss: 0.005161\n",
            "Episode 1370 | Reward: -6.29 | Epsilon: 0.050\n",
            "Step 274100 | Loss: 0.008704\n",
            "Step 274200 | Loss: 0.004386\n",
            "Episode 1371 | Reward: -7.95 | Epsilon: 0.050\n",
            "Step 274300 | Loss: 0.003556\n",
            "Step 274400 | Loss: 0.003780\n",
            "Episode 1372 | Reward: -5.88 | Epsilon: 0.050\n",
            "Step 274500 | Loss: 0.010974\n",
            "Step 274600 | Loss: 0.013293\n",
            "Episode 1373 | Reward: -12.85 | Epsilon: 0.050\n",
            "Step 274700 | Loss: 0.013267\n",
            "Step 274800 | Loss: 0.005532\n",
            "Episode 1374 | Reward: -3.79 | Epsilon: 0.050\n",
            "Step 274900 | Loss: 0.077239\n",
            "Step 275000 | Loss: 0.006123\n",
            "Episode 1375 | Reward: -31.75 | Epsilon: 0.050\n",
            "Step 275100 | Loss: 0.003089\n",
            "Step 275200 | Loss: 0.011217\n",
            "Episode 1376 | Reward: -44.54 | Epsilon: 0.050\n",
            "Step 275300 | Loss: 0.008889\n",
            "Step 275400 | Loss: 0.011904\n",
            "Episode 1377 | Reward: -9.93 | Epsilon: 0.050\n",
            "Step 275500 | Loss: 0.009880\n",
            "Step 275600 | Loss: 0.006380\n",
            "Episode 1378 | Reward: -92.54 | Epsilon: 0.050\n",
            "Step 275700 | Loss: 0.005391\n",
            "Step 275800 | Loss: 0.009868\n",
            "Episode 1379 | Reward: -23.45 | Epsilon: 0.050\n",
            "Step 275900 | Loss: 0.005587\n",
            "Step 276000 | Loss: 0.004367\n",
            "Episode 1380 | Target network updated.\n",
            "Episode 1380 | Reward: -25.07 | Epsilon: 0.050\n",
            "Step 276100 | Loss: 0.028023\n",
            "Step 276200 | Loss: 0.002743\n",
            "Episode 1381 | Reward: -8.99 | Epsilon: 0.050\n",
            "Step 276300 | Loss: 0.004545\n",
            "Step 276400 | Loss: 0.002670\n",
            "Episode 1382 | Reward: -57.57 | Epsilon: 0.050\n",
            "Step 276500 | Loss: 0.004207\n",
            "Step 276600 | Loss: 0.006723\n",
            "Episode 1383 | Reward: -2.94 | Epsilon: 0.050\n",
            "Step 276700 | Loss: 0.008058\n",
            "Step 276800 | Loss: 0.012778\n",
            "Episode 1384 | Reward: -16.01 | Epsilon: 0.050\n",
            "Step 276900 | Loss: 0.004251\n",
            "Step 277000 | Loss: 0.013295\n",
            "Episode 1385 | Reward: -8.53 | Epsilon: 0.050\n",
            "Step 277100 | Loss: 0.004856\n",
            "Step 277200 | Loss: 0.006568\n",
            "Episode 1386 | Reward: -71.66 | Epsilon: 0.050\n",
            "Step 277300 | Loss: 0.002972\n",
            "Step 277400 | Loss: 0.008241\n",
            "Episode 1387 | Reward: -9.26 | Epsilon: 0.050\n",
            "Step 277500 | Loss: 0.006852\n",
            "Step 277600 | Loss: 0.002225\n",
            "Episode 1388 | Reward: -9.42 | Epsilon: 0.050\n",
            "Step 277700 | Loss: 0.007684\n",
            "Step 277800 | Loss: 0.007537\n",
            "Episode 1389 | Reward: -2.23 | Epsilon: 0.050\n",
            "Step 277900 | Loss: 0.005790\n",
            "Step 278000 | Loss: 18.428585\n",
            "Episode 1390 | Reward: -14.96 | Epsilon: 0.050\n",
            "Step 278100 | Loss: 0.002564\n",
            "Step 278200 | Loss: 0.010330\n",
            "Episode 1391 | Reward: -7.35 | Epsilon: 0.050\n",
            "Step 278300 | Loss: 0.005754\n",
            "Step 278400 | Loss: 0.011958\n",
            "Episode 1392 | Reward: -3.29 | Epsilon: 0.050\n",
            "Step 278500 | Loss: 0.004790\n",
            "Step 278600 | Loss: 0.008088\n",
            "Episode 1393 | Reward: -10.44 | Epsilon: 0.050\n",
            "Step 278700 | Loss: 0.003254\n",
            "Step 278800 | Loss: 0.004281\n",
            "Episode 1394 | Reward: -12.15 | Epsilon: 0.050\n",
            "Step 278900 | Loss: 0.029285\n",
            "Step 279000 | Loss: 0.787800\n",
            "Episode 1395 | Reward: -73.82 | Epsilon: 0.050\n",
            "Step 279100 | Loss: 0.004278\n",
            "Step 279200 | Loss: 0.015126\n",
            "Episode 1396 | Reward: -782.37 | Epsilon: 0.050\n",
            "Step 279300 | Loss: 0.015474\n",
            "Step 279400 | Loss: 0.008749\n",
            "Episode 1397 | Reward: -73.85 | Epsilon: 0.050\n",
            "Step 279500 | Loss: 0.005690\n",
            "Step 279600 | Loss: 0.010636\n",
            "Episode 1398 | Reward: -11.31 | Epsilon: 0.050\n",
            "Step 279700 | Loss: 0.004796\n",
            "Step 279800 | Loss: 0.004412\n",
            "Episode 1399 | Reward: -15.17 | Epsilon: 0.050\n",
            "Step 279900 | Loss: 0.004544\n",
            "Step 280000 | Loss: 0.005100\n",
            "Episode 1400 | Target network updated.\n",
            "model saved to models/2025-04-29_04-15-12/q_network_ep_1400.pth\n",
            "\n",
            "Episode 1400 | Reward: -1.76 | Epsilon: 0.050\n",
            "Step 280100 | Loss: 0.050654\n",
            "Step 280200 | Loss: 0.010874\n",
            "Episode 1401 | Reward: -5.39 | Epsilon: 0.050\n",
            "Step 280300 | Loss: 0.009955\n",
            "Step 280400 | Loss: 0.018935\n",
            "Episode 1402 | Reward: -14.68 | Epsilon: 0.050\n",
            "Step 280500 | Loss: 0.004030\n",
            "Step 280600 | Loss: 0.005660\n",
            "Episode 1403 | Reward: -267.39 | Epsilon: 0.050\n",
            "Step 280700 | Loss: 0.004182\n",
            "Step 280800 | Loss: 0.006292\n",
            "Episode 1404 | Reward: -25.91 | Epsilon: 0.050\n",
            "Step 280900 | Loss: 0.005792\n",
            "Step 281000 | Loss: 0.012881\n",
            "Episode 1405 | Reward: -5.68 | Epsilon: 0.050\n",
            "Step 281100 | Loss: 0.012639\n",
            "Step 281200 | Loss: 0.010623\n",
            "Episode 1406 | Reward: -19.11 | Epsilon: 0.050\n",
            "Step 281300 | Loss: 19.577265\n",
            "Step 281400 | Loss: 0.011509\n",
            "Episode 1407 | Reward: -8.64 | Epsilon: 0.050\n",
            "Step 281500 | Loss: 0.012563\n",
            "Step 281600 | Loss: 0.008561\n",
            "Episode 1408 | Reward: -2.62 | Epsilon: 0.050\n",
            "Step 281700 | Loss: 0.006257\n",
            "Step 281800 | Loss: 0.009727\n",
            "Episode 1409 | Reward: -39.89 | Epsilon: 0.050\n",
            "Step 281900 | Loss: 0.019382\n",
            "Step 282000 | Loss: 0.012112\n",
            "Episode 1410 | Reward: -133.43 | Epsilon: 0.050\n",
            "Step 282100 | Loss: 0.008989\n",
            "Step 282200 | Loss: 0.003771\n",
            "Episode 1411 | Reward: -11.51 | Epsilon: 0.050\n",
            "Step 282300 | Loss: 0.009810\n",
            "Step 282400 | Loss: 0.004894\n",
            "Episode 1412 | Reward: -12.42 | Epsilon: 0.050\n",
            "Step 282500 | Loss: 0.011607\n",
            "Step 282600 | Loss: 0.006427\n",
            "Episode 1413 | Reward: -3.70 | Epsilon: 0.050\n",
            "Step 282700 | Loss: 0.007302\n",
            "Step 282800 | Loss: 4.786701\n",
            "Episode 1414 | Reward: -3.18 | Epsilon: 0.050\n",
            "Step 282900 | Loss: 0.007232\n",
            "Step 283000 | Loss: 0.026143\n",
            "Episode 1415 | Reward: -461.77 | Epsilon: 0.050\n",
            "Step 283100 | Loss: 0.012644\n",
            "Step 283200 | Loss: 0.008271\n",
            "Episode 1416 | Reward: -78.14 | Epsilon: 0.050\n",
            "Step 283300 | Loss: 0.015128\n",
            "Step 283400 | Loss: 5.141554\n",
            "Episode 1417 | Reward: -25.72 | Epsilon: 0.050\n",
            "Step 283500 | Loss: 0.929138\n",
            "Step 283600 | Loss: 0.009609\n",
            "Episode 1418 | Reward: -21.06 | Epsilon: 0.050\n",
            "Step 283700 | Loss: 5.441296\n",
            "Step 283800 | Loss: 0.009713\n",
            "Episode 1419 | Reward: -45.60 | Epsilon: 0.050\n",
            "Step 283900 | Loss: 0.004180\n",
            "Step 284000 | Loss: 4.514424\n",
            "Episode 1420 | Target network updated.\n",
            "Episode 1420 | Reward: -12.43 | Epsilon: 0.050\n",
            "Step 284100 | Loss: 0.004774\n",
            "Step 284200 | Loss: 0.005444\n",
            "Episode 1421 | Reward: -15.56 | Epsilon: 0.050\n",
            "Step 284300 | Loss: 0.006683\n",
            "Step 284400 | Loss: 0.007022\n",
            "Episode 1422 | Reward: -17.48 | Epsilon: 0.050\n",
            "Step 284500 | Loss: 0.003434\n",
            "Step 284600 | Loss: 0.009738\n",
            "Episode 1423 | Reward: -8.08 | Epsilon: 0.050\n",
            "Step 284700 | Loss: 0.020305\n",
            "Step 284800 | Loss: 0.007278\n",
            "Episode 1424 | Reward: -45.60 | Epsilon: 0.050\n",
            "Step 284900 | Loss: 0.286759\n",
            "Step 285000 | Loss: 0.005349\n",
            "Episode 1425 | Reward: -2.34 | Epsilon: 0.050\n",
            "Step 285100 | Loss: 0.013595\n",
            "Step 285200 | Loss: 0.026243\n",
            "Episode 1426 | Reward: -23.95 | Epsilon: 0.050\n",
            "Step 285300 | Loss: 0.005530\n",
            "Step 285400 | Loss: 0.008425\n",
            "Episode 1427 | Reward: -45.49 | Epsilon: 0.050\n",
            "Step 285500 | Loss: 0.007799\n",
            "Step 285600 | Loss: 0.005911\n",
            "Episode 1428 | Reward: -16.52 | Epsilon: 0.050\n",
            "Step 285700 | Loss: 3.085724\n",
            "Step 285800 | Loss: 0.004203\n",
            "Episode 1429 | Reward: -33.55 | Epsilon: 0.050\n",
            "Step 285900 | Loss: 0.021221\n",
            "Step 286000 | Loss: 0.008188\n",
            "Episode 1430 | Reward: -56.91 | Epsilon: 0.050\n",
            "Step 286100 | Loss: 0.006794\n",
            "Step 286200 | Loss: 0.004509\n",
            "Episode 1431 | Reward: -9.27 | Epsilon: 0.050\n",
            "Step 286300 | Loss: 0.007976\n",
            "Step 286400 | Loss: 0.014194\n",
            "Episode 1432 | Reward: -27.73 | Epsilon: 0.050\n",
            "Step 286500 | Loss: 0.011608\n",
            "Step 286600 | Loss: 0.003866\n",
            "Episode 1433 | Reward: -7.58 | Epsilon: 0.050\n",
            "Step 286700 | Loss: 18.089199\n",
            "Step 286800 | Loss: 0.007646\n",
            "Episode 1434 | Reward: -14.99 | Epsilon: 0.050\n",
            "Step 286900 | Loss: 0.005826\n",
            "Step 287000 | Loss: 0.006262\n",
            "Episode 1435 | Reward: -49.23 | Epsilon: 0.050\n",
            "Step 287100 | Loss: 0.012301\n",
            "Step 287200 | Loss: 7.499621\n",
            "Episode 1436 | Reward: -11.58 | Epsilon: 0.050\n",
            "Step 287300 | Loss: 0.009649\n",
            "Step 287400 | Loss: 0.008555\n",
            "Episode 1437 | Reward: -8.00 | Epsilon: 0.050\n",
            "Step 287500 | Loss: 0.005021\n",
            "Step 287600 | Loss: 0.003334\n",
            "Episode 1438 | Reward: -10.88 | Epsilon: 0.050\n",
            "Step 287700 | Loss: 0.077300\n",
            "Step 287800 | Loss: 0.011069\n",
            "Episode 1439 | Reward: -46.92 | Epsilon: 0.050\n",
            "Step 287900 | Loss: 0.011102\n",
            "Step 288000 | Loss: 0.003500\n",
            "Episode 1440 | Target network updated.\n",
            "Episode 1440 | Reward: -24.45 | Epsilon: 0.050\n",
            "Step 288100 | Loss: 0.008765\n",
            "Step 288200 | Loss: 0.005423\n",
            "Episode 1441 | Reward: -13.01 | Epsilon: 0.050\n",
            "Step 288300 | Loss: 0.037385\n",
            "Step 288400 | Loss: 0.006480\n",
            "Episode 1442 | Reward: -9.13 | Epsilon: 0.050\n",
            "Step 288500 | Loss: 0.801872\n",
            "Step 288600 | Loss: 0.003557\n",
            "Episode 1443 | Reward: -13.74 | Epsilon: 0.050\n",
            "Step 288700 | Loss: 0.004326\n",
            "Step 288800 | Loss: 0.008018\n",
            "Episode 1444 | Reward: -139.63 | Epsilon: 0.050\n",
            "Step 288900 | Loss: 0.013469\n",
            "Step 289000 | Loss: 0.017679\n",
            "Episode 1445 | Reward: -36.40 | Epsilon: 0.050\n",
            "Step 289100 | Loss: 0.004104\n",
            "Step 289200 | Loss: 0.005808\n",
            "Episode 1446 | Reward: -31.95 | Epsilon: 0.050\n",
            "Step 289300 | Loss: 0.084372\n",
            "Step 289400 | Loss: 0.005913\n",
            "Episode 1447 | Reward: -75.38 | Epsilon: 0.050\n",
            "Step 289500 | Loss: 0.012809\n",
            "Step 289600 | Loss: 0.011301\n",
            "Episode 1448 | Reward: -8.24 | Epsilon: 0.050\n",
            "Step 289700 | Loss: 0.005172\n",
            "Step 289800 | Loss: 0.491404\n",
            "Episode 1449 | Reward: -28.23 | Epsilon: 0.050\n",
            "Step 289900 | Loss: 0.009265\n",
            "Step 290000 | Loss: 0.010612\n",
            "Episode 1450 | Reward: -2.92 | Epsilon: 0.050\n",
            "Step 290100 | Loss: 0.007109\n",
            "Step 290200 | Loss: 0.006993\n",
            "Episode 1451 | Reward: -30.52 | Epsilon: 0.050\n",
            "Step 290300 | Loss: 0.007256\n",
            "Step 290400 | Loss: 0.022886\n",
            "Episode 1452 | Reward: -5.36 | Epsilon: 0.050\n",
            "Step 290500 | Loss: 0.002857\n",
            "Step 290600 | Loss: 0.017260\n",
            "Episode 1453 | Reward: -27.89 | Epsilon: 0.050\n",
            "Step 290700 | Loss: 0.003373\n",
            "Step 290800 | Loss: 0.008251\n",
            "Episode 1454 | Reward: -10.87 | Epsilon: 0.050\n",
            "Step 290900 | Loss: 0.008569\n",
            "Step 291000 | Loss: 0.005914\n",
            "Episode 1455 | Reward: -15.17 | Epsilon: 0.050\n",
            "Step 291100 | Loss: 0.008553\n",
            "Step 291200 | Loss: 0.007860\n",
            "Episode 1456 | Reward: -11.01 | Epsilon: 0.050\n",
            "Step 291300 | Loss: 0.515415\n",
            "Step 291400 | Loss: 0.015872\n",
            "Episode 1457 | Reward: -54.26 | Epsilon: 0.050\n",
            "Step 291500 | Loss: 0.011010\n",
            "Step 291600 | Loss: 2.571979\n",
            "Episode 1458 | Reward: -66.46 | Epsilon: 0.050\n",
            "Step 291700 | Loss: 0.006579\n",
            "Step 291800 | Loss: 0.011230\n",
            "Episode 1459 | Reward: -7.27 | Epsilon: 0.050\n",
            "Step 291900 | Loss: 0.033759\n",
            "Step 292000 | Loss: 0.013154\n",
            "Episode 1460 | Target network updated.\n",
            "Episode 1460 | Reward: -10.14 | Epsilon: 0.050\n",
            "Step 292100 | Loss: 0.006530\n",
            "Step 292200 | Loss: 0.005956\n",
            "Episode 1461 | Reward: -122.60 | Epsilon: 0.050\n",
            "Step 292300 | Loss: 0.007589\n",
            "Step 292400 | Loss: 0.014916\n",
            "Episode 1462 | Reward: -66.80 | Epsilon: 0.050\n",
            "Step 292500 | Loss: 0.043891\n",
            "Step 292600 | Loss: 0.003765\n",
            "Episode 1463 | Reward: -34.74 | Epsilon: 0.050\n",
            "Step 292700 | Loss: 0.004948\n",
            "Step 292800 | Loss: 0.013793\n",
            "Episode 1464 | Reward: -12.81 | Epsilon: 0.050\n",
            "Step 292900 | Loss: 0.004990\n",
            "Step 293000 | Loss: 0.012439\n",
            "Episode 1465 | Reward: -12.85 | Epsilon: 0.050\n",
            "Step 293100 | Loss: 0.007445\n",
            "Step 293200 | Loss: 0.004706\n",
            "Episode 1466 | Reward: -29.72 | Epsilon: 0.050\n",
            "Step 293300 | Loss: 0.007057\n",
            "Step 293400 | Loss: 0.525703\n",
            "Episode 1467 | Reward: -8.39 | Epsilon: 0.050\n",
            "Step 293500 | Loss: 0.004974\n",
            "Step 293600 | Loss: 0.005433\n",
            "Episode 1468 | Reward: -4.87 | Epsilon: 0.050\n",
            "Step 293700 | Loss: 0.007689\n",
            "Step 293800 | Loss: 0.013502\n",
            "Episode 1469 | Reward: -3.81 | Epsilon: 0.050\n",
            "Step 293900 | Loss: 3.025925\n",
            "Step 294000 | Loss: 0.003617\n",
            "Episode 1470 | Reward: -2.74 | Epsilon: 0.050\n",
            "Step 294100 | Loss: 0.007555\n",
            "Step 294200 | Loss: 0.758365\n",
            "Episode 1471 | Reward: -18.00 | Epsilon: 0.050\n",
            "Step 294300 | Loss: 0.005099\n",
            "Step 294400 | Loss: 0.004316\n",
            "Episode 1472 | Reward: -33.81 | Epsilon: 0.050\n",
            "Step 294500 | Loss: 0.005704\n",
            "Step 294600 | Loss: 0.004819\n",
            "Episode 1473 | Reward: -27.19 | Epsilon: 0.050\n",
            "Step 294700 | Loss: 0.004916\n",
            "Step 294800 | Loss: 0.002933\n",
            "Episode 1474 | Reward: -7.00 | Epsilon: 0.050\n",
            "Step 294900 | Loss: 0.011658\n",
            "Step 295000 | Loss: 0.011421\n",
            "Episode 1475 | Reward: -14.10 | Epsilon: 0.050\n",
            "Step 295100 | Loss: 0.009924\n",
            "Step 295200 | Loss: 0.005696\n",
            "Episode 1476 | Reward: -32.61 | Epsilon: 0.050\n",
            "Step 295300 | Loss: 0.007465\n",
            "Step 295400 | Loss: 0.005365\n",
            "Episode 1477 | Reward: -58.98 | Epsilon: 0.050\n",
            "Step 295500 | Loss: 0.010872\n",
            "Step 295600 | Loss: 0.005970\n",
            "Episode 1478 | Reward: -14.00 | Epsilon: 0.050\n",
            "Step 295700 | Loss: 0.052775\n",
            "Step 295800 | Loss: 0.003409\n",
            "Episode 1479 | Reward: -2.02 | Epsilon: 0.050\n",
            "Step 295900 | Loss: 12.050267\n",
            "Step 296000 | Loss: 0.003756\n",
            "Episode 1480 | Target network updated.\n",
            "Episode 1480 | Reward: -51.54 | Epsilon: 0.050\n",
            "Step 296100 | Loss: 1.222488\n",
            "Step 296200 | Loss: 0.004537\n",
            "Episode 1481 | Reward: -19.57 | Epsilon: 0.050\n",
            "Step 296300 | Loss: 0.006786\n",
            "Step 296400 | Loss: 0.009558\n",
            "Episode 1482 | Reward: -6.23 | Epsilon: 0.050\n",
            "Step 296500 | Loss: 0.007228\n",
            "Step 296600 | Loss: 0.005793\n",
            "Episode 1483 | Reward: -17.56 | Epsilon: 0.050\n",
            "Step 296700 | Loss: 0.007335\n",
            "Step 296800 | Loss: 0.004420\n",
            "Episode 1484 | Reward: -19.77 | Epsilon: 0.050\n",
            "Step 296900 | Loss: 0.004457\n",
            "Step 297000 | Loss: 0.006974\n",
            "Episode 1485 | Reward: -15.03 | Epsilon: 0.050\n",
            "Step 297100 | Loss: 0.003495\n",
            "Step 297200 | Loss: 0.005051\n",
            "Episode 1486 | Reward: -9.95 | Epsilon: 0.050\n",
            "Step 297300 | Loss: 0.002825\n",
            "Step 297400 | Loss: 0.004295\n",
            "Episode 1487 | Reward: -6.50 | Epsilon: 0.050\n",
            "Step 297500 | Loss: 0.003404\n",
            "Step 297600 | Loss: 0.003302\n",
            "Episode 1488 | Reward: -8.50 | Epsilon: 0.050\n",
            "Step 297700 | Loss: 0.090016\n",
            "Step 297800 | Loss: 0.005299\n",
            "Episode 1489 | Reward: -10.33 | Epsilon: 0.050\n",
            "Step 297900 | Loss: 0.003849\n",
            "Step 298000 | Loss: 0.004909\n",
            "Episode 1490 | Reward: -33.27 | Epsilon: 0.050\n",
            "Step 298100 | Loss: 0.010295\n",
            "Step 298200 | Loss: 0.105910\n",
            "Episode 1491 | Reward: -38.73 | Epsilon: 0.050\n",
            "Step 298300 | Loss: 0.005322\n",
            "Step 298400 | Loss: 0.002433\n",
            "Episode 1492 | Reward: -5.49 | Epsilon: 0.050\n",
            "Step 298500 | Loss: 0.004960\n",
            "Step 298600 | Loss: 0.003131\n",
            "Episode 1493 | Reward: -8.73 | Epsilon: 0.050\n",
            "Step 298700 | Loss: 0.002615\n",
            "Step 298800 | Loss: 0.018656\n",
            "Episode 1494 | Reward: -44.13 | Epsilon: 0.050\n",
            "Step 298900 | Loss: 0.003135\n",
            "Step 299000 | Loss: 0.007029\n",
            "Episode 1495 | Reward: -2.79 | Epsilon: 0.050\n",
            "Step 299100 | Loss: 0.004912\n",
            "Step 299200 | Loss: 0.014363\n",
            "Episode 1496 | Reward: -10.69 | Epsilon: 0.050\n",
            "Step 299300 | Loss: 0.005261\n",
            "Step 299400 | Loss: 0.002700\n",
            "Episode 1497 | Reward: -19.42 | Epsilon: 0.050\n",
            "Step 299500 | Loss: 0.008877\n",
            "Step 299600 | Loss: 0.003461\n",
            "Episode 1498 | Reward: -27.39 | Epsilon: 0.050\n",
            "Step 299700 | Loss: 0.003308\n",
            "Step 299800 | Loss: 0.009148\n",
            "Episode 1499 | Reward: -3.73 | Epsilon: 0.050\n",
            "Step 299900 | Loss: 0.002914\n",
            "Step 300000 | Loss: 0.005566\n",
            "Episode 1500 | Target network updated.\n",
            "model saved to models/2025-04-29_04-17-20/q_network_ep_1500.pth\n",
            "\n",
            "Episode 1500 | Reward: -5.97 | Epsilon: 0.050\n"
          ]
        }
      ],
      "source": [
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "\n",
        "# DO NOT CHANGE\n",
        "# ---------------\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "env = ArmEnv(arm, gui=False)\n",
        "tqdn = TrainDQN(env)\n",
        "# ---------------\n",
        "\n",
        "# Call your trin function here\n",
        "tqdn.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To keep track of your experiments, it is good practice to plot and check how well is your model trained based on the returns vs episodes plot. With a large number of episodes, this  plot may look very jagged making it difficult to ascertain how well you are doing. We are proving code to smoothen out the plot by. This will take a large list of returns in every episode and plot a smoothened version of the list. Feel free to use it if it helps.\n",
        "```\n",
        "import seaborn as sns\n",
        "returns = __\n",
        "smoothing = 10\n",
        "\n",
        "smoothened = [sum(returns[i:i+smoothing])/smoothing for i in range(0, len(returns), smoothing)]\n",
        "sns.lineplot(smoothened)\n",
        "```"
      ],
      "metadata": {
        "id": "jAlaOcVJsn5a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIcBDbeTRNZI"
      },
      "source": [
        "### Load your model and test its performance\n",
        "Change your model path and the goal to see how well your learnt model is performing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "x5gTRNKhRQQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4480f42e-bacb-451c-b5a2-19330c269fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/geometry.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  R[0,0] = np.cos(theta)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py:48: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  return conv.wrap(result, to_scalar=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode return:  -14.547049968960614\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "import numpy as np\n",
        "import os\n",
        "from math import dist\n",
        "import seaborn as sns\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from geometry import polar2cartesian\n",
        "\n",
        "\n",
        "# DO NOT CHANGE arm parameters\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "# ------------------\n",
        "\n",
        "env = ArmEnv(arm, gui=False)\n",
        "model_path = 'models/2025-04-29_04-17-20/q_network_ep_1500.pth' # Fill in the model_path\n",
        "device = torch.device('cpu')\n",
        "qnet = QNetwork(env).to(device)\n",
        "qnet.load_state_dict(torch.load(model_path))\n",
        "qnet.eval()\n",
        "goal = polar2cartesian(1.6, 0.25 - np.pi/2.0)\n",
        "done = False\n",
        "obs = env.reset(goal)\n",
        "\n",
        "episode_return = 0\n",
        "while not done:\n",
        "    obs_tensor = torch.tensor(obs, dtype=torch.float32, device=device)\n",
        "    #obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
        "    action = qnet.select_discrete_action(obs, device)\n",
        "    action = qnet.action_discrete_to_continuous(action)\n",
        "    new_obs, reward, done, info = env.step(action)\n",
        "    episode_return += reward\n",
        "\n",
        "    pos_ee = info['pos_ee']\n",
        "    vel_ee = info['vel_ee']\n",
        "    dist = np.linalg.norm(pos_ee - goal)\n",
        "\n",
        "    obs = new_obs\n",
        "print('Episode return: ', episode_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grading and Evaluation\n",
        "You will be evaluated on 5 different goal positions worth 1.5 points each. You must pass the best `model_path` for your network. The scoring function will run one episode for every goal position and find the total reward (aka return) for the episode. For every goal you get:\n",
        "\n",
        "* 1 Point if `easy target < total reward < hard target`\n",
        "* 1.5 Points if `hard target < total reward`"
      ],
      "metadata": {
        "id": "pUDEYLsZLSp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from score import compute_score\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# DO NOT CHANGE arm parameters\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "# ------------------\n",
        "\n",
        "env = ArmEnv(arm, gui=False)\n",
        "model_path = 'models/2025-04-29_04-17-20/q_network_ep_1500.pth' # Fill in the model_path\n",
        "device = torch.device('cpu')\n",
        "qnet = QNetwork(env).to(device)\n",
        "qnet.load_state_dict(torch.load(model_path))\n",
        "qnet.eval()\n",
        "score = compute_score(qnet, env, device)"
      ],
      "metadata": {
        "id": "OhPD-u6TIxdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0771b45-8f7f-4e8d-b78e-369b6c167991"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -65.7917961252195\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -14.547049968960614\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -10.940031814601141\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -20.63792301734081\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 0\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -21.406424593384767\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 0\n",
            "\n",
            "\n",
            "Final score: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: PPO with an open source RL library\n",
        "\n",
        "In this part, you will use one of the most popular open source RL libraries ([Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)) to solve the same goal reaching problem as Part 1. We will use the same `ArmEnv` gym environment. The algorithm you should choose to use is PPO."
      ],
      "metadata": {
        "id": "vkCvO0-05XK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO training\n",
        "\n",
        "We provide the code to construct parallel environments. Parallel environments can be very useful if you have good CPUs and it can speed up training."
      ],
      "metadata": {
        "id": "UBK89P2B8CgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
        "from stable_baselines3.common.vec_env.vec_monitor import VecMonitor\n",
        "from copy import deepcopy\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from arm_env import ArmEnv\n",
        "\n",
        "class EnvMaker:\n",
        "    def __init__(self,  arm, seed):\n",
        "        self.seed = seed\n",
        "        self.arm = arm\n",
        "\n",
        "    def __call__(self):\n",
        "        arm = deepcopy(self.arm)\n",
        "        env = ArmEnv(arm)\n",
        "        env.seed(self.seed)\n",
        "        return env\n",
        "\n",
        "def make_vec_env(arm, nenv, seed):\n",
        "    return VecMonitor(SubprocVecEnv([EnvMaker(arm, seed  + 100 * i) for i in range(nenv)]))\n",
        "\n",
        "# conveniet function to create a robot arm\n",
        "def make_arm():\n",
        "    arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01\n",
        "        )\n",
        "    )\n",
        "    arm.reset()\n",
        "    return arm\n"
      ],
      "metadata": {
        "id": "2RTqfmpVwMja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need to complete the code to train the policy using the [PPO class](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) from stable_baselines3. We provide the code to generate the name of the directory to save the checkpoint, an example is `ppo_models/2025-04-21_01-14-13`. Your checkpoint model should be named `ppo_network.zip`. See the [save](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO.save) function. Training should take less than 40 minutes."
      ],
      "metadata": {
        "id": "Bniz2TouwM3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.ppo import PPO\n",
        "import os\n",
        "import time\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "\n",
        "# Default parameters\n",
        "timesteps = 500000\n",
        "nenv = 8  # number of parallel environments. This can speed up training when you have good CPUs\n",
        "seed = 8\n",
        "batch_size = 2048\n",
        "\n",
        "# Generate path of the directory to save the checkpoint\n",
        "timestr = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "save_dir = os.path.join('ppo_models', timestr)\n",
        "\n",
        "# Set random seed\n",
        "set_random_seed(seed)\n",
        "\n",
        "# Create arm\n",
        "arm = make_arm()\n",
        "\n",
        "# Create parallel envs\n",
        "vec_env = make_vec_env(arm=arm, nenv=nenv, seed=seed)\n",
        "\n",
        "# ------ IMPLEMENT YOUR TRAINING CODE HERE ------------\n",
        "raise NotImplementedError\n",
        "\n",
        "# Do not forget to save your model at the end of training"
      ],
      "metadata": {
        "id": "FHoSWOnG-2sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading and evaluation\n",
        "\n",
        "The total number of points for Part 2 is 7.5. We will evaluate your trained model on 5 random goal locations. For each test, we assign points based on the distance between the end effector and the goal location at the end of the episode.\n",
        "\n",
        "- If 0 < distance < 0.05, you get 1.5 points.\n",
        "- If 0.05 <= distance < 0.1, you get 1 point.\n",
        "- If distance >= 0.1, you get 0 point.\n",
        "\n"
      ],
      "metadata": {
        "id": "f9N2falIz9rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from score import score_policy\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from render import Renderer\n",
        "import time\n",
        "\n",
        "# Set the path to your model\n",
        "model_path = 'ppo_network.zip'\n",
        "\n",
        "set_random_seed(seed=100)\n",
        "\n",
        "# Create arm robot\n",
        "arm = make_arm()\n",
        "\n",
        "# Create environment\n",
        "env = ArmEnv(arm, gui=False)\n",
        "env.seed(100)\n",
        "\n",
        "# Load and test policy\n",
        "policy = PPO.load(model_path)\n",
        "score_policy(policy, env)"
      ],
      "metadata": {
        "id": "X6eQ2mzglwd0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}